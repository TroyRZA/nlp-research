{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# COS 760 Research Project: Analysing Sentiments for Low-resource African Languages",
   "id": "7de0be731c215516"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Group Members: Mihir Arjun, Troy Clark, Hamza Mokiwa",
   "id": "c9f2dd9f3ba5fd01"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Establishing Baselines with Monolingual Long Short-Term Memory networks(LSTMs) and pre-trained Multilingual transformers",
   "id": "b25d18b3f0f68fc0"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### First we need to install the datasets",
   "id": "67717bc76114282d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "%pip install datasets",
   "id": "a69cddc1a533f9cb"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import os\n",
    "from datasets import load_from_disk, load_dataset\n",
    "\n",
    "def load_local_datasets():\n",
    "    swa_path = \"./datasets/afrisenti/swa\"\n",
    "    por_path = \"./datasets/afrisenti/por\"\n",
    "    sot_path = \"./datasets/news\"\n",
    "\n",
    "    if not all(os.path.exists(path) for path in [swa_path, por_path,sot_path]):\n",
    "        print(\"One or more dataset directories not found. Please check the paths.\")\n",
    "        return None, None, None\n",
    "\n",
    "    print(\"Loading Swahili (swa) dataset from disk...\")\n",
    "    swa_dataset = load_from_disk(swa_path)\n",
    "    print(\"Swahili dataset loaded!\")\n",
    "\n",
    "    print(\"Loading Portuguese (por) dataset from disk...\")\n",
    "    por_dataset = load_from_disk(por_path)\n",
    "    print(\"Portuguese dataset loaded!\")\n",
    "\n",
    "    print(\"Loading Sesotho (sot) dataset from disk...\")\n",
    "    sot_dataset = load_dataset(\"csv\",  data_files=\"datasets/sotho-news/sotho_news_dataset.csv\")\n",
    "    print(\"Sesotho dataset loaded!\")\n",
    "\n",
    "    return swa_dataset, por_dataset, sot_dataset\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    swa, por, sot = load_local_datasets()\n",
    "\n",
    "    if swa is not None:\n",
    "        print(f\"Swahili dataset size: {len(swa['train'])} examples\")\n",
    "    if por is not None:\n",
    "        print(f\"Portuguese dataset size: {len(por['train'])} examples\")\n",
    "    if sot is not None:\n",
    "        print(f\"Sesotho dataset size: {len(sot['train'])} examples\")"
   ],
   "id": "f3237e99e3cceddb"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Now that the datasets have been loaded, we can start creating our LSTM baseline models below:",
   "id": "51950033e7dbbef2"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### First we will build an LSTM model for Swahili",
   "id": "ee1f3a31fa87d3ab"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datasets import load_from_disk\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from sklearn.metrics import accuracy_score, f1_score, classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "from collections import Counter\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "\n",
    "EMBEDDING_DIM = 100\n",
    "HIDDEN_DIM = 128\n",
    "NUM_LAYERS = 2\n",
    "DROPOUT = 0.2\n",
    "LEARNING_RATE = 0.001\n",
    "BATCH_SIZE = 32\n",
    "NUM_EPOCHS = 10\n",
    "\n",
    "class SwahiliSentimentDataset(Dataset):\n",
    "    def __init__(self, tweets, labels, vocab, label_map):\n",
    "        self.tweets = tweets\n",
    "        self.labels = labels\n",
    "        self.vocab = vocab\n",
    "        self.label_map = label_map\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.tweets)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        tweet = self.tweets[idx]\n",
    "        label = self.labels[idx]\n",
    "\n",
    "\n",
    "        tokenized = [self.vocab.get(word, self.vocab['<UNK>']) for word in tweet.split()]\n",
    "        return torch.tensor(tokenized, dtype=torch.long), torch.tensor(self.label_map[label], dtype=torch.long)\n",
    "\n",
    "def collate_fn(batch):\n",
    "    tweets, labels = zip(*batch)\n",
    "\n",
    "    tweets_padded = pad_sequence(tweets, batch_first=True, padding_value=0)\n",
    "    return tweets_padded, torch.stack(labels)\n",
    "\n",
    "class LSTMSentiment(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim, n_layers, dropout, pad_idx):\n",
    "        super().__init__()\n",
    "\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=pad_idx)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, num_layers=n_layers,\n",
    "                           bidirectional=True, dropout=dropout, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim * 2, output_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, text):\n",
    "\n",
    "        embedded = self.dropout(self.embedding(text))\n",
    "\n",
    "\n",
    "        output, (hidden, cell) = self.lstm(embedded)\n",
    "\n",
    "\n",
    "\n",
    "        hidden = torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim=1)\n",
    "\n",
    "\n",
    "        hidden = self.dropout(hidden)\n",
    "        return self.fc(hidden)\n",
    "\n",
    "def train_model(model, train_loader, val_loader, optimizer, criterion, device, epochs):\n",
    "    best_val_loss = float('inf')\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "\n",
    "        for tweets, labels in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{epochs} - Training\"):\n",
    "            tweets, labels = tweets.to(device), labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "\n",
    "            outputs = model(tweets)\n",
    "\n",
    "\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "\n",
    "        train_loss = running_loss / len(train_loader)\n",
    "\n",
    "\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        all_preds = []\n",
    "        all_labels = []\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for tweets, labels in tqdm(val_loader, desc=f\"Epoch {epoch+1}/{epochs} - Validation\"):\n",
    "                tweets, labels = tweets.to(device), labels.to(device)\n",
    "\n",
    "                outputs = model(tweets)\n",
    "                loss = criterion(outputs, labels)\n",
    "\n",
    "                val_loss += loss.item()\n",
    "\n",
    "                _, preds = torch.max(outputs, 1)\n",
    "                all_preds.extend(preds.cpu().numpy())\n",
    "                all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "        val_loss /= len(val_loader)\n",
    "        val_accuracy = accuracy_score(all_labels, all_preds)\n",
    "        val_f1 = f1_score(all_labels, all_preds, average='weighted')\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{epochs}:\")\n",
    "        print(f\"  Train Loss: {train_loss:.4f}\")\n",
    "        print(f\"  Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.4f}, Val F1: {val_f1:.4f}\")\n",
    "\n",
    "\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            torch.save(model.state_dict(), \"best_swahili_lstm_model.pt\")\n",
    "            print(\"  Saved new best model!\")\n",
    "\n",
    "        print(\"-\" * 60)\n",
    "\n",
    "    return model\n",
    "\n",
    "def evaluate_model(model, test_loader, criterion, device, label_list):\n",
    "    model.eval()\n",
    "    test_loss = 0.0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for tweets, labels in tqdm(test_loader, desc=\"Testing\"):\n",
    "            tweets, labels = tweets.to(device), labels.to(device)\n",
    "\n",
    "            outputs = model(tweets)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            test_loss += loss.item()\n",
    "\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    test_loss /= len(test_loader)\n",
    "    test_accuracy = accuracy_score(all_labels, all_preds)\n",
    "    test_f1 = f1_score(all_labels, all_preds, average='weighted')\n",
    "\n",
    "    print(f\"Test Loss: {test_loss:.4f}\")\n",
    "    print(f\"Test Accuracy: {test_accuracy:.4f}\")\n",
    "    print(f\"Test F1 Score: {test_f1:.4f}\")\n",
    "\n",
    "\n",
    "    class_names = [label_list[i] for i in range(len(label_list))]\n",
    "    report = classification_report(all_labels, all_preds, target_names=class_names)\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(report)\n",
    "\n",
    "    return test_loss, test_accuracy, test_f1\n",
    "\n",
    "def main():\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print(f\"Using device: {device}\")\n",
    "\n",
    "\n",
    "    try:\n",
    "        swa_dataset = load_from_disk(\"./datasets/afrisenti/swa\")\n",
    "        print(\"Swahili dataset loaded successfully!\")\n",
    "\n",
    "        print(f\"Available columns in train split: {swa_dataset['train'].column_names}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading dataset: {e}\")\n",
    "        return\n",
    "\n",
    "    print(f\"Dataset structure: {swa_dataset}\")\n",
    "    print(f\"Train set size: {len(swa_dataset['train'])}\")\n",
    "    print(f\"Test set size: {len(swa_dataset['test'])}\")\n",
    "    print(f\"Validation set size: {len(swa_dataset['validation'])}\")\n",
    "\n",
    "    train_tweets = swa_dataset['train']['tweet']\n",
    "    train_labels = swa_dataset['train']['label']\n",
    "    val_tweets = swa_dataset['validation']['tweet']\n",
    "    val_labels = swa_dataset['validation']['label']\n",
    "    test_tweets = swa_dataset['test']['tweet']\n",
    "    test_labels = swa_dataset['test']['label']\n",
    "\n",
    "    unique_labels = set(train_labels + val_labels + test_labels)\n",
    "    label_to_idx = {label: idx for idx, label in enumerate(unique_labels)}\n",
    "    idx_to_label = {idx: label for label, idx in label_to_idx.items()}\n",
    "    print(f\"Label mapping: {label_to_idx}\")\n",
    "\n",
    "    word_counts = Counter()\n",
    "    for tweet in train_tweets:\n",
    "        word_counts.update(tweet.split())\n",
    "\n",
    "    min_freq = 2\n",
    "    vocabulary = {'<PAD>': 0, '<UNK>': 1}\n",
    "    vocab_idx = 2\n",
    "\n",
    "    for word, count in word_counts.items():\n",
    "        if count >= min_freq:\n",
    "            vocabulary[word] = vocab_idx\n",
    "            vocab_idx += 1\n",
    "\n",
    "    print(f\"Vocabulary size: {len(vocabulary)}\")\n",
    "\n",
    "    train_dataset = SwahiliSentimentDataset(train_tweets, train_labels, vocabulary, label_to_idx)\n",
    "    val_dataset = SwahiliSentimentDataset(val_tweets, val_labels, vocabulary, label_to_idx)\n",
    "    test_dataset = SwahiliSentimentDataset(test_tweets, test_labels, vocabulary, label_to_idx)\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_fn)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, collate_fn=collate_fn)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, collate_fn=collate_fn)\n",
    "\n",
    "    model = LSTMSentiment(\n",
    "        vocab_size=len(vocabulary),\n",
    "        embedding_dim=EMBEDDING_DIM,\n",
    "        hidden_dim=HIDDEN_DIM,\n",
    "        output_dim=len(unique_labels),\n",
    "        n_layers=NUM_LAYERS,\n",
    "        dropout=DROPOUT,\n",
    "        pad_idx=vocabulary['<PAD>']\n",
    "    ).to(device)\n",
    "\n",
    "    print(f\"Model architecture:\\n{model}\")\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    print(\"Starting training...\")\n",
    "    model = train_model(\n",
    "        model=model,\n",
    "        train_loader=train_loader,\n",
    "        val_loader=val_loader,\n",
    "        optimizer=optimizer,\n",
    "        criterion=criterion,\n",
    "        device=device,\n",
    "        epochs=NUM_EPOCHS\n",
    "    )\n",
    "\n",
    "    model.load_state_dict(torch.load(\"best_swahili_lstm_model.pt\"))\n",
    "\n",
    "    print(\"\\nEvaluating on test set...\")\n",
    "    test_loss, test_accuracy, test_f1 = evaluate_model(\n",
    "        model=model,\n",
    "        test_loader=test_loader,\n",
    "        criterion=criterion,\n",
    "        device=device,\n",
    "        label_list=list(idx_to_label.values())\n",
    "    )\n",
    "\n",
    "    results = {\n",
    "        \"test_loss\": test_loss,\n",
    "        \"test_accuracy\": test_accuracy,\n",
    "        \"test_f1\": test_f1,\n",
    "        \"embedding_dim\": EMBEDDING_DIM,\n",
    "        \"hidden_dim\": HIDDEN_DIM,\n",
    "        \"num_layers\": NUM_LAYERS,\n",
    "        \"dropout\": DROPOUT,\n",
    "        \"learning_rate\": LEARNING_RATE,\n",
    "        \"batch_size\": BATCH_SIZE,\n",
    "        \"epochs\": NUM_EPOCHS,\n",
    "        \"vocab_size\": len(vocabulary)\n",
    "    }\n",
    "\n",
    "\n",
    "    pd.DataFrame([results]).to_csv(\"swahili_lstm_results.csv\", index=False)\n",
    "    print(f\"Results saved to swahili_lstm_results.csv\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ],
   "id": "7f998b732fd7fec8"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Next, we build an LSTM model for Mozambican Portuguese:",
   "id": "2a12e7021c49117"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datasets import load_from_disk\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from sklearn.metrics import accuracy_score, f1_score, classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "from collections import Counter\n",
    "from tqdm import tqdm\n",
    "\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "EMBEDDING_DIM = 100\n",
    "HIDDEN_DIM = 128\n",
    "NUM_LAYERS = 2\n",
    "DROPOUT = 0.2\n",
    "LEARNING_RATE = 0.001\n",
    "BATCH_SIZE = 32\n",
    "NUM_EPOCHS = 10\n",
    "\n",
    "class PorSentimentDataset(Dataset):\n",
    "    def __init__(self, tweets, labels, vocab, label_map):\n",
    "        self.tweets = tweets\n",
    "        self.labels = labels\n",
    "        self.vocab = vocab\n",
    "        self.label_map = label_map\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.tweets)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        tweet = self.tweets[idx]\n",
    "        label = self.labels[idx]\n",
    "\n",
    "        tokenized = [self.vocab.get(word, self.vocab['<UNK>']) for word in tweet.split()]\n",
    "        return torch.tensor(tokenized, dtype=torch.long), torch.tensor(self.label_map[label], dtype=torch.long)\n",
    "\n",
    "def collate_fn(batch):\n",
    "    tweets, labels = zip(*batch)\n",
    "    tweets_padded = pad_sequence(tweets, batch_first=True, padding_value=0)\n",
    "    return tweets_padded, torch.stack(labels)\n",
    "\n",
    "class LSTMSentiment(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim, n_layers, dropout, pad_idx):\n",
    "        super().__init__()\n",
    "\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=pad_idx)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, num_layers=n_layers,\n",
    "                           bidirectional=True, dropout=dropout, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim * 2, output_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, text):\n",
    "        embedded = self.dropout(self.embedding(text))\n",
    "\n",
    "        output, (hidden, cell) = self.lstm(embedded)\n",
    "\n",
    "        hidden = torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim=1)\n",
    "\n",
    "        hidden = self.dropout(hidden)\n",
    "        return self.fc(hidden)\n",
    "\n",
    "def train_model(model, train_loader, val_loader, optimizer, criterion, device, epochs):\n",
    "    best_val_loss = float('inf')\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "\n",
    "        for tweets, labels in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{epochs} - Training\"):\n",
    "            tweets, labels = tweets.to(device), labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            outputs = model(tweets)\n",
    "\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "\n",
    "        train_loss = running_loss / len(train_loader)\n",
    "\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        all_preds = []\n",
    "        all_labels = []\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for tweets, labels in tqdm(val_loader, desc=f\"Epoch {epoch+1}/{epochs} - Validation\"):\n",
    "                tweets, labels = tweets.to(device), labels.to(device)\n",
    "\n",
    "                outputs = model(tweets)\n",
    "                loss = criterion(outputs, labels)\n",
    "\n",
    "                val_loss += loss.item()\n",
    "\n",
    "                _, preds = torch.max(outputs, 1)\n",
    "                all_preds.extend(preds.cpu().numpy())\n",
    "                all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "        val_loss /= len(val_loader)\n",
    "        val_accuracy = accuracy_score(all_labels, all_preds)\n",
    "        val_f1 = f1_score(all_labels, all_preds, average='weighted')\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{epochs}:\")\n",
    "        print(f\"  Train Loss: {train_loss:.4f}\")\n",
    "        print(f\"  Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.4f}, Val F1: {val_f1:.4f}\")\n",
    "\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            torch.save(model.state_dict(), \"best_portuguese_lstm_model.pt\")\n",
    "            print(\"  Saved new best model!\")\n",
    "\n",
    "        print(\"-\" * 60)\n",
    "\n",
    "    return model\n",
    "\n",
    "def evaluate_model(model, test_loader, criterion, device, label_list):\n",
    "    model.eval()\n",
    "    test_loss = 0.0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for tweets, labels in tqdm(test_loader, desc=\"Testing\"):\n",
    "            tweets, labels = tweets.to(device), labels.to(device)\n",
    "\n",
    "            outputs = model(tweets)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            test_loss += loss.item()\n",
    "\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    test_loss /= len(test_loader)\n",
    "    test_accuracy = accuracy_score(all_labels, all_preds)\n",
    "    test_f1 = f1_score(all_labels, all_preds, average='weighted')\n",
    "\n",
    "    print(f\"Test Loss: {test_loss:.4f}\")\n",
    "    print(f\"Test Accuracy: {test_accuracy:.4f}\")\n",
    "    print(f\"Test F1 Score: {test_f1:.4f}\")\n",
    "\n",
    "    class_names = [label_list[i] for i in range(len(label_list))]\n",
    "    report = classification_report(all_labels, all_preds, target_names=class_names)\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(report)\n",
    "\n",
    "    return test_loss, test_accuracy, test_f1\n",
    "\n",
    "def main():\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print(f\"Using device: {device}\")\n",
    "\n",
    "    try:\n",
    "        por_dataset = load_from_disk(\"./datasets/afrisenti/por\")\n",
    "        print(\"Mozambican Portuguese dataset loaded successfully!\")\n",
    "\n",
    "        print(f\"Available columns in train split: {por_dataset['train'].column_names}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading dataset: {e}\")\n",
    "        return\n",
    "\n",
    "    print(f\"Dataset structure: {por_dataset}\")\n",
    "    print(f\"Train set size: {len(por_dataset['train'])}\")\n",
    "    print(f\"Test set size: {len(por_dataset['test'])}\")\n",
    "    print(f\"Validation set size: {len(por_dataset['validation'])}\")\n",
    "\n",
    "    train_tweets = por_dataset['train']['tweet']\n",
    "    train_labels = por_dataset['train']['label']\n",
    "    val_tweets = por_dataset['validation']['tweet']\n",
    "    val_labels = por_dataset['validation']['label']\n",
    "    test_tweets = por_dataset['test']['tweet']\n",
    "    test_labels = por_dataset['test']['label']\n",
    "\n",
    "    unique_labels = set(train_labels + val_labels + test_labels)\n",
    "    label_to_idx = {label: idx for idx, label in enumerate(unique_labels)}\n",
    "    idx_to_label = {idx: label for label, idx in label_to_idx.items()}\n",
    "    print(f\"Label mapping: {label_to_idx}\")\n",
    "\n",
    "    word_counts = Counter()\n",
    "    for tweet in train_tweets:\n",
    "        word_counts.update(tweet.split())\n",
    "\n",
    "    min_freq = 2\n",
    "    vocabulary = {'<PAD>': 0, '<UNK>': 1}\n",
    "    vocab_idx = 2\n",
    "\n",
    "    for word, count in word_counts.items():\n",
    "        if count >= min_freq:\n",
    "            vocabulary[word] = vocab_idx\n",
    "            vocab_idx += 1\n",
    "\n",
    "    print(f\"Vocabulary size: {len(vocabulary)}\")\n",
    "\n",
    "    train_dataset = PorSentimentDataset(train_tweets, train_labels, vocabulary, label_to_idx)\n",
    "    val_dataset = PorSentimentDataset(val_tweets, val_labels, vocabulary, label_to_idx)\n",
    "    test_dataset = PorSentimentDataset(test_tweets, test_labels, vocabulary, label_to_idx)\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_fn)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, collate_fn=collate_fn)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, collate_fn=collate_fn)\n",
    "\n",
    "    model = LSTMSentiment(\n",
    "        vocab_size=len(vocabulary),\n",
    "        embedding_dim=EMBEDDING_DIM,\n",
    "        hidden_dim=HIDDEN_DIM,\n",
    "        output_dim=len(unique_labels),\n",
    "        n_layers=NUM_LAYERS,\n",
    "        dropout=DROPOUT,\n",
    "        pad_idx=vocabulary['<PAD>']\n",
    "    ).to(device)\n",
    "\n",
    "    print(f\"Model architecture:\\n{model}\")\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    print(\"Starting training...\")\n",
    "    model = train_model(\n",
    "        model=model,\n",
    "        train_loader=train_loader,\n",
    "        val_loader=val_loader,\n",
    "        optimizer=optimizer,\n",
    "        criterion=criterion,\n",
    "        device=device,\n",
    "        epochs=NUM_EPOCHS\n",
    "    )\n",
    "\n",
    "    model.load_state_dict(torch.load(\"best_portuguese_lstm_model.pt\"))\n",
    "\n",
    "    print(\"\\nEvaluating on test set...\")\n",
    "    test_loss, test_accuracy, test_f1 = evaluate_model(\n",
    "        model=model,\n",
    "        test_loader=test_loader,\n",
    "        criterion=criterion,\n",
    "        device=device,\n",
    "        label_list=list(idx_to_label.values())\n",
    "    )\n",
    "\n",
    "    results = {\n",
    "        \"test_loss\": test_loss,\n",
    "        \"test_accuracy\": test_accuracy,\n",
    "        \"test_f1\": test_f1,\n",
    "        \"embedding_dim\": EMBEDDING_DIM,\n",
    "        \"hidden_dim\": HIDDEN_DIM,\n",
    "        \"num_layers\": NUM_LAYERS,\n",
    "        \"dropout\": DROPOUT,\n",
    "        \"learning_rate\": LEARNING_RATE,\n",
    "        \"batch_size\": BATCH_SIZE,\n",
    "        \"epochs\": NUM_EPOCHS,\n",
    "        \"vocab_size\": len(vocabulary)\n",
    "    }\n",
    "\n",
    "\n",
    "    pd.DataFrame([results]).to_csv(\"portuguese_lstm_results.csv\", index=False)\n",
    "    print(f\"Results saved to portuguese_lstm_results.csv\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ],
   "id": "30f8b35af19adc30"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Lastly, we build an LSTM model for Sesotho",
   "id": "74bf601ec231dd07"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datasets import load_from_disk\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from sklearn.metrics import accuracy_score, f1_score, classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "from collections import Counter\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "\n",
    "EMBEDDING_DIM = 100\n",
    "HIDDEN_DIM = 128\n",
    "NUM_LAYERS = 2\n",
    "DROPOUT = 0.2\n",
    "LEARNING_RATE = 0.001\n",
    "BATCH_SIZE = 32\n",
    "NUM_EPOCHS = 10\n",
    "\n",
    "class SotSentimentDataset(Dataset):\n",
    "    def __init__(self, headlines, labels, vocab, label_map):\n",
    "        self.headlines = headlines\n",
    "        self.labels = labels\n",
    "        self.vocab = vocab\n",
    "        self.label_map = label_map\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.headlines)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        tweet = self.headlines[idx]\n",
    "        label = self.labels[idx]\n",
    "\n",
    "\n",
    "        tokenized = [self.vocab.get(word, self.vocab['<UNK>']) for word in tweet.split()]\n",
    "        return torch.tensor(tokenized, dtype=torch.long), torch.tensor(self.label_map[label], dtype=torch.long)\n",
    "\n",
    "def collate_fn(batch):\n",
    "    headlines, labels = zip(*batch)\n",
    "\n",
    "    headlines_padded = pad_sequence(headlines, batch_first=True, padding_value=0)\n",
    "    return headlines_padded, torch.stack(labels)\n",
    "\n",
    "class LSTMSentiment(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim, n_layers, dropout, pad_idx):\n",
    "        super().__init__()\n",
    "\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=pad_idx)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, num_layers=n_layers,\n",
    "                           bidirectional=True, dropout=dropout, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim * 2, output_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, text):\n",
    "\n",
    "        embedded = self.dropout(self.embedding(text))\n",
    "\n",
    "\n",
    "        output, (hidden, cell) = self.lstm(embedded)\n",
    "\n",
    "\n",
    "\n",
    "        hidden = torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim=1)\n",
    "\n",
    "\n",
    "        hidden = self.dropout(hidden)\n",
    "        return self.fc(hidden)\n",
    "\n",
    "def train_model(model, train_loader, val_loader, optimizer, criterion, device, epochs):\n",
    "    best_val_loss = float('inf')\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "\n",
    "        for headlines, labels in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{epochs} - Training\"):\n",
    "            headlines, labels = headlines.to(device), labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "\n",
    "            outputs = model(headlines)\n",
    "\n",
    "\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "\n",
    "        train_loss = running_loss / len(train_loader)\n",
    "\n",
    "\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        all_preds = []\n",
    "        all_labels = []\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for headlines, labels in tqdm(val_loader, desc=f\"Epoch {epoch+1}/{epochs} - Validation\"):\n",
    "                headlines, labels = headlines.to(device), labels.to(device)\n",
    "\n",
    "                outputs = model(headlines)\n",
    "                loss = criterion(outputs, labels)\n",
    "\n",
    "                val_loss += loss.item()\n",
    "\n",
    "                _, preds = torch.max(outputs, 1)\n",
    "                all_preds.extend(preds.cpu().numpy())\n",
    "                all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "        val_loss /= len(val_loader)\n",
    "        val_accuracy = accuracy_score(all_labels, all_preds)\n",
    "        val_f1 = f1_score(all_labels, all_preds, average='weighted')\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{epochs}:\")\n",
    "        print(f\"  Train Loss: {train_loss:.4f}\")\n",
    "        print(f\"  Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.4f}, Val F1: {val_f1:.4f}\")\n",
    "\n",
    "\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            torch.save(model.state_dict(), \"best_portuguese_lstm_model.pt\")\n",
    "            print(\"  Saved new best model!\")\n",
    "\n",
    "        print(\"-\" * 60)\n",
    "\n",
    "    return model\n",
    "\n",
    "def evaluate_model(model, test_loader, criterion, device, label_list):\n",
    "    model.eval()\n",
    "    test_loss = 0.0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for headlines, labels in tqdm(test_loader, desc=\"Testing\"):\n",
    "            headlines, labels = headlines.to(device), labels.to(device)\n",
    "\n",
    "            outputs = model(headlines)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            test_loss += loss.item()\n",
    "\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    test_loss /= len(test_loader)\n",
    "    test_accuracy = accuracy_score(all_labels, all_preds)\n",
    "    test_f1 = f1_score(all_labels, all_preds, average='weighted')\n",
    "\n",
    "    print(f\"Test Loss: {test_loss:.4f}\")\n",
    "    print(f\"Test Accuracy: {test_accuracy:.4f}\")\n",
    "    print(f\"Test F1 Score: {test_f1:.4f}\")\n",
    "\n",
    "\n",
    "    class_names = [label_list[i] for i in range(len(label_list))]\n",
    "    report = classification_report(all_labels, all_preds, target_names=class_names)\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(report)\n",
    "\n",
    "    return test_loss, test_accuracy, test_f1\n",
    "\n",
    "def main():\n",
    "\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print(f\"Using device: {device}\")\n",
    "\n",
    "\n",
    "    try:\n",
    "        sot_dataset = load_from_disk(\"./datasets/sesotho_news_dataset\")\n",
    "        print(\"Sesotho dataset loaded successfully!\")\n",
    "\n",
    "\n",
    "        print(f\"Available columns in train split: {sot_dataset['train'].column_names}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading dataset: {e}\")\n",
    "        return\n",
    "\n",
    "\n",
    "    print(f\"Dataset structure: {sot_dataset}\")\n",
    "    print(f\"Train set size: {len(sot_dataset['train'])}\")\n",
    "    print(f\"Test set size: {len(sot_dataset['test'])}\")\n",
    "    print(f\"Validation set size: {len(sot_dataset['validation'])}\")\n",
    "\n",
    "\n",
    "    train_headlines = sot_dataset['train']['headline']\n",
    "    train_labels = sot_dataset['train']['label']\n",
    "    val_headlines = sot_dataset['validation']['headline']\n",
    "    val_labels = sot_dataset['validation']['label']\n",
    "    test_headlines = sot_dataset['test']['headline']\n",
    "    test_labels = sot_dataset['test']['label']\n",
    "\n",
    "\n",
    "\n",
    "    unique_labels = set(train_labels + val_labels + test_labels)\n",
    "    label_to_idx = {label: idx for idx, label in enumerate(unique_labels)}\n",
    "    idx_to_label = {idx: label for label, idx in label_to_idx.items()}\n",
    "    print(f\"Label mapping: {label_to_idx}\")\n",
    "\n",
    "\n",
    "    word_counts = Counter()\n",
    "    for headline in train_headlines:\n",
    "        word_counts.update(headline.split())\n",
    "\n",
    "\n",
    "    min_freq = 2\n",
    "    vocabulary = {'<PAD>': 0, '<UNK>': 1}\n",
    "    vocab_idx = 2\n",
    "\n",
    "    for word, count in word_counts.items():\n",
    "        if count >= min_freq:\n",
    "            vocabulary[word] = vocab_idx\n",
    "            vocab_idx += 1\n",
    "\n",
    "    print(f\"Vocabulary size: {len(vocabulary)}\")\n",
    "\n",
    "\n",
    "    train_dataset = SotSentimentDataset(train_headlines, train_labels, vocabulary, label_to_idx)\n",
    "    val_dataset = SotSentimentDataset(val_headlines, val_labels, vocabulary, label_to_idx)\n",
    "    test_dataset = SotSentimentDataset(test_headlines, test_labels, vocabulary, label_to_idx)\n",
    "\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_fn)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, collate_fn=collate_fn)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, collate_fn=collate_fn)\n",
    "\n",
    "\n",
    "    model = LSTMSentiment(\n",
    "        vocab_size=len(vocabulary),\n",
    "        embedding_dim=EMBEDDING_DIM,\n",
    "        hidden_dim=HIDDEN_DIM,\n",
    "        output_dim=len(unique_labels),\n",
    "        n_layers=NUM_LAYERS,\n",
    "        dropout=DROPOUT,\n",
    "        pad_idx=vocabulary['<PAD>']\n",
    "    ).to(device)\n",
    "\n",
    "\n",
    "    print(f\"Model architecture:\\n{model}\")\n",
    "\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "\n",
    "    print(\"Starting training...\")\n",
    "    model = train_model(\n",
    "        model=model,\n",
    "        train_loader=train_loader,\n",
    "        val_loader=val_loader,\n",
    "        optimizer=optimizer,\n",
    "        criterion=criterion,\n",
    "        device=device,\n",
    "        epochs=NUM_EPOCHS\n",
    "    )\n",
    "\n",
    "\n",
    "    torch.save(model.state_dict(), \"best_sesotho_lstm_model.pt\");\n",
    "\n",
    "\n",
    "    model.load_state_dict(torch.load(\"best_sesotho_lstm_model.pt\"))\n",
    "\n",
    "\n",
    "    print(\"\\nEvaluating on test set...\")\n",
    "    test_loss, test_accuracy, test_f1 = evaluate_model(\n",
    "        model=model,\n",
    "        test_loader=test_loader,\n",
    "        criterion=criterion,\n",
    "        device=device,\n",
    "        label_list=list(idx_to_label.values())\n",
    "    )\n",
    "\n",
    "\n",
    "    results = {\n",
    "        \"test_loss\": test_loss,\n",
    "        \"test_accuracy\": test_accuracy,\n",
    "        \"test_f1\": test_f1,\n",
    "        \"embedding_dim\": EMBEDDING_DIM,\n",
    "        \"hidden_dim\": HIDDEN_DIM,\n",
    "        \"num_layers\": NUM_LAYERS,\n",
    "        \"dropout\": DROPOUT,\n",
    "        \"learning_rate\": LEARNING_RATE,\n",
    "        \"batch_size\": BATCH_SIZE,\n",
    "        \"epochs\": NUM_EPOCHS,\n",
    "        \"vocab_size\": len(vocabulary)\n",
    "    }\n",
    "\n",
    "\n",
    "    pd.DataFrame([results]).to_csv(\"sesotho_lstm_results.csv\", index=False)\n",
    "    print(f\"Results saved to sesotho_lstm_results.csv\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ],
   "id": "4524a7ff20c0a5be"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# We now create Baselines with pre-trained Multilingual transformers",
   "id": "d22a1be27cd62f5"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## AfroXLMR",
   "id": "f0d2d39d3c4fe95b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "!pip install -U transformers datasets peft evaluate plotly --quiet\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
    "from datasets import load_dataset\n",
    "from peft import LoraConfig, get_peft_model\n",
    "import numpy as np\n",
    "from sklearn.manifold import TSNE\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "# por_dataset=load_dataset(\"HausaNLP/AfriSenti-Twitter\", \"pt-MZ\")\n",
    "# swa_dataset=load_dataset(\"HausaNLP/AfriSenti-Twitter\", \"swa\")\n",
    "# sot_dataset=load_dataset(\"hamza-student-123/nlp-assignment-news-data\",'sot')\n",
    "#\n",
    "# for ds in [por_dataset,swa_dataset,sot_dataset]: # Change to all three later\n",
    "#     for lbl in [\"train\",\"validation\",\"test\"]:\n",
    "#         if ds[lbl].column_names[0]== \"tweet\":\n",
    "#             ds[lbl] = ds[lbl].rename_column(\"tweet\",\"text\")\n",
    "#         else:\n",
    "#             ds[lbl] = ds[lbl].rename_column(\"headline\",\"text\")\n",
    "#\n",
    "# por_df = por_dataset[\"train\"].to_pandas()\n",
    "# swa_df = swa_dataset[\"train\"].to_pandas()\n",
    "# sot_df = sot_dataset[\"train\"].to_pandas()\n"
   ],
   "id": "2f17a89a322dc762"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Baseline with [AfroXLMR](https://huggingface.co/Davlan/afro-xlmr-large)",
   "id": "d02b40ccb4a5095"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "!pip install -U transformers datasets peft evaluate plotly sentencepiece --quiet\n",
    "\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from transformers import XLMRobertaForSequenceClassification, XLMRobertaTokenizer\n",
    "from datasets import load_dataset\n",
    "# import torch_optimizer as optim\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, confusion_matrix\n",
    "# import numpy as np\n",
    "import pandas as pd\n",
    "# import matplotlib.pyplot as plt\n",
    "# import seaborn as sns\n",
    "# from tqdm import tqdm\n",
    "import logging\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "class SentimentDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_length=128):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = str(self.texts[idx])\n",
    "        label = self.labels[idx]\n",
    "\n",
    "        encoding = self.tokenizer(\n",
    "            text,\n",
    "            truncation=True,\n",
    "            padding='max_length',\n",
    "            max_length=self.max_length,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'label': torch.tensor(label, dtype=torch.long)\n",
    "        }\n",
    "\n",
    "def load_data(language):\n",
    "    try:\n",
    "        if language=='por':\n",
    "            chosen_dataset=load_dataset(\"HausaNLP/AfriSenti-Twitter\", \"por\",trust_remote_code=True)\n",
    "        elif language=='swa':\n",
    "            chosen_dataset=load_dataset(\"HausaNLP/AfriSenti-Twitter\", \"swa\",trust_remote_code=True)\n",
    "        elif language=='sot':\n",
    "            chosen_dataset=load_dataset(\"hamza-student-123/nlp-assignment-news-data\",'sot')\n",
    "        else:\n",
    "            raise Exception\n",
    "\n",
    "        for ds in [chosen_dataset]: # Change to all three later\n",
    "            for lbl in [\"train\",\"validation\",\"test\"]:\n",
    "                if ds[lbl].column_names[0]== \"tweet\":\n",
    "                    ds[lbl] = ds[lbl].rename_column(\"tweet\",\"text\")\n",
    "                else:\n",
    "                    ds[lbl] = ds[lbl].rename_column(\"headline\",\"text\")\n",
    "\n",
    "        train_df  = chosen_dataset[\"train\"].to_pandas()\n",
    "        val_df  = chosen_dataset[\"validation\"].to_pandas()\n",
    "        test_df  = chosen_dataset[\"test\"].to_pandas()\n",
    "        logger.info(f\"Data loaded successfully: {len(train_df)} training, {len(val_df)} validation, {len(test_df)} test examples\")\n",
    "        return train_df, val_df, test_df\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error loading data: {str(e)}\")\n",
    "        raise e\n",
    "\n",
    "# Function to create DataLoaders\n",
    "def create_data_loaders(train_df, val_df, test_df, tokenizer, batch_size=16, text_column='text', label_column='label'):\n",
    "    train_dataset = SentimentDataset(\n",
    "        texts=train_df[text_column].tolist(),\n",
    "        labels=train_df[label_column].tolist(),\n",
    "        tokenizer=tokenizer\n",
    "    )\n",
    "\n",
    "    val_dataset = SentimentDataset(\n",
    "        texts=val_df[text_column].tolist(),\n",
    "        labels=val_df[label_column].tolist(),\n",
    "        tokenizer=tokenizer\n",
    "    )\n",
    "\n",
    "    test_dataset = SentimentDataset(\n",
    "        texts=test_df[text_column].tolist(),\n",
    "        labels=test_df[label_column].tolist(),\n",
    "        tokenizer=tokenizer\n",
    "    )\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size)\n",
    "\n",
    "    return train_loader, val_loader, test_loader\n",
    "\n",
    "def train_epoch(model, dataloader, optimizer, scheduler, device):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "\n",
    "    progress_bar = tqdm(dataloader, desc=\"Training\")\n",
    "    for batch in progress_bar:\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['label'].to(device)\n",
    "\n",
    "        outputs = model(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            labels=labels\n",
    "        )\n",
    "\n",
    "        loss = outputs.loss\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "        progress_bar.set_postfix({\"loss\": f\"{loss.item():.4f}\"})\n",
    "\n",
    "    return epoch_loss / len(dataloader)\n",
    "\n",
    "def evaluate(model, dataloader, device):\n",
    "    \"\"\"\n",
    "    Returns:\n",
    "        Tuple of (loss, accuracy, precision, recall, f1)\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(dataloader, desc=\"Evaluating\"):\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['label'].to(device)\n",
    "\n",
    "            outputs = model(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                labels=labels\n",
    "            )\n",
    "\n",
    "            loss = outputs.loss\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            logits = outputs.logits\n",
    "            preds = torch.argmax(logits, dim=1).cpu().numpy()\n",
    "\n",
    "            all_preds.extend(preds)\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    # Calculate metrics\n",
    "    accuracy = accuracy_score(all_labels, all_preds)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(\n",
    "        all_labels, all_preds, average='weighted'\n",
    "    )\n",
    "\n",
    "    return total_loss / len(dataloader), accuracy, precision, recall, f1, all_preds, all_labels\n",
    "\n",
    "def plot_confusion_matrix(true_labels, predictions, class_names):\n",
    "    cm = confusion_matrix(true_labels, predictions)\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=class_names, yticklabels=class_names)\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('True')\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('confusion_matrix.png')\n",
    "    plt.close()\n",
    "\n",
    "def evaluate_afro_xlmr_for_lang(language):\n",
    "    config = {\n",
    "        'model_name': 'Davlan/afro-xlmr-base',\n",
    "        'num_labels': 3,\n",
    "        'batch_size': 16,\n",
    "        'learning_rate': 2e-5,\n",
    "        'epochs': 3,\n",
    "        'warmup_steps': 0,\n",
    "        'max_grad_norm': 1.0,\n",
    "        'text_column': 'text',\n",
    "        'label_column': 'label',\n",
    "        'class_names': ['negative', 'neutral', 'positive']\n",
    "    }\n",
    "\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    logger.info(f\"Using device: {device}\")\n",
    "\n",
    "    logger.info(f\"Loading model: {config['model_name']}...\")\n",
    "    tokenizer = XLMRobertaTokenizer.from_pretrained(config['model_name'])\n",
    "    model = XLMRobertaForSequenceClassification.from_pretrained(\n",
    "        config['model_name'],\n",
    "        num_labels=config['num_labels']\n",
    "    )\n",
    "    model.to(device)\n",
    "\n",
    "    # Load data\n",
    "    logger.info(\"Loading data...\")\n",
    "    train_df, val_df, test_df = load_data(f'{language}')\n",
    "\n",
    "    # Create data loaders\n",
    "    logger.info(\"Creating data loaders...\")\n",
    "    train_loader, val_loader, test_loader = create_data_loaders(\n",
    "        train_df, val_df, test_df,\n",
    "        tokenizer,\n",
    "        batch_size=config['batch_size'],\n",
    "        text_column=config['text_column'],\n",
    "        label_column=config['label_column']\n",
    "    )\n",
    "\n",
    "\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=config['learning_rate'])\n",
    "    total_steps = len(train_loader) * config['epochs']\n",
    "    scheduler = get_linear_schedule_with_warmup(\n",
    "        optimizer,\n",
    "        num_warmup_steps=config['warmup_steps'],\n",
    "        num_training_steps=total_steps\n",
    "    )\n",
    "\n",
    "    logger.info(\"Starting training...\")\n",
    "    best_val_f1 = 0\n",
    "    best_model_state = None\n",
    "\n",
    "    for epoch in range(config['epochs']):\n",
    "        logger.info(f\"Epoch {epoch + 1}/{config['epochs']}\")\n",
    "\n",
    "        # Train\n",
    "        start_time = time.time()\n",
    "        train_loss = train_epoch(model, train_loader,optimizer,scheduler, device) # train_epoch(model, train_loader, scheduler, device)\n",
    "        train_time = time.time() - start_time\n",
    "\n",
    "        # Validate\n",
    "        val_loss, val_accuracy, val_precision, val_recall, val_f1, _, _ = evaluate(model, val_loader, device)\n",
    "\n",
    "        logger.info(f\"Epoch {epoch + 1} results:\")\n",
    "        logger.info(f\"Train Loss: {train_loss:.4f}, Time: {train_time:.2f}s\")\n",
    "        logger.info(f\"Val Loss: {val_loss:.4f}, Accuracy: {val_accuracy:.4f}, Precision: {val_precision:.4f}, Recall: {val_recall:.4f}, F1: {val_f1:.4f}\")\n",
    "\n",
    "        # Save best model\n",
    "        if val_f1 > best_val_f1:\n",
    "            best_val_f1 = val_f1\n",
    "            best_model_state = model.state_dict().copy()\n",
    "            logger.info(f\"New best model with F1: {best_val_f1:.4f}\")\n",
    "\n",
    "    # Load best model for testing\n",
    "    if best_model_state:\n",
    "        logger.info(\"Loading best model for testing...\")\n",
    "        model.load_state_dict(best_model_state)\n",
    "\n",
    "    # Test evaluation\n",
    "    logger.info(\"Evaluating on test set...\")\n",
    "    test_loss, test_accuracy, test_precision, test_recall, test_f1, test_preds, test_labels = evaluate(model, test_loader, device)\n",
    "\n",
    "    logger.info(f\"Test Results:\")\n",
    "    logger.info(f\"Loss: {test_loss:.4f}\")\n",
    "    logger.info(f\"Accuracy: {test_accuracy:.4f}\")\n",
    "    logger.info(f\"Precision: {test_precision:.4f}\")\n",
    "    logger.info(f\"Recall: {test_recall:.4f}\")\n",
    "    logger.info(f\"F1 Score: {test_f1:.4f}\")\n",
    "\n",
    "    results_df_afro = {\n",
    "    \"test_loss\": test_loss,\n",
    "    \"test_accuracy\": test_accuracy,\n",
    "    \"test_f1\": test_f1,\n",
    "    \"test_precision\": test_precision,\n",
    "    \"test_recall\": test_recall,\n",
    "    \"epochs\": config['epochs'],\n",
    "    \"learning_rate\": config['learning_rate'],\n",
    "    \"batch_size\": config['batch_size'],\n",
    "}\n",
    "\n",
    "    path = f\"afroxlmr_results_{language}.csv\"\n",
    "    pd.DataFrame([results_df_afro]).to_csv(path, index=False)\n",
    "    print(f\"Results saved to {path}\")\n",
    "\n",
    "    # Save model\n",
    "    logger.info(\"Saving model...\")\n",
    "    model_save_path = f'./xmlr_sentiment_model_{language}'\n",
    "    model.save_pretrained(model_save_path)\n",
    "    tokenizer.save_pretrained(model_save_path)\n",
    "    logger.info(f\"Model saved to {model_save_path}\")\n",
    "\n",
    "# evaluate_afro_xlmr()\n",
    "langs = ['por','swa']\n",
    "# langs = ['sot']\n",
    "for l in langs:\n",
    "    evaluate_afro_xlmr_for_lang(l)"
   ],
   "id": "597bc55152aa1ee3"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "##### Evaluation for Sesotho",
   "id": "f9dae98057b88b54"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "!pip install -U transformers datasets peft evaluate plotly sentencepiece --quiet\n",
    "\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from transformers import XLMRobertaForSequenceClassification, XLMRobertaTokenizer\n",
    "from datasets import load_dataset\n",
    "# import torch_optimizer as optim\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, confusion_matrix\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import logging\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "class SentimentDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_length=128):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        self.label_mapping = {\"negative\": 0, \"neutral\": 1, \"positive\": 2}  # Label conversion\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = str(self.texts[idx])\n",
    "        label = self.label_mapping[self.labels[idx]]\n",
    "\n",
    "        encoding = self.tokenizer(\n",
    "            text,\n",
    "            truncation=True,\n",
    "            padding='max_length',\n",
    "            max_length=self.max_length,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'label': torch.tensor(label, dtype=torch.long)\n",
    "        }\n",
    "\n",
    "def load_data(language):\n",
    "    try:\n",
    "        if language=='por':\n",
    "            chosen_dataset=load_dataset(\"HausaNLP/AfriSenti-Twitter\", \"por\",trust_remote_code=True)\n",
    "        elif language=='swa':\n",
    "            chosen_dataset=load_dataset(\"HausaNLP/AfriSenti-Twitter\", \"swa\",trust_remote_code=True)\n",
    "        elif language=='sot':\n",
    "            chosen_dataset=load_dataset(\"hamza-student-123/nlp-assignment-news-data\",'sot')\n",
    "        else:\n",
    "            raise Exception\n",
    "\n",
    "        for ds in [chosen_dataset]: # Change to all three later\n",
    "            for lbl in [\"train\",\"validation\",\"test\"]:\n",
    "                if ds[lbl].column_names[0]== \"tweet\":\n",
    "                    ds[lbl] = ds[lbl].rename_column(\"tweet\",\"text\")\n",
    "                else:\n",
    "                    ds[lbl] = ds[lbl].rename_column(\"headline\",\"text\")\n",
    "\n",
    "        train_df  = chosen_dataset[\"train\"].to_pandas()\n",
    "        val_df  = chosen_dataset[\"validation\"].to_pandas()\n",
    "        test_df  = chosen_dataset[\"test\"].to_pandas()\n",
    "        logger.info(f\"Data loaded successfully: {len(train_df)} training, {len(val_df)} validation, {len(test_df)} test examples\")\n",
    "        return train_df, val_df, test_df\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error loading data: {str(e)}\")\n",
    "        raise e\n",
    "\n",
    "# Function to create DataLoaders\n",
    "def create_data_loaders(train_df, val_df, test_df, tokenizer, batch_size=16, text_column='text', label_column='label'):\n",
    "    train_dataset = SentimentDataset(\n",
    "        texts=train_df[text_column].tolist(),\n",
    "        labels=train_df[label_column].tolist(),\n",
    "        tokenizer=tokenizer\n",
    "    )\n",
    "\n",
    "    val_dataset = SentimentDataset(\n",
    "        texts=val_df[text_column].tolist(),\n",
    "        labels=val_df[label_column].tolist(),\n",
    "        tokenizer=tokenizer\n",
    "    )\n",
    "\n",
    "    test_dataset = SentimentDataset(\n",
    "        texts=test_df[text_column].tolist(),\n",
    "        labels=test_df[label_column].tolist(),\n",
    "        tokenizer=tokenizer\n",
    "    )\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size)\n",
    "\n",
    "    return train_loader, val_loader, test_loader\n",
    "\n",
    "def train_epoch(model, dataloader, optimizer, scheduler, device):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "\n",
    "    progress_bar = tqdm(dataloader, desc=\"Training\")\n",
    "    for batch in progress_bar:\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['label'].to(device)\n",
    "\n",
    "        outputs = model(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            labels=labels\n",
    "        )\n",
    "\n",
    "        loss = outputs.loss\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "        progress_bar.set_postfix({\"loss\": f\"{loss.item():.4f}\"})\n",
    "\n",
    "    return epoch_loss / len(dataloader)\n",
    "\n",
    "def evaluate(model, dataloader, device):\n",
    "    \"\"\"\n",
    "    Returns:\n",
    "        Tuple of (loss, accuracy, precision, recall, f1)\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(dataloader, desc=\"Evaluating\"):\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['label'].to(device)\n",
    "\n",
    "            outputs = model(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                labels=labels\n",
    "            )\n",
    "\n",
    "            loss = outputs.loss\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            logits = outputs.logits\n",
    "            preds = torch.argmax(logits, dim=1).cpu().numpy()\n",
    "\n",
    "            all_preds.extend(preds)\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    # Calculate metrics\n",
    "    accuracy = accuracy_score(all_labels, all_preds)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(\n",
    "        all_labels, all_preds, average='weighted'\n",
    "    )\n",
    "\n",
    "    return total_loss / len(dataloader), accuracy, precision, recall, f1, all_preds, all_labels\n",
    "\n",
    "def plot_confusion_matrix(true_labels, predictions, class_names):\n",
    "    cm = confusion_matrix(true_labels, predictions)\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=class_names, yticklabels=class_names)\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('True')\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('confusion_matrix.png')\n",
    "    plt.close()\n",
    "\n",
    "def evaluate_afro_xlmr_for_lang(language):\n",
    "    config = {\n",
    "        'model_name': 'Davlan/afro-xlmr-base',\n",
    "        'num_labels': 3,\n",
    "        'batch_size': 16,\n",
    "        'learning_rate': 2e-5,\n",
    "        'epochs': 3,\n",
    "        'warmup_steps': 0,\n",
    "        'max_grad_norm': 1.0,\n",
    "        'text_column': 'text',\n",
    "        'label_column': 'label',\n",
    "        'class_names': ['negative', 'neutral', 'positive']\n",
    "    }\n",
    "\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    logger.info(f\"Using device: {device}\")\n",
    "\n",
    "    logger.info(f\"Loading model: {config['model_name']}...\")\n",
    "    tokenizer = XLMRobertaTokenizer.from_pretrained(config['model_name'])\n",
    "    model = XLMRobertaForSequenceClassification.from_pretrained(\n",
    "        config['model_name'],\n",
    "        num_labels=config['num_labels']\n",
    "    )\n",
    "    model.to(device)\n",
    "\n",
    "    # Load data\n",
    "    logger.info(\"Loading data...\")\n",
    "    train_df, val_df, test_df = load_data(f'{language}')\n",
    "\n",
    "    # Create data loaders\n",
    "    logger.info(\"Creating data loaders...\")\n",
    "    train_loader, val_loader, test_loader = create_data_loaders(\n",
    "        train_df, val_df, test_df,\n",
    "        tokenizer,\n",
    "        batch_size=config['batch_size'],\n",
    "        text_column=config['text_column'],\n",
    "        label_column=config['label_column']\n",
    "    )\n",
    "\n",
    "\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=config['learning_rate'])\n",
    "    total_steps = len(train_loader) * config['epochs']\n",
    "    scheduler = get_linear_schedule_with_warmup(\n",
    "        optimizer,\n",
    "        num_warmup_steps=config['warmup_steps'],\n",
    "        num_training_steps=total_steps\n",
    "    )\n",
    "\n",
    "    logger.info(\"Starting training...\")\n",
    "    best_val_f1 = 0\n",
    "    best_model_state = None\n",
    "\n",
    "    for epoch in range(config['epochs']):\n",
    "        logger.info(f\"Epoch {epoch + 1}/{config['epochs']}\")\n",
    "\n",
    "        # Train\n",
    "        start_time = time.time()\n",
    "        train_loss = train_epoch(model, train_loader,optimizer,scheduler, device) # train_epoch(model, train_loader, scheduler, device)\n",
    "        train_time = time.time() - start_time\n",
    "\n",
    "        # Validate\n",
    "        val_loss, val_accuracy, val_precision, val_recall, val_f1, _, _ = evaluate(model, val_loader, device)\n",
    "\n",
    "        logger.info(f\"Epoch {epoch + 1} results:\")\n",
    "        logger.info(f\"Train Loss: {train_loss:.4f}, Time: {train_time:.2f}s\")\n",
    "        logger.info(f\"Val Loss: {val_loss:.4f}, Accuracy: {val_accuracy:.4f}, Precision: {val_precision:.4f}, Recall: {val_recall:.4f}, F1: {val_f1:.4f}\")\n",
    "\n",
    "        # Save best model\n",
    "        if val_f1 > best_val_f1:\n",
    "            best_val_f1 = val_f1\n",
    "            best_model_state = model.state_dict().copy()\n",
    "            logger.info(f\"New best model with F1: {best_val_f1:.4f}\")\n",
    "\n",
    "    # Load best model for testing\n",
    "    if best_model_state:\n",
    "        logger.info(\"Loading best model for testing...\")\n",
    "        model.load_state_dict(best_model_state)\n",
    "\n",
    "    # Test evaluation\n",
    "    logger.info(\"Evaluating on test set...\")\n",
    "    test_loss, test_accuracy, test_precision, test_recall, test_f1, test_preds, test_labels = evaluate(model, test_loader, device)\n",
    "\n",
    "    logger.info(f\"Test Results:\")\n",
    "    logger.info(f\"Loss: {test_loss:.4f}\")\n",
    "    logger.info(f\"Accuracy: {test_accuracy:.4f}\")\n",
    "    logger.info(f\"Precision: {test_precision:.4f}\")\n",
    "    logger.info(f\"Recall: {test_recall:.4f}\")\n",
    "    logger.info(f\"F1 Score: {test_f1:.4f}\")\n",
    "\n",
    "    results_df_afro = {\n",
    "    \"test_loss\": test_loss,\n",
    "    \"test_accuracy\": test_accuracy,\n",
    "    \"test_f1\": test_f1,\n",
    "    \"test_precision\": test_precision,\n",
    "    \"test_recall\": test_recall,\n",
    "    \"epochs\": config['epochs'],\n",
    "    \"learning_rate\": config['learning_rate'],\n",
    "    \"batch_size\": config['batch_size'],\n",
    "}\n",
    "\n",
    "    path = f\"afroxlmr_results_{language}.csv\"\n",
    "    pd.DataFrame([results_df_afro]).to_csv(path, index=False)\n",
    "    print(f\"Results saved to {path}\")\n",
    "\n",
    "    # Save model\n",
    "    logger.info(\"Saving model...\")\n",
    "    model_save_path = f'./xmlr_sentiment_model_{language}'\n",
    "    model.save_pretrained(model_save_path)\n",
    "    tokenizer.save_pretrained(model_save_path)\n",
    "    logger.info(f\"Model saved to {model_save_path}\")\n",
    "\n",
    "langs = ['sot']\n",
    "for l in langs:\n",
    "    evaluate_afro_xlmr_for_lang(l)"
   ],
   "id": "c02e542c5c0bc7d1"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## mBERT",
   "id": "69bcd45ec3108994"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Baseline with [mBERT](https://huggingface.co/google-bert/bert-base-multilingual-cased)\n",
   "id": "fcfb23c36f59178c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import pandas as pd\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments\n",
    "from datasets import load_dataset, Dataset, DatasetDict\n",
    "import torch\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "import os\n",
    "import time\n",
    "import json\n",
    "import random\n",
    "from lime.lime_text import LimeTextExplainer\n",
    "import torch.nn.functional as F\n",
    "\n",
    "run_timestamp = time.strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "os.makedirs(\"./benchmark_results\", exist_ok=True)\n",
    "os.makedirs(\"./lime_explanations\", exist_ok=True)\n",
    "\n",
    "output_csv = os.path.abspath(f\"./benchmark_results/mbert_benchmark_results_{run_timestamp}.csv\")\n",
    "explanations_dir = os.path.abspath(f\"./lime_explanations/explanations_{run_timestamp}\")\n",
    "os.makedirs(explanations_dir, exist_ok=True)\n",
    "\n",
    "results_df = pd.DataFrame(columns=[\n",
    "    \"Dataset\", \"Model\", \"Loss\", \"Accuracy\", \"F1\", \"Precision\", \"Recall\"\n",
    "])\n",
    "\n",
    "try:\n",
    "    swa_dataset = load_dataset(\"masakhane/afrisenti\", \"swa\")\n",
    "except Exception as e:\n",
    "    swa_dataset = None\n",
    "\n",
    "try:\n",
    "    por_dataset = load_dataset(\"masakhane/afrisenti\", \"por\")\n",
    "except Exception as e:\n",
    "    por_dataset = None\n",
    "\n",
    "try:\n",
    "    sot_dataset = load_dataset(\"csv\", data_files={\"train\": \"./datasets/sotho-news/sotho_news_dataset.csv\"})\n",
    "except Exception as e:\n",
    "    sot_dataset = None\n",
    "\n",
    "def compute_metrics(pred):\n",
    "    labels = pred.label_ids\n",
    "    preds = pred.predictions.argmax(-1)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='weighted')\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    return {\n",
    "        'accuracy': acc,\n",
    "        'f1': f1,\n",
    "        'precision': precision,\n",
    "        'recall': recall\n",
    "    }\n",
    "\n",
    "class ModelPredictor:\n",
    "    \"\"\"Wrapper class for LIME explanations\"\"\"\n",
    "    def __init__(self, model, tokenizer, device, num_labels):\n",
    "        self.model = model\n",
    "        self.tokenizer = tokenizer\n",
    "        self.device = device\n",
    "        self.num_labels = num_labels\n",
    "        self.model.eval()\n",
    "    \n",
    "    def predict_proba(self, texts):\n",
    "        \"\"\"Predict probabilities for LIME\"\"\"\n",
    "        if isinstance(texts, str):\n",
    "            texts = [texts]\n",
    "        \n",
    "        inputs = self.tokenizer(\n",
    "            texts,\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            max_length=128,\n",
    "            return_tensors=\"pt\"\n",
    "        ).to(self.device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = self.model(**inputs)\n",
    "            probas = F.softmax(outputs.logits, dim=-1)\n",
    "        \n",
    "        return probas.cpu().numpy()\n",
    "\n",
    "def generate_lime_explanations(model_predictor, test_texts, test_labels, dataset_name, \n",
    "                              label_names=None, num_samples=10):\n",
    "    \"\"\"Generate LIME explanations for sample predictions\"\"\"\n",
    "    \n",
    "    explainer = LimeTextExplainer(class_names=label_names or [f\"Class_{i}\" for i in range(3)])\n",
    "    \n",
    "    sample_indices = random.sample(range(len(test_texts)), min(num_samples, len(test_texts)))\n",
    "    explanations_data = []\n",
    "    \n",
    "    print(f\"\\nGenerating LIME explanations for {dataset_name}...\")\n",
    "    \n",
    "    for i, idx in enumerate(sample_indices):\n",
    "        try:\n",
    "            text = test_texts[idx]\n",
    "            true_label = test_labels[idx]\n",
    "            \n",
    "            pred_proba = model_predictor.predict_proba([text])[0]\n",
    "            pred_label = np.argmax(pred_proba)\n",
    "            \n",
    "            exp = explainer.explain_instance(\n",
    "                text, \n",
    "                model_predictor.predict_proba, \n",
    "                num_features=10,\n",
    "                num_samples=1000\n",
    "            )\n",
    "            \n",
    "            explanation_data = {\n",
    "                'sample_id': idx,\n",
    "                'text': text,\n",
    "                'true_label': int(true_label),\n",
    "                'predicted_label': int(pred_label),\n",
    "                'prediction_probability': float(pred_proba[pred_label]),\n",
    "                'all_probabilities': pred_proba.tolist(),\n",
    "                'lime_explanation': []\n",
    "            }\n",
    "            \n",
    "            for feature, importance in exp.as_list():\n",
    "                explanation_data['lime_explanation'].append({\n",
    "                    'feature': feature,\n",
    "                    'importance': float(importance)\n",
    "                })\n",
    "            \n",
    "            explanations_data.append(explanation_data)\n",
    "            \n",
    "            html_file = os.path.join(explanations_dir, f\"{dataset_name}_sample_{idx}_explanation.html\")\n",
    "            exp.save_to_file(html_file)\n",
    "            \n",
    "            print(f\"  Generated explanation {i+1}/{len(sample_indices)} for sample {idx}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"  Error generating explanation for sample {idx}: {str(e)}\")\n",
    "            continue\n",
    "    \n",
    "    # Save explanations as JSON\n",
    "    json_file = os.path.join(explanations_dir, f\"{dataset_name}_lime_explanations.json\")\n",
    "    with open(json_file, 'w', encoding='utf-8') as f:\n",
    "        json.dump(explanations_data, f, indent=2, ensure_ascii=False)\n",
    "    \n",
    "    return explanations_data\n",
    "\n",
    "def get_benchmark_metrics(dataset_name, dataset, num_labels):\n",
    "\n",
    "    if dataset is None:\n",
    "        return None, None\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"bert-base-multilingual-cased\")\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(\n",
    "        \"bert-base-multilingual-cased\",\n",
    "        num_labels=num_labels\n",
    "    ).to(device)\n",
    "\n",
    "    text_column = None\n",
    "    label_column = None\n",
    "\n",
    "    text_candidates = [\"text\", \"content\", \"tweet\", \"sentence\", \"document\"]\n",
    "    for split in dataset:\n",
    "        columns = dataset[split].column_names\n",
    "        for candidate in text_candidates:\n",
    "            if candidate in columns:\n",
    "                text_column = candidate\n",
    "                break\n",
    "        if text_column:\n",
    "            break\n",
    "\n",
    "    label_candidates = [\"label\", \"sentiment\", \"class\", \"category\"]\n",
    "    for split in dataset:\n",
    "        columns = dataset[split].column_names\n",
    "        for candidate in label_candidates:\n",
    "            if candidate in columns:\n",
    "                label_column = candidate\n",
    "                break\n",
    "        if label_column:\n",
    "            break\n",
    "\n",
    "    if text_column is None:\n",
    "        for split in dataset:\n",
    "            for col in dataset[split].column_names:\n",
    "                if isinstance(dataset[split][col][0], str):\n",
    "                    text_column = col\n",
    "                    break\n",
    "            if text_column:\n",
    "                break\n",
    "\n",
    "    if text_column is None:\n",
    "        return None, None\n",
    "\n",
    "    if label_column is None:\n",
    "        return None, None\n",
    "\n",
    "    required_splits = [\"train\", \"validation\", \"test\"]\n",
    "    missing_splits = [split for split in required_splits if split not in dataset]\n",
    "\n",
    "    if missing_splits:\n",
    "        if \"train\" in dataset:\n",
    "            train_valid_test = {}\n",
    "\n",
    "            for split in dataset:\n",
    "                if split in required_splits:\n",
    "                    train_valid_test[split] = dataset[split]\n",
    "\n",
    "            if \"train\" in dataset and (\"validation\" not in train_valid_test or \"test\" not in train_valid_test):\n",
    "                if \"validation\" not in train_valid_test:\n",
    "                    if \"test\" not in train_valid_test:\n",
    "                        split_datasets = dataset[\"train\"].train_test_split(test_size=0.2, seed=42)\n",
    "                        test_valid_split = split_datasets[\"test\"].train_test_split(test_size=0.5, seed=42)\n",
    "                        train_valid_test[\"train\"] = split_datasets[\"train\"]\n",
    "                        train_valid_test[\"validation\"] = test_valid_split[\"train\"]\n",
    "                        train_valid_test[\"test\"] = test_valid_split[\"test\"]\n",
    "                    else:\n",
    "                        split_datasets = dataset[\"train\"].train_test_split(test_size=0.1, seed=42)\n",
    "                        train_valid_test[\"train\"] = split_datasets[\"train\"]\n",
    "                        train_valid_test[\"validation\"] = split_datasets[\"test\"]\n",
    "                else:\n",
    "                    split_datasets = dataset[\"train\"].train_test_split(test_size=0.1, seed=42)\n",
    "                    train_valid_test[\"test\"] = split_datasets[\"test\"]\n",
    "\n",
    "            dataset = DatasetDict(train_valid_test)\n",
    "        else:\n",
    "            return None, None\n",
    "\n",
    "    label_mapping = None\n",
    "    label_names = None\n",
    "    for split in dataset:\n",
    "        if isinstance(dataset[split][label_column][0], str):\n",
    "            all_labels = set()\n",
    "            for example in dataset[split][label_column]:\n",
    "                all_labels.add(example)\n",
    "\n",
    "            sorted_labels = sorted(all_labels)\n",
    "            label_mapping = {label: i for i, label in enumerate(sorted_labels)}\n",
    "            label_names = sorted_labels\n",
    "            break\n",
    "\n",
    "    processed_dataset = DatasetDict()\n",
    "    for split_name, split_dataset in dataset.items():\n",
    "        texts = split_dataset[text_column]\n",
    "\n",
    "        if label_mapping:\n",
    "            labels = [label_mapping[label] for label in split_dataset[label_column]]\n",
    "        else:\n",
    "            labels = []\n",
    "            for label in split_dataset[label_column]:\n",
    "                if isinstance(label, (int, np.integer)):\n",
    "                    labels.append(int(label))\n",
    "                elif isinstance(label, str):\n",
    "                    try:\n",
    "                        labels.append(int(label))\n",
    "                    except ValueError:\n",
    "                        labels.append(0)\n",
    "                else:\n",
    "                    labels.append(0)\n",
    "\n",
    "        processed_dataset[split_name] = Dataset.from_dict({\n",
    "            \"text\": texts,\n",
    "            \"label\": labels\n",
    "        })\n",
    "\n",
    "    def tokenize_function(examples):\n",
    "        return tokenizer(\n",
    "            examples[\"text\"],\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            max_length=128,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "\n",
    "    tokenized_dataset = DatasetDict()\n",
    "    for split_name, split_dataset in processed_dataset.items():\n",
    "        tokenized_split = split_dataset.map(\n",
    "            tokenize_function,\n",
    "            batched=True,\n",
    "            remove_columns=[\"text\"]\n",
    "        )\n",
    "        tokenized_dataset[split_name] = tokenized_split\n",
    "\n",
    "        for required_col in [\"input_ids\", \"attention_mask\", \"label\"]:\n",
    "            if required_col not in tokenized_split.column_names:\n",
    "                return None, None\n",
    "\n",
    "    eval_args = TrainingArguments(\n",
    "        output_dir=f\"./benchmark_results/{dataset_name}_{run_timestamp}\",\n",
    "        per_device_eval_batch_size=16,\n",
    "        logging_dir=f\"./benchmark_results/{dataset_name}_{run_timestamp}/logs\",\n",
    "        report_to=\"none\",\n",
    "        remove_unused_columns=True\n",
    "    )\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=eval_args,\n",
    "        compute_metrics=compute_metrics,\n",
    "    )\n",
    "\n",
    "    try:\n",
    "        benchmark_results = trainer.evaluate(eval_dataset=tokenized_dataset[\"test\"])\n",
    "        \n",
    "        model_predictor = ModelPredictor(model, tokenizer, device, num_labels)\n",
    "        test_texts = processed_dataset[\"test\"][\"text\"]\n",
    "        test_labels = processed_dataset[\"test\"][\"label\"]\n",
    "        \n",
    "        lime_explanations = generate_lime_explanations(\n",
    "            model_predictor, test_texts, test_labels, dataset_name, \n",
    "            label_names=label_names, num_samples=10\n",
    "        )\n",
    "\n",
    "        return benchmark_results, lime_explanations\n",
    "    except Exception as e:\n",
    "        print(f\"Error in benchmarking {dataset_name}: {str(e)}\")\n",
    "        return None, None\n",
    "\n",
    "available_datasets = []\n",
    "if swa_dataset:\n",
    "    available_datasets.append((\"Swahili\", swa_dataset, 3))\n",
    "if por_dataset:\n",
    "    available_datasets.append((\"Portuguese\", por_dataset, 3))\n",
    "if sot_dataset:\n",
    "    available_datasets.append((\"Sesotho\", sot_dataset, 3))\n",
    "\n",
    "all_results = []\n",
    "all_explanations = {}\n",
    "\n",
    "print(\"Starting benchmarking with LIME explanations...\")\n",
    "\n",
    "for dataset_name, dataset, num_labels in available_datasets:\n",
    "    try:\n",
    "        print(f\"\\nProcessing {dataset_name} dataset...\")\n",
    "        results, explanations = get_benchmark_metrics(dataset_name, dataset, num_labels)\n",
    "\n",
    "        if results:\n",
    "            results_df = results_df._append({\n",
    "                \"Dataset\": dataset_name,\n",
    "                \"Model\": \"mBERT\",\n",
    "                \"Loss\": results.get(\"eval_loss\"),\n",
    "                \"Accuracy\": results.get(\"eval_accuracy\"),\n",
    "                \"F1\": results.get(\"eval_f1\"),\n",
    "                \"Precision\": results.get(\"eval_precision\"),\n",
    "                \"Recall\": results.get(\"eval_recall\")\n",
    "            }, ignore_index=True)\n",
    "\n",
    "            all_results.append({\"Dataset\": dataset_name, \"Results\": results})\n",
    "            if explanations:\n",
    "                all_explanations[dataset_name] = explanations\n",
    "        else:\n",
    "            print(f\"Failed to process {dataset_name} dataset\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {dataset_name}: {str(e)}\")\n",
    "\n",
    "try:\n",
    "    results_df.to_csv(output_csv, index=False)\n",
    "    print(f\"\\nResults saved to: {output_csv}\")\n",
    "except Exception as e:\n",
    "    emergency_path = f\"./emergency_results_{run_timestamp}.csv\"\n",
    "    results_df.to_csv(emergency_path, index=False)\n",
    "    print(f\"\\nEmergency results saved to: {emergency_path}\")\n",
    "\n",
    "if all_explanations:\n",
    "    summary_file = os.path.join(explanations_dir, \"explanations_summary.json\")\n",
    "    explanation_summary = {}\n",
    "    \n",
    "    for dataset_name, explanations in all_explanations.items():\n",
    "        summary_stats = {\n",
    "            'total_explanations': len(explanations),\n",
    "            'average_prediction_confidence': np.mean([exp['prediction_probability'] for exp in explanations]),\n",
    "            'correct_predictions': sum(1 for exp in explanations if exp['true_label'] == exp['predicted_label']),\n",
    "            'top_important_features': {}\n",
    "        }\n",
    "        \n",
    "        feature_importance = {}\n",
    "        for exp in explanations:\n",
    "            for feature_data in exp['lime_explanation']:\n",
    "                feature = feature_data['feature']\n",
    "                importance = abs(feature_data['importance'])\n",
    "                if feature in feature_importance:\n",
    "                    feature_importance[feature].append(importance)\n",
    "                else:\n",
    "                    feature_importance[feature] = [importance]\n",
    "        \n",
    "        avg_feature_importance = {\n",
    "            feature: np.mean(importances) \n",
    "            for feature, importances in feature_importance.items()\n",
    "        }\n",
    "        \n",
    "        sorted_features = sorted(avg_feature_importance.items(), key=lambda x: x[1], reverse=True)\n",
    "        summary_stats['top_important_features'] = dict(sorted_features[:10])\n",
    "        \n",
    "        explanation_summary[dataset_name] = summary_stats\n",
    "    \n",
    "    with open(summary_file, 'w', encoding='utf-8') as f:\n",
    "        json.dump(explanation_summary, f, indent=2, ensure_ascii=False)\n",
    "    \n",
    "\n",
    "\n",
    "print(\"\\nResults:\")\n",
    "print(results_df)"
   ],
   "id": "cb39467712760eff"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Fine-Tuning BERT\n",
   "id": "7e80d705d9d2355e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import pandas as pd\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments\n",
    "from datasets import load_dataset, Dataset, DatasetDict\n",
    "import torch\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "import os\n",
    "import logging\n",
    "import time\n",
    "import sys\n",
    "import json\n",
    "import shutil\n",
    "\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "    handlers=[\n",
    "        logging.StreamHandler(sys.stdout)\n",
    "    ]\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "try:\n",
    "    swa_dataset = load_dataset(\"masakhane/afrisenti\", \"swa\")\n",
    "    logger.info(\"Successfully loaded Swahili dataset from HuggingFace Hub\")\n",
    "except Exception as e:\n",
    "    logger.error(f\"Failed to load Swahili dataset: {e}\")\n",
    "    swa_dataset = None\n",
    "\n",
    "try:\n",
    "    por_dataset = load_dataset(\"masakhane/afrisenti\", \"por\")\n",
    "    logger.info(\"Successfully loaded Portuguese dataset from HuggingFace Hub\")\n",
    "except Exception as e:\n",
    "    logger.error(f\"Failed to load Portuguese dataset: {e}\")\n",
    "    por_dataset = None\n",
    "\n",
    "try:\n",
    "    sot_dataset = load_dataset(\"csv\", data_files={\"train\": \"./datasets/sotho-news/sotho_news_dataset.csv\"})\n",
    "    logger.info(\"Successfully loaded Sesotho dataset from CSV\")\n",
    "except Exception as e:\n",
    "    logger.error(f\"Failed to load Sesotho dataset: {e}\")\n",
    "    sot_dataset = None\n",
    "\n",
    "def compute_metrics(pred):\n",
    "    labels = pred.label_ids\n",
    "    preds = pred.predictions.argmax(-1)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='weighted')\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    return {\n",
    "        'accuracy': acc,\n",
    "        'f1': f1,\n",
    "        'precision': precision,\n",
    "        'recall': recall\n",
    "    }\n",
    "\n",
    "def finetune_and_evaluate(dataset_name, dataset, num_labels):\n",
    "    logger.info(f\"Starting fine-tuning for {dataset_name}\")\n",
    "\n",
    "    if dataset is None:\n",
    "        logger.error(f\"Dataset {dataset_name} is None, cannot proceed with fine-tuning\")\n",
    "        return None\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    logger.info(f\"Using device: {device}\")\n",
    "\n",
    "    model_save_dir = f\"./models/{dataset_name.lower()}\"\n",
    "    os.makedirs(model_save_dir, exist_ok=True)\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"bert-base-multilingual-cased\")\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(\n",
    "        \"bert-base-multilingual-cased\",\n",
    "        num_labels=num_labels\n",
    "    ).to(device)\n",
    "\n",
    "    logger.info(f\"Dataset {dataset_name} structure:\")\n",
    "    for split in dataset:\n",
    "        logger.info(f\"  - Split: {split}, Examples: {len(dataset[split])}\")\n",
    "        logger.info(f\"  - Features: {dataset[split].features}\")\n",
    "        logger.info(f\"  - Columns: {dataset[split].column_names}\")\n",
    "\n",
    "    text_column = None\n",
    "    label_column = None\n",
    "\n",
    "    text_candidates = [\"text\", \"content\", \"tweet\", \"sentence\", \"document\", \"headline\"]\n",
    "    for split in dataset:\n",
    "        columns = dataset[split].column_names\n",
    "        for candidate in text_candidates:\n",
    "            if candidate in columns:\n",
    "                text_column = candidate\n",
    "                break\n",
    "        if text_column:\n",
    "            break\n",
    "\n",
    "    label_candidates = [\"label\", \"sentiment\", \"class\", \"category\"]\n",
    "    for split in dataset:\n",
    "        columns = dataset[split].column_names\n",
    "        for candidate in label_candidates:\n",
    "            if candidate in columns:\n",
    "                label_column = candidate\n",
    "                break\n",
    "        if label_column:\n",
    "            break\n",
    "\n",
    "    logger.info(f\"For {dataset_name}, using text_column={text_column}, label_column={label_column}\")\n",
    "\n",
    "    if text_column is None:\n",
    "        for split in dataset:\n",
    "            for col in dataset[split].column_names:\n",
    "                if isinstance(dataset[split][col][0], str):\n",
    "                    text_column = col\n",
    "                    logger.info(f\"Using '{col}' as text column based on string data\")\n",
    "                    break\n",
    "            if text_column:\n",
    "                break\n",
    "\n",
    "    if text_column is None:\n",
    "        logger.error(f\"Could not identify a text column for {dataset_name}\")\n",
    "        return None\n",
    "\n",
    "    if label_column is None:\n",
    "        logger.error(f\"Could not identify a label column for {dataset_name}\")\n",
    "        return None\n",
    "\n",
    "    required_splits = [\"train\", \"validation\", \"test\"]\n",
    "    missing_splits = [split for split in required_splits if split not in dataset]\n",
    "\n",
    "    if missing_splits:\n",
    "        logger.info(f\"Creating missing splits: {missing_splits} for {dataset_name}\")\n",
    "        if \"train\" in dataset:\n",
    "            train_valid_test = {}\n",
    "\n",
    "            for split in dataset:\n",
    "                if split in required_splits:\n",
    "                    train_valid_test[split] = dataset[split]\n",
    "\n",
    "            if \"train\" in dataset and (\"validation\" not in train_valid_test or \"test\" not in train_valid_test):\n",
    "                if \"validation\" not in train_valid_test:\n",
    "                    if \"test\" not in train_valid_test:\n",
    "                        split_datasets = dataset[\"train\"].train_test_split(test_size=0.2, seed=42)\n",
    "                        test_valid_split = split_datasets[\"test\"].train_test_split(test_size=0.5, seed=42)\n",
    "                        train_valid_test[\"train\"] = split_datasets[\"train\"]\n",
    "                        train_valid_test[\"validation\"] = test_valid_split[\"train\"]\n",
    "                        train_valid_test[\"test\"] = test_valid_split[\"test\"]\n",
    "                    else:\n",
    "                        split_datasets = dataset[\"train\"].train_test_split(test_size=0.1, seed=42)\n",
    "                        train_valid_test[\"train\"] = split_datasets[\"train\"]\n",
    "                        train_valid_test[\"validation\"] = split_datasets[\"test\"]\n",
    "                else:\n",
    "                    split_datasets = dataset[\"train\"].train_test_split(test_size=0.1, seed=42)\n",
    "                    train_valid_test[\"test\"] = split_datasets[\"test\"]\n",
    "\n",
    "            dataset = DatasetDict(train_valid_test)\n",
    "        else:\n",
    "            logger.error(f\"Dataset {dataset_name} has no train split and cannot create splits\")\n",
    "            return None\n",
    "\n",
    "    label_mapping = None\n",
    "    for split in dataset:\n",
    "        if isinstance(dataset[split][label_column][0], str):\n",
    "            all_labels = set()\n",
    "            for example in dataset[split][label_column]:\n",
    "                all_labels.add(example)\n",
    "\n",
    "            label_mapping = {label: i for i, label in enumerate(sorted(all_labels))}\n",
    "            logger.info(f\"Created label mapping for {dataset_name}: {label_mapping}\")\n",
    "            break\n",
    "\n",
    "    processed_dataset = DatasetDict()\n",
    "    for split_name, split_dataset in dataset.items():\n",
    "        texts = split_dataset[text_column]\n",
    "\n",
    "        if label_mapping:\n",
    "            labels = [label_mapping[label] for label in split_dataset[label_column]]\n",
    "        else:\n",
    "            labels = []\n",
    "            for label in split_dataset[label_column]:\n",
    "                if isinstance(label, (int, np.integer)):\n",
    "                    labels.append(int(label))\n",
    "                elif isinstance(label, str):\n",
    "                    try:\n",
    "                        labels.append(int(label))\n",
    "                    except ValueError:\n",
    "                        logger.warning(f\"Unexpected string label found in {split_name}: {label}\")\n",
    "                        labels.append(0)\n",
    "                else:\n",
    "                    logger.warning(f\"Unexpected label type in {split_name}: {type(label)}\")\n",
    "                    labels.append(0)\n",
    "\n",
    "        processed_dataset[split_name] = Dataset.from_dict({\n",
    "            \"text\": texts,\n",
    "            \"label\": labels\n",
    "        })\n",
    "\n",
    "        logger.info(f\"Processed {split_name} split: {len(processed_dataset[split_name])} examples\")\n",
    "\n",
    "    def tokenize_function(examples):\n",
    "        return tokenizer(\n",
    "            examples[\"text\"],\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            max_length=128,\n",
    "        )\n",
    "\n",
    "    tokenized_dataset = DatasetDict()\n",
    "    for split_name, split_dataset in processed_dataset.items():\n",
    "        tokenized_split = split_dataset.map(\n",
    "            tokenize_function,\n",
    "            batched=True,\n",
    "            remove_columns=[\"text\"]\n",
    "        )\n",
    "        tokenized_dataset[split_name] = tokenized_split\n",
    "\n",
    "        logger.info(f\"Tokenized {split_name} split: {len(tokenized_split)} examples\")\n",
    "        logger.info(f\"Columns after tokenization: {tokenized_split.column_names}\")\n",
    "\n",
    "        for required_col in [\"input_ids\", \"attention_mask\", \"label\"]:\n",
    "            if required_col not in tokenized_split.column_names:\n",
    "                logger.error(f\"Required column {required_col} missing after tokenization\")\n",
    "                return None\n",
    "\n",
    "    model_output_dir = f\"./tmp_model_dir_{dataset_name}\"\n",
    "    if os.path.exists(model_output_dir):\n",
    "        shutil.rmtree(model_output_dir)\n",
    "    os.makedirs(model_output_dir, exist_ok=True)\n",
    "\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=model_output_dir,\n",
    "        learning_rate=2e-5,\n",
    "        per_device_train_batch_size=16,\n",
    "        per_device_eval_batch_size=16,\n",
    "        num_train_epochs=3,\n",
    "        weight_decay=0.01,\n",
    "        logging_dir=\"./tmp_logs\",\n",
    "        logging_steps=10,\n",
    "        save_strategy=\"no\",\n",
    "        save_steps=1000000,\n",
    "        eval_steps=100,\n",
    "        do_eval=True,\n",
    "    )\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=tokenized_dataset[\"train\"],\n",
    "        eval_dataset=tokenized_dataset[\"validation\"],\n",
    "        compute_metrics=compute_metrics,\n",
    "    )\n",
    "\n",
    "    logger.info(f\"Starting fine-tuning {dataset_name} with {len(tokenized_dataset['train'])} examples\")\n",
    "    start_time = time.time()\n",
    "\n",
    "    try:\n",
    "        try:\n",
    "            train_output = trainer.train()\n",
    "\n",
    "            training_loss = None\n",
    "            try:\n",
    "                if hasattr(train_output, \"metrics\") and \"loss\" in train_output.metrics:\n",
    "                    training_loss = train_output.metrics[\"loss\"]\n",
    "                elif isinstance(train_output, dict) and \"loss\" in train_output:\n",
    "                    training_loss = train_output[\"loss\"]\n",
    "\n",
    "                if training_loss is None and hasattr(trainer, \"state\"):\n",
    "                    if hasattr(trainer.state, \"log_history\") and trainer.state.log_history:\n",
    "                        for log in reversed(trainer.state.log_history):\n",
    "                            if \"loss\" in log:\n",
    "                                training_loss = log[\"loss\"]\n",
    "                                break\n",
    "            except Exception as e:\n",
    "                logger.warning(f\"Could not extract training loss: {e}\")\n",
    "        except RuntimeError as e:\n",
    "            if \"PytorchStreamWriter failed writing file\" in str(e):\n",
    "                logger.warning(\"Caught PyTorch serialization error during training. Will proceed with model saving.\")\n",
    "\n",
    "            else:\n",
    "\n",
    "                raise\n",
    "\n",
    "        try:\n",
    "            model_path = os.path.join(model_save_dir, \"model.pt\")\n",
    "            torch.save(model.state_dict(), model_path)\n",
    "            logger.info(f\"Model saved to {model_path}\")\n",
    "        except RuntimeError as e:\n",
    "            if \"PytorchStreamWriter failed writing file\" in str(e):\n",
    "                logger.warning(f\"PyTorch serialization error when saving model state dict. Trying alternative method.\")\n",
    "\n",
    "                try:\n",
    "\n",
    "                    with open(model_path, 'wb') as f:\n",
    "                        torch.save(model.state_dict(), f, _use_new_zipfile_serialization=False)\n",
    "                    logger.info(f\"Model saved to {model_path} using legacy serialization\")\n",
    "                except Exception as e2:\n",
    "                    logger.error(f\"Failed to save model with alternative method: {e2}\")\n",
    "            else:\n",
    "\n",
    "                raise\n",
    "\n",
    "        tokenizer.save_pretrained(model_save_dir)\n",
    "        logger.info(f\"Tokenizer saved to {model_save_dir}\")\n",
    "\n",
    "        if label_mapping:\n",
    "            label_mapping_path = os.path.join(model_save_dir, \"label_mapping.json\")\n",
    "            with open(label_mapping_path, \"w\") as f:\n",
    "\n",
    "                json_mapping = {str(k): int(v) for k, v in label_mapping.items()}\n",
    "                json.dump(json_mapping, f, indent=2)\n",
    "            logger.info(f\"Label mapping saved to {label_mapping_path}\")\n",
    "\n",
    "        model_config = {\n",
    "            \"base_model\": \"bert-base-multilingual-cased\",\n",
    "            \"num_labels\": num_labels,\n",
    "            \"text_column\": text_column,\n",
    "            \"label_column\": label_column,\n",
    "            \"max_length\": 128,\n",
    "\n",
    "            \"dataset_name\": dataset_name,\n",
    "        }\n",
    "\n",
    "        config_path = os.path.join(model_save_dir, \"config.json\")\n",
    "        with open(config_path, \"w\") as f:\n",
    "            json.dump(model_config, f, indent=2)\n",
    "        logger.info(f\"Model config saved to {config_path}\")\n",
    "\n",
    "        training_time = time.time() - start_time\n",
    "        logger.info(f\"Training completed in {training_time:.2f} seconds\")\n",
    "\n",
    "        logger.info(f\"Evaluating on validation set with {len(tokenized_dataset['validation'])} examples\")\n",
    "        validation_results = trainer.evaluate(eval_dataset=tokenized_dataset[\"validation\"])\n",
    "\n",
    "        logger.info(f\"Evaluating on test set with {len(tokenized_dataset['test'])} examples\")\n",
    "        test_results = trainer.evaluate(eval_dataset=tokenized_dataset[\"test\"])\n",
    "\n",
    "\n",
    "        processed_validation = {}\n",
    "        for k, v in validation_results.items():\n",
    "            if isinstance(v, (int, float, str, bool)) or v is None:\n",
    "                processed_validation[k] = v\n",
    "            else:\n",
    "                processed_validation[k] = float(v)\n",
    "\n",
    "        processed_test = {}\n",
    "        for k, v in test_results.items():\n",
    "            if isinstance(v, (int, float, str, bool)) or v is None:\n",
    "                processed_test[k] = v\n",
    "            else:\n",
    "                processed_test[k] = float(v)\n",
    "\n",
    "        metrics = {\n",
    "            \"training_time\": float(training_time),\n",
    "            \"training_loss\": float(training_loss) if training_loss is not None else None,\n",
    "            \"validation_results\": processed_validation,\n",
    "            \"test_results\": processed_test,\n",
    "        }\n",
    "\n",
    "        metrics_path = os.path.join(model_save_dir, \"metrics.json\")\n",
    "        with open(metrics_path, \"w\") as f:\n",
    "            json.dump(metrics, f, indent=2)\n",
    "        logger.info(f\"Model metrics saved to {metrics_path}\")\n",
    "\n",
    "        logger.info(f\"============== RESULTS FOR {dataset_name} ==============\")\n",
    "        logger.info(f\"Training loss: {training_loss}\")\n",
    "        logger.info(f\"Validation results: {validation_results}\")\n",
    "        logger.info(f\"Test results: {test_results}\")\n",
    "\n",
    "        return {\n",
    "            \"dataset\": dataset_name,\n",
    "            \"training_time\": training_time,\n",
    "            \"training_loss\": training_loss,\n",
    "            \"validation_results\": validation_results,\n",
    "            \"test_results\": test_results,\n",
    "            \"model_path\": model_path,\n",
    "        }\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Fine-tuning failed for {dataset_name}: {e}\", exc_info=True)\n",
    "        return None\n",
    "\n",
    "available_datasets = []\n",
    "if swa_dataset:\n",
    "    available_datasets.append((\"Swahili\", swa_dataset, 3))\n",
    "if por_dataset:\n",
    "    available_datasets.append((\"Portuguese\", por_dataset, 3))\n",
    "if sot_dataset:\n",
    "    available_datasets.append((\"Sesotho\", sot_dataset, 3))\n",
    "\n",
    "logger.info(f\"Available datasets: {[name for name, _, _ in available_datasets]}\")\n",
    "\n",
    "all_results = []\n",
    "for dataset_name, dataset, num_labels in available_datasets:\n",
    "    logger.info(f\"\\n==== FINE-TUNING ON {dataset_name.upper()} DATASET ====\")\n",
    "    try:\n",
    "        results = finetune_and_evaluate(dataset_name, dataset, num_labels)\n",
    "\n",
    "        if results:\n",
    "            all_results.append(results)\n",
    "        else:\n",
    "            logger.warning(f\"No results returned for {dataset_name}\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Unexpected error during fine-tuning {dataset_name}: {e}\", exc_info=True)\n",
    "\n",
    "logger.info(\"\\n==== FINE-TUNING COMPLETE ====\")\n",
    "\n",
    "print(f\"{'Dataset':<12} | {'Acc (val)':<10} | {'F1 (val)':<10} | {'Acc (test)':<10} | {'F1 (test)':<10} | {'Model Dir':<20}\")\n",
    "\n",
    "for result in all_results:\n",
    "    dataset = result[\"dataset\"]\n",
    "    val_acc = result[\"validation_results\"].get(\"eval_accuracy\", float('nan'))\n",
    "    val_f1 = result[\"validation_results\"].get(\"eval_f1\", float('nan'))\n",
    "    test_acc = result[\"test_results\"].get(\"eval_accuracy\", float('nan'))\n",
    "    test_f1 = result[\"test_results\"].get(\"eval_f1\", float('nan'))\n",
    "    model_dir = f\"./models/{dataset.lower()}\"\n",
    "\n",
    "    print(f\"{dataset:<12} | {val_acc:<10.4f} | {val_f1:<10.4f} | {test_acc:<10.4f} | {test_f1:<10.4f} | {model_dir}\")"
   ],
   "id": "4e3b8ccf024595a5"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## XLM-RoBERTa",
   "id": "3fd29af297c21306"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Baseline with [XLM-RoBERTa](https://huggingface.co/docs/transformers/en/model_doc/xlm-roberta)\n",
   "id": "871d9039ff19435e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import pandas as pd\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments\n",
    "from datasets import load_dataset, Dataset, DatasetDict\n",
    "import torch\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "import os\n",
    "import time\n",
    "\n",
    "run_timestamp = time.strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "os.makedirs(\"./benchmark_results\", exist_ok=True)\n",
    "\n",
    "output_csv = os.path.abspath(f\"./benchmark_results/xlmroberta_benchmark_results_{run_timestamp}.csv\")\n",
    "\n",
    "results_df = pd.DataFrame(columns=[\n",
    "    \"Dataset\", \"Model\", \"Loss\", \"Accuracy\", \"F1\", \"Precision\", \"Recall\"\n",
    "])\n",
    "\n",
    "try:\n",
    "    swa_dataset = load_dataset(\"masakhane/afrisenti\", \"swa\")\n",
    "except Exception as e:\n",
    "    swa_dataset = None\n",
    "\n",
    "try:\n",
    "    por_dataset = load_dataset(\"masakhane/afrisenti\", \"por\")\n",
    "except Exception as e:\n",
    "    por_dataset = None\n",
    "\n",
    "try:\n",
    "    sot_dataset = load_dataset(\"csv\", data_files={\"train\": \"./datasets/sotho-news/sotho_news_dataset.csv\"})\n",
    "except Exception as e:\n",
    "    sot_dataset = None\n",
    "    \n",
    "def compute_metrics(pred):\n",
    "    labels = pred.label_ids\n",
    "    preds = pred.predictions.argmax(-1)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='weighted')\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    return {\n",
    "        'accuracy': acc,\n",
    "        'f1': f1,\n",
    "        'precision': precision,\n",
    "        'recall': recall\n",
    "    }\n",
    "\n",
    "def get_benchmark_metrics(dataset_name, dataset, num_labels):\n",
    "    \n",
    "    if dataset is None:\n",
    "        return None\n",
    "        \n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"xlm-roberta-base\")\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(\n",
    "        \"xlm-roberta-base\", \n",
    "        num_labels=num_labels\n",
    "    ).to(device)\n",
    "    \n",
    "    text_column = None\n",
    "    label_column = None\n",
    "    \n",
    "    text_candidates = [\"text\", \"content\", \"tweet\", \"sentence\", \"document\"]\n",
    "    for split in dataset:\n",
    "        columns = dataset[split].column_names\n",
    "        for candidate in text_candidates:\n",
    "            if candidate in columns:\n",
    "                text_column = candidate\n",
    "                break\n",
    "        if text_column:\n",
    "            break\n",
    "    \n",
    "    label_candidates = [\"label\", \"sentiment\", \"class\", \"category\"]\n",
    "    for split in dataset:\n",
    "        columns = dataset[split].column_names\n",
    "        for candidate in label_candidates:\n",
    "            if candidate in columns:\n",
    "                label_column = candidate\n",
    "                break\n",
    "        if label_column:\n",
    "            break\n",
    "    \n",
    "    if text_column is None:\n",
    "        for split in dataset:\n",
    "            for col in dataset[split].column_names:\n",
    "                if isinstance(dataset[split][col][0], str):\n",
    "                    text_column = col\n",
    "                    break\n",
    "            if text_column:\n",
    "                break\n",
    "    \n",
    "    if text_column is None:\n",
    "        return None\n",
    "    \n",
    "    if label_column is None:\n",
    "        return None\n",
    "    \n",
    "    required_splits = [\"train\", \"validation\", \"test\"]\n",
    "    missing_splits = [split for split in required_splits if split not in dataset]\n",
    "    \n",
    "    if missing_splits:\n",
    "        if \"train\" in dataset:\n",
    "            train_valid_test = {}\n",
    "            \n",
    "            for split in dataset:\n",
    "                if split in required_splits:\n",
    "                    train_valid_test[split] = dataset[split]\n",
    "            \n",
    "            if \"train\" in dataset and (\"validation\" not in train_valid_test or \"test\" not in train_valid_test):\n",
    "                if \"validation\" not in train_valid_test:\n",
    "                    if \"test\" not in train_valid_test:\n",
    "                        split_datasets = dataset[\"train\"].train_test_split(test_size=0.2, seed=42)\n",
    "                        test_valid_split = split_datasets[\"test\"].train_test_split(test_size=0.5, seed=42)\n",
    "                        train_valid_test[\"train\"] = split_datasets[\"train\"]\n",
    "                        train_valid_test[\"validation\"] = test_valid_split[\"train\"]\n",
    "                        train_valid_test[\"test\"] = test_valid_split[\"test\"]\n",
    "                    else:\n",
    "                        split_datasets = dataset[\"train\"].train_test_split(test_size=0.1, seed=42)\n",
    "                        train_valid_test[\"train\"] = split_datasets[\"train\"]\n",
    "                        train_valid_test[\"validation\"] = split_datasets[\"test\"]\n",
    "                else:\n",
    "                    split_datasets = dataset[\"train\"].train_test_split(test_size=0.1, seed=42)\n",
    "                    train_valid_test[\"test\"] = split_datasets[\"test\"]\n",
    "            \n",
    "            dataset = DatasetDict(train_valid_test)\n",
    "        else:\n",
    "            return None\n",
    "    \n",
    "    label_mapping = None\n",
    "    for split in dataset:\n",
    "        if isinstance(dataset[split][label_column][0], str):\n",
    "            all_labels = set()\n",
    "            for example in dataset[split][label_column]:\n",
    "                all_labels.add(example)\n",
    "            \n",
    "            label_mapping = {label: i for i, label in enumerate(sorted(all_labels))}\n",
    "            break\n",
    "    \n",
    "    processed_dataset = DatasetDict()\n",
    "    for split_name, split_dataset in dataset.items():\n",
    "        texts = split_dataset[text_column]\n",
    "        \n",
    "        if label_mapping:\n",
    "            labels = [label_mapping[label] for label in split_dataset[label_column]]\n",
    "        else:\n",
    "            labels = []\n",
    "            for label in split_dataset[label_column]:\n",
    "                if isinstance(label, (int, np.integer)):\n",
    "                    labels.append(int(label))\n",
    "                elif isinstance(label, str):\n",
    "                    try:\n",
    "                        labels.append(int(label))\n",
    "                    except ValueError:\n",
    "                        labels.append(0)\n",
    "                else:\n",
    "                    labels.append(0)\n",
    "        \n",
    "        processed_dataset[split_name] = Dataset.from_dict({\n",
    "            \"text\": texts,\n",
    "            \"label\": labels\n",
    "        })\n",
    "        \n",
    "    def tokenize_function(examples):\n",
    "        return tokenizer(\n",
    "            examples[\"text\"], \n",
    "            padding=\"max_length\", \n",
    "            truncation=True, \n",
    "            max_length=128,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "    \n",
    "    tokenized_dataset = DatasetDict()\n",
    "    for split_name, split_dataset in processed_dataset.items():\n",
    "        tokenized_split = split_dataset.map(\n",
    "            tokenize_function, \n",
    "            batched=True, \n",
    "            remove_columns=[\"text\"]\n",
    "        )\n",
    "        tokenized_dataset[split_name] = tokenized_split\n",
    "        \n",
    "        for required_col in [\"input_ids\", \"attention_mask\", \"label\"]:\n",
    "            if required_col not in tokenized_split.column_names:\n",
    "                return None\n",
    "    \n",
    "    eval_args = TrainingArguments(\n",
    "        output_dir=f\"./benchmark_results/{dataset_name}_{run_timestamp}\",\n",
    "        per_device_eval_batch_size=16,\n",
    "        logging_dir=f\"./benchmark_results/{dataset_name}_{run_timestamp}/logs\",\n",
    "        report_to=\"none\",\n",
    "        remove_unused_columns=True\n",
    "    )\n",
    "    \n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=eval_args,\n",
    "        compute_metrics=compute_metrics,\n",
    "    )\n",
    "    \n",
    "    try:\n",
    "        benchmark_results = trainer.evaluate(eval_dataset=tokenized_dataset[\"test\"])\n",
    "        \n",
    "        return benchmark_results\n",
    "    except Exception as e:\n",
    "        return None\n",
    "\n",
    "available_datasets = []\n",
    "if swa_dataset:\n",
    "    available_datasets.append((\"Swahili\", swa_dataset, 3))\n",
    "if por_dataset:\n",
    "    available_datasets.append((\"Portuguese\", por_dataset, 3))\n",
    "if sot_dataset:\n",
    "    available_datasets.append((\"Sesotho\", sot_dataset, 3))\n",
    "\n",
    "all_results = []\n",
    "for dataset_name, dataset, num_labels in available_datasets:\n",
    "    try:\n",
    "        results = get_benchmark_metrics(dataset_name, dataset, num_labels)\n",
    "        \n",
    "        if results:\n",
    "            results_df = results_df._append({\n",
    "                \"Dataset\": dataset_name,\n",
    "                \"Model\": \"XLM-RoBERTa\",\n",
    "                \"Loss\": results.get(\"eval_loss\"),\n",
    "                \"Accuracy\": results.get(\"eval_accuracy\"),\n",
    "                \"F1\": results.get(\"eval_f1\"),\n",
    "                \"Precision\": results.get(\"eval_precision\"),\n",
    "                \"Recall\": results.get(\"eval_recall\")\n",
    "            }, ignore_index=True)\n",
    "            \n",
    "            all_results.append({\"Dataset\": dataset_name, \"Results\": results})\n",
    "        else:\n",
    "            pass\n",
    "    except Exception as e:\n",
    "        pass\n",
    "\n",
    "try:\n",
    "    results_df.to_csv(output_csv, index=False)\n",
    "except Exception as e:\n",
    "    emergency_path = f\"./emergency_results_{run_timestamp}.csv\"\n",
    "    results_df.to_csv(emergency_path, index=False)\n",
    "\n",
    "print(\"\\n==== BENCHMARKING COMPLETE ====\")\n",
    "print(\"\\nResults Summary:\")\n",
    "print(results_df)"
   ],
   "id": "9057ca89a3dc5b6f"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Fine-Tuning XLMR-Roberta",
   "id": "30ea9eca0382a07c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "#run this on google colab, my gpu doesnt have enough vram - Troy\n",
    "import pandas as pd\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments\n",
    "from datasets import load_dataset, Dataset, DatasetDict\n",
    "import torch\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "import os\n",
    "import logging\n",
    "import time\n",
    "import sys\n",
    "import json\n",
    "import shutil\n",
    "\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "    handlers=[\n",
    "        logging.StreamHandler(sys.stdout)\n",
    "    ]\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "try:\n",
    "    swa_dataset = load_dataset(\"masakhane/afrisenti\", \"swa\")\n",
    "    logger.info(\"Successfully loaded Swahili dataset from HuggingFace Hub\")\n",
    "except Exception as e:\n",
    "    logger.error(f\"Failed to load Swahili dataset: {e}\")\n",
    "    swa_dataset = None\n",
    "\n",
    "try:\n",
    "    por_dataset = load_dataset(\"masakhane/afrisenti\", \"por\") \n",
    "    logger.info(\"Successfully loaded Portuguese dataset from HuggingFace Hub\")\n",
    "except Exception as e:\n",
    "    logger.error(f\"Failed to load Portuguese dataset: {e}\")\n",
    "    por_dataset = None\n",
    "\n",
    "try:\n",
    "    sot_dataset = load_dataset(\"csv\", data_files={\"train\": \"./datasets/sotho-news/sotho_news_dataset.csv\"})\n",
    "    logger.info(\"Successfully loaded Sesotho dataset from CSV\")\n",
    "except Exception as e:\n",
    "    logger.error(f\"Failed to load Sesotho dataset: {e}\")\n",
    "    sot_dataset = None\n",
    "    \n",
    "def compute_metrics(pred):\n",
    "    labels = pred.label_ids\n",
    "    preds = pred.predictions.argmax(-1)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='weighted')\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    return {\n",
    "        'accuracy': acc,\n",
    "        'f1': f1,\n",
    "        'precision': precision,\n",
    "        'recall': recall\n",
    "    }\n",
    "\n",
    "def finetune_and_evaluate(dataset_name, dataset, num_labels):\n",
    "    logger.info(f\"Starting fine-tuning for {dataset_name}\")\n",
    "    \n",
    "    if dataset is None:\n",
    "        logger.error(f\"Dataset {dataset_name} is None, cannot proceed with fine-tuning\")\n",
    "        return None\n",
    "        \n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    logger.info(f\"Using device: {device}\")\n",
    "    \n",
    "    model_save_dir = f\"./models/{dataset_name.lower()}\"\n",
    "    os.makedirs(model_save_dir, exist_ok=True)\n",
    "    \n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"xlm-roberta-base\")\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(\n",
    "        \"xlm-roberta-base\", \n",
    "        num_labels=num_labels\n",
    "    ).to(device)\n",
    "    \n",
    "    logger.info(f\"Dataset {dataset_name} structure:\")\n",
    "    for split in dataset:\n",
    "        logger.info(f\"  - Split: {split}, Examples: {len(dataset[split])}\")\n",
    "        logger.info(f\"  - Features: {dataset[split].features}\")\n",
    "        logger.info(f\"  - Columns: {dataset[split].column_names}\")\n",
    "    \n",
    "    text_column = None\n",
    "    label_column = None\n",
    "    \n",
    "    text_candidates = [\"text\", \"content\", \"tweet\", \"sentence\", \"document\", \"headline\"]\n",
    "    for split in dataset:\n",
    "        columns = dataset[split].column_names\n",
    "        for candidate in text_candidates:\n",
    "            if candidate in columns:\n",
    "                text_column = candidate\n",
    "                break\n",
    "        if text_column:\n",
    "            break\n",
    "    \n",
    "    label_candidates = [\"label\", \"sentiment\", \"class\", \"category\"]\n",
    "    for split in dataset:\n",
    "        columns = dataset[split].column_names\n",
    "        for candidate in label_candidates:\n",
    "            if candidate in columns:\n",
    "                label_column = candidate\n",
    "                break\n",
    "        if label_column:\n",
    "            break\n",
    "    \n",
    "    logger.info(f\"For {dataset_name}, using text_column={text_column}, label_column={label_column}\")\n",
    "    \n",
    "    if text_column is None:\n",
    "        for split in dataset:\n",
    "            for col in dataset[split].column_names:\n",
    "                if isinstance(dataset[split][col][0], str):\n",
    "                    text_column = col\n",
    "                    logger.info(f\"Using '{col}' as text column based on string data\")\n",
    "                    break\n",
    "            if text_column:\n",
    "                break\n",
    "    \n",
    "    if text_column is None:\n",
    "        logger.error(f\"Could not identify a text column for {dataset_name}\")\n",
    "        return None\n",
    "    \n",
    "    if label_column is None:\n",
    "        logger.error(f\"Could not identify a label column for {dataset_name}\")\n",
    "        return None\n",
    "    \n",
    "    required_splits = [\"train\", \"validation\", \"test\"]\n",
    "    missing_splits = [split for split in required_splits if split not in dataset]\n",
    "    \n",
    "    if missing_splits:\n",
    "        logger.info(f\"Creating missing splits: {missing_splits} for {dataset_name}\")\n",
    "        if \"train\" in dataset:\n",
    "            train_valid_test = {}\n",
    "            \n",
    "            for split in dataset:\n",
    "                if split in required_splits:\n",
    "                    train_valid_test[split] = dataset[split]\n",
    "            \n",
    "            if \"train\" in dataset and (\"validation\" not in train_valid_test or \"test\" not in train_valid_test):\n",
    "                if \"validation\" not in train_valid_test:\n",
    "                    if \"test\" not in train_valid_test:\n",
    "                        split_datasets = dataset[\"train\"].train_test_split(test_size=0.2, seed=42)\n",
    "                        test_valid_split = split_datasets[\"test\"].train_test_split(test_size=0.5, seed=42)\n",
    "                        train_valid_test[\"train\"] = split_datasets[\"train\"]\n",
    "                        train_valid_test[\"validation\"] = test_valid_split[\"train\"]\n",
    "                        train_valid_test[\"test\"] = test_valid_split[\"test\"]\n",
    "                    else:\n",
    "                        split_datasets = dataset[\"train\"].train_test_split(test_size=0.1, seed=42)\n",
    "                        train_valid_test[\"train\"] = split_datasets[\"train\"]\n",
    "                        train_valid_test[\"validation\"] = split_datasets[\"test\"]\n",
    "                else:\n",
    "                    split_datasets = dataset[\"train\"].train_test_split(test_size=0.1, seed=42)\n",
    "                    train_valid_test[\"test\"] = split_datasets[\"test\"]\n",
    "            \n",
    "            dataset = DatasetDict(train_valid_test)\n",
    "        else:\n",
    "            logger.error(f\"Dataset {dataset_name} has no train split and cannot create splits\")\n",
    "            return None\n",
    "    \n",
    "    label_mapping = None\n",
    "    for split in dataset:\n",
    "        if isinstance(dataset[split][label_column][0], str):\n",
    "            all_labels = set()\n",
    "            for example in dataset[split][label_column]:\n",
    "                all_labels.add(example)\n",
    "            \n",
    "            label_mapping = {label: i for i, label in enumerate(sorted(all_labels))}\n",
    "            logger.info(f\"Created label mapping for {dataset_name}: {label_mapping}\")\n",
    "            break\n",
    "    \n",
    "    processed_dataset = DatasetDict()\n",
    "    for split_name, split_dataset in dataset.items():\n",
    "        texts = split_dataset[text_column]\n",
    "        \n",
    "        if label_mapping:\n",
    "            labels = [label_mapping[label] for label in split_dataset[label_column]]\n",
    "        else:\n",
    "            labels = []\n",
    "            for label in split_dataset[label_column]:\n",
    "                if isinstance(label, (int, np.integer)):\n",
    "                    labels.append(int(label))\n",
    "                elif isinstance(label, str):\n",
    "                    try:\n",
    "                        labels.append(int(label))\n",
    "                    except ValueError:\n",
    "                        logger.warning(f\"Unexpected string label found in {split_name}: {label}\")\n",
    "                        labels.append(0)\n",
    "                else:\n",
    "                    logger.warning(f\"Unexpected label type in {split_name}: {type(label)}\")\n",
    "                    labels.append(0)\n",
    "        \n",
    "        processed_dataset[split_name] = Dataset.from_dict({\n",
    "            \"text\": texts,\n",
    "            \"label\": labels\n",
    "        })\n",
    "        \n",
    "        logger.info(f\"Processed {split_name} split: {len(processed_dataset[split_name])} examples\")\n",
    "    \n",
    "    def tokenize_function(examples):\n",
    "        return tokenizer(\n",
    "            examples[\"text\"], \n",
    "            padding=\"max_length\", \n",
    "            truncation=True, \n",
    "            max_length=128,\n",
    "        )\n",
    "    \n",
    "    tokenized_dataset = DatasetDict()\n",
    "    for split_name, split_dataset in processed_dataset.items():\n",
    "        tokenized_split = split_dataset.map(\n",
    "            tokenize_function, \n",
    "            batched=True, \n",
    "            remove_columns=[\"text\"]\n",
    "        )\n",
    "        tokenized_dataset[split_name] = tokenized_split\n",
    "        \n",
    "        logger.info(f\"Tokenized {split_name} split: {len(tokenized_split)} examples\")\n",
    "        logger.info(f\"Columns after tokenization: {tokenized_split.column_names}\")\n",
    "        \n",
    "        for required_col in [\"input_ids\", \"attention_mask\", \"label\"]:\n",
    "            if required_col not in tokenized_split.column_names:\n",
    "                logger.error(f\"Required column {required_col} missing after tokenization\")\n",
    "                return None\n",
    "    \n",
    "    model_output_dir = f\"./tmp_model_dir_{dataset_name}\"\n",
    "    if os.path.exists(model_output_dir):\n",
    "        shutil.rmtree(model_output_dir)\n",
    "    os.makedirs(model_output_dir, exist_ok=True)\n",
    "    \n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=model_output_dir,\n",
    "        learning_rate=2e-5,\n",
    "        per_device_train_batch_size=16,\n",
    "        per_device_eval_batch_size=16,\n",
    "        num_train_epochs=3,\n",
    "        weight_decay=0.01,\n",
    "        logging_dir=\"./tmp_logs\",\n",
    "        logging_steps=10,\n",
    "        save_strategy=\"no\",\n",
    "        save_steps=1000000,\n",
    "        eval_steps=100,\n",
    "        do_eval=True,\n",
    "    )\n",
    "    \n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=tokenized_dataset[\"train\"],\n",
    "        eval_dataset=tokenized_dataset[\"validation\"],\n",
    "        compute_metrics=compute_metrics,\n",
    "    )\n",
    "    \n",
    "    logger.info(f\"Starting fine-tuning {dataset_name} with {len(tokenized_dataset['train'])} examples\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    try:\n",
    "        try:\n",
    "            train_output = trainer.train()\n",
    "            \n",
    "            training_loss = None\n",
    "            try:\n",
    "                if hasattr(train_output, \"metrics\") and \"loss\" in train_output.metrics:\n",
    "                    training_loss = train_output.metrics[\"loss\"]\n",
    "                elif isinstance(train_output, dict) and \"loss\" in train_output:\n",
    "                    training_loss = train_output[\"loss\"]\n",
    "                \n",
    "                if training_loss is None and hasattr(trainer, \"state\"):\n",
    "                    if hasattr(trainer.state, \"log_history\") and trainer.state.log_history:\n",
    "                        for log in reversed(trainer.state.log_history):\n",
    "                            if \"loss\" in log:\n",
    "                                training_loss = log[\"loss\"]\n",
    "                                break\n",
    "            except Exception as e:\n",
    "                logger.warning(f\"Could not extract training loss: {e}\")\n",
    "        except RuntimeError as e:\n",
    "            if \"PytorchStreamWriter failed writing file\" in str(e):\n",
    "                logger.warning(\"Caught PyTorch serialization error during training. Will proceed with model saving.\")\n",
    "                \n",
    "            else:\n",
    "                \n",
    "                raise\n",
    "        \n",
    "        try:\n",
    "            model_path = os.path.join(model_save_dir, \"model.pt\")\n",
    "            torch.save(model.state_dict(), model_path)\n",
    "            logger.info(f\"Model saved to {model_path}\")\n",
    "        except RuntimeError as e:\n",
    "            if \"PytorchStreamWriter failed writing file\" in str(e):\n",
    "                logger.warning(f\"PyTorch serialization error when saving model state dict. Trying alternative method.\")\n",
    "                \n",
    "                try:\n",
    "                    \n",
    "                    with open(model_path, 'wb') as f:\n",
    "                        torch.save(model.state_dict(), f, _use_new_zipfile_serialization=False)\n",
    "                    logger.info(f\"Model saved to {model_path} using legacy serialization\")\n",
    "                except Exception as e2:\n",
    "                    logger.error(f\"Failed to save model with alternative method: {e2}\")\n",
    "            else:\n",
    "                \n",
    "                raise\n",
    "        \n",
    "        tokenizer.save_pretrained(model_save_dir)\n",
    "        logger.info(f\"Tokenizer saved to {model_save_dir}\")\n",
    "        \n",
    "        if label_mapping:\n",
    "            label_mapping_path = os.path.join(model_save_dir, \"label_mapping.json\")\n",
    "            with open(label_mapping_path, \"w\") as f:\n",
    "                \n",
    "                json_mapping = {str(k): int(v) for k, v in label_mapping.items()}\n",
    "                json.dump(json_mapping, f, indent=2)\n",
    "            logger.info(f\"Label mapping saved to {label_mapping_path}\")\n",
    "            \n",
    "        model_config = {\n",
    "            \"base_model\": \"xlm-roberta-base\",\n",
    "            \"num_labels\": num_labels,\n",
    "            \"text_column\": text_column,\n",
    "            \"label_column\": label_column,\n",
    "            \"max_length\": 128,\n",
    "            \n",
    "            \"dataset_name\": dataset_name,\n",
    "        }\n",
    "        \n",
    "        config_path = os.path.join(model_save_dir, \"config.json\")\n",
    "        with open(config_path, \"w\") as f:\n",
    "            json.dump(model_config, f, indent=2)\n",
    "        logger.info(f\"Model config saved to {config_path}\")\n",
    "        \n",
    "        training_time = time.time() - start_time\n",
    "        logger.info(f\"Training completed in {training_time:.2f} seconds\")\n",
    "        \n",
    "        logger.info(f\"Evaluating on validation set with {len(tokenized_dataset['validation'])} examples\")\n",
    "        validation_results = trainer.evaluate(eval_dataset=tokenized_dataset[\"validation\"])\n",
    "        \n",
    "        logger.info(f\"Evaluating on test set with {len(tokenized_dataset['test'])} examples\")\n",
    "        test_results = trainer.evaluate(eval_dataset=tokenized_dataset[\"test\"])\n",
    "        \n",
    "        \n",
    "        processed_validation = {}\n",
    "        for k, v in validation_results.items():\n",
    "            if isinstance(v, (int, float, str, bool)) or v is None:\n",
    "                processed_validation[k] = v\n",
    "            else:\n",
    "                processed_validation[k] = float(v)\n",
    "                \n",
    "        processed_test = {}\n",
    "        for k, v in test_results.items():\n",
    "            if isinstance(v, (int, float, str, bool)) or v is None:\n",
    "                processed_test[k] = v\n",
    "            else:\n",
    "                processed_test[k] = float(v)\n",
    "        \n",
    "        metrics = {\n",
    "            \"training_time\": float(training_time),\n",
    "            \"training_loss\": float(training_loss) if training_loss is not None else None,\n",
    "            \"validation_results\": processed_validation,\n",
    "            \"test_results\": processed_test,\n",
    "        }\n",
    "        \n",
    "        metrics_path = os.path.join(model_save_dir, \"metrics.json\")\n",
    "        with open(metrics_path, \"w\") as f:\n",
    "            json.dump(metrics, f, indent=2)\n",
    "        logger.info(f\"Model metrics saved to {metrics_path}\")\n",
    "        \n",
    "        logger.info(f\"============== RESULTS FOR {dataset_name} ==============\")\n",
    "        logger.info(f\"Training loss: {training_loss}\")\n",
    "        logger.info(f\"Validation results: {validation_results}\")\n",
    "        logger.info(f\"Test results: {test_results}\")\n",
    "        \n",
    "        return {\n",
    "            \"dataset\": dataset_name,\n",
    "            \"training_time\": training_time,\n",
    "            \"training_loss\": training_loss,\n",
    "            \"validation_results\": validation_results,\n",
    "            \"test_results\": test_results,\n",
    "            \"model_path\": model_path,\n",
    "        }\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Fine-tuning failed for {dataset_name}: {e}\", exc_info=True)\n",
    "        return None\n",
    "\n",
    "available_datasets = []\n",
    "if swa_dataset:\n",
    "    available_datasets.append((\"Swahili\", swa_dataset, 3))\n",
    "if por_dataset:\n",
    "    available_datasets.append((\"Portuguese\", por_dataset, 3))\n",
    "if sot_dataset:\n",
    "    available_datasets.append((\"Sesotho\", sot_dataset, 3))\n",
    "\n",
    "logger.info(f\"Available datasets: {[name for name, _, _ in available_datasets]}\")\n",
    "\n",
    "all_results = []\n",
    "for dataset_name, dataset, num_labels in available_datasets:\n",
    "    logger.info(f\"\\n==== FINE-TUNING ON {dataset_name.upper()} DATASET ====\")a\n",
    "    try:\n",
    "        results = finetune_and_evaluate(dataset_name, dataset, num_labels)\n",
    "        \n",
    "        if results:\n",
    "            all_results.append(results)\n",
    "        else:\n",
    "            logger.warning(f\"No results returned for {dataset_name}\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Unexpected error during fine-tuning {dataset_name}: {e}\", exc_info=True)\n",
    "\n",
    "logger.info(\"\\n==== FINE-TUNING COMPLETE ====\")\n",
    "\n",
    "print(f\"{'Dataset':<12} | {'Acc (val)':<10} | {'F1 (val)':<10} | {'Acc (test)':<10} | {'F1 (test)':<10} | {'Model Dir':<20}\")\n",
    "\n",
    "for result in all_results:\n",
    "    dataset = result[\"dataset\"]\n",
    "    val_acc = result[\"validation_results\"].get(\"eval_accuracy\", float('nan'))\n",
    "    val_f1 = result[\"validation_results\"].get(\"eval_f1\", float('nan'))\n",
    "    test_acc = result[\"test_results\"].get(\"eval_accuracy\", float('nan'))\n",
    "    test_f1 = result[\"test_results\"].get(\"eval_f1\", float('nan'))\n",
    "    model_dir = f\"./models/{dataset.lower()}\"\n",
    "    \n",
    "    print(f\"{dataset:<12} | {val_acc:<10.4f} | {val_f1:<10.4f} | {test_acc:<10.4f} | {test_f1:<10.4f} | {model_dir}\")"
   ],
   "id": "d3201cd37233e5aa"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Data Augmentation",
   "id": "3ad195f0ddae14b5"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Firstly, we want to check the actual distribution of the classes for Mozambican Portuguese",
   "id": "5f86678abeca96da"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import os\n",
    "from datasets import load_from_disk, load_dataset\n",
    "from collections import Counter\n",
    "\n",
    "def analyze_sentiment_distribution(dataset, split='train'):\n",
    "    \"\"\"Analyze sentiment distribution in a dataset with string labels.\"\"\"\n",
    "    if dataset is None:\n",
    "        return\n",
    "    \n",
    "    # Count the occurrences of each sentiment label\n",
    "    label_counts = Counter(dataset[split]['label'])\n",
    "    \n",
    "    # Print the distribution\n",
    "    print(f\"\\nSentiment Distribution:\")\n",
    "    total = len(dataset[split])\n",
    "    \n",
    "    # Sort labels in a meaningful order: negative, neutral, positive\n",
    "    ordered_labels = ['negative', 'neutral', 'positive']\n",
    "    \n",
    "    for label in ordered_labels:\n",
    "        if label in label_counts:\n",
    "            count = label_counts[label]\n",
    "            percentage = (count / total) * 100\n",
    "            # Capitalize first letter for display\n",
    "            display_label = label.capitalize()\n",
    "            print(f\"{display_label}: {count} ({percentage:.2f}%)\")\n",
    "    \n",
    "    # Check for any other labels not in our expected list\n",
    "    for label, count in label_counts.items():\n",
    "        if label not in ordered_labels:\n",
    "            percentage = (count / total) * 100\n",
    "            print(f\"Other ({label}): {count} ({percentage:.2f}%)\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    swa, por, sot = load_local_datasets()\n",
    "\n",
    "    if por is not None:\n",
    "        print(f\"Portuguese dataset size: {len(por['train'])} examples\")\n",
    "        print(f\"Dataset features: {por['train'].features}\")\n",
    "        print(\"\\nSample entry:\")\n",
    "        print(por['train'][0])\n",
    "        analyze_sentiment_distribution(por)"
   ],
   "id": "60da823a0d920a68"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### There is a clear imbalance in the classes and so data augmentation techniques will be used such as dropout and back-translation in order to create synthetic data for the minority classes",
   "id": "77199268270ccf9"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import os\n",
    "import random\n",
    "from datasets import load_from_disk, load_dataset, Dataset\n",
    "from collections import Counter\n",
    "\n",
    "def load_local_datasets():\n",
    "    swa_path = \"./datasets/afrisenti/swa\"\n",
    "    por_path = \"./datasets/afrisenti/por\"\n",
    "    sot_path = \"./datasets/news\"\n",
    "\n",
    "    if not all(os.path.exists(path) for path in [swa_path, por_path, sot_path]):\n",
    "        print(\"One or more dataset directories not found. Please check the paths.\")\n",
    "        return None, None, None\n",
    "\n",
    "    print(\"Loading Swahili (swa) dataset from disk...\")\n",
    "    swa_dataset = load_from_disk(swa_path)\n",
    "    print(\"Swahili dataset loaded!\")\n",
    "\n",
    "    print(\"Loading Portuguese (por) dataset from disk...\")\n",
    "    por_dataset = load_from_disk(por_path)\n",
    "    print(\"Portuguese dataset loaded!\")\n",
    "\n",
    "    print(\"Loading Sesotho (sot) dataset from disk...\")\n",
    "    sot_dataset = load_dataset(\"csv\", data_files=\"datasets/sotho-news/sotho_news_dataset.csv\")\n",
    "    print(\"Sesotho dataset loaded!\")\n",
    "\n",
    "    return swa_dataset, por_dataset, sot_dataset\n",
    "\n",
    "def analyze_sentiment_distribution(dataset, split='train'):\n",
    "    \"\"\"Analyze sentiment distribution in a dataset with string labels.\"\"\"\n",
    "    if dataset is None:\n",
    "        return\n",
    "    \n",
    "    # Check if this is a Dataset object with splits or just a simple Dataset\n",
    "    if split in dataset:\n",
    "        # This is a dataset with splits\n",
    "        data = dataset[split]\n",
    "    else:\n",
    "        # This is a simple Dataset without splits\n",
    "        data = dataset\n",
    "    \n",
    "    # Count the occurrences of each sentiment label\n",
    "    label_counts = Counter(data['label'])\n",
    "    \n",
    "    # Print the distribution\n",
    "    print(f\"\\nSentiment Distribution:\")\n",
    "    total = len(data)\n",
    "    \n",
    "    # Sort labels in a meaningful order: negative, neutral, positive\n",
    "    ordered_labels = ['negative', 'neutral', 'positive']\n",
    "    \n",
    "    for label in ordered_labels:\n",
    "        if label in label_counts:\n",
    "            count = label_counts[label]\n",
    "            percentage = (count / total) * 100\n",
    "            # Capitalize first letter for display\n",
    "            display_label = label.capitalize()\n",
    "            print(f\"{display_label}: {count} ({percentage:.2f}%)\")\n",
    "    \n",
    "    # Check for any other labels not in our expected list\n",
    "    for label, count in label_counts.items():\n",
    "        if label not in ordered_labels:\n",
    "            percentage = (count / total) * 100\n",
    "            print(f\"Other ({label}): {count} ({percentage:.2f}%)\")\n",
    "\n",
    "def simple_augment_text(text):\n",
    "    \"\"\"Simple text augmentation without relying on translation models\"\"\"\n",
    "    augmentation_techniques = [\n",
    "        lambda t: word_deletion(t, p=0.1),\n",
    "        lambda t: word_swap(t, p=0.1),\n",
    "        lambda t: add_punctuation(t)\n",
    "    ]\n",
    "    \n",
    "    # Randomly select an augmentation technique\n",
    "    technique = random.choice(augmentation_techniques)\n",
    "    return technique(text)\n",
    "\n",
    "def word_deletion(text, p=0.1):\n",
    "    \"\"\"Randomly delete words with probability p\"\"\"\n",
    "    words = text.split()\n",
    "    if len(words) <= 3:  # Don't delete from very short texts\n",
    "        return text\n",
    "        \n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        if random.random() > p:  # Keep the word with probability (1-p)\n",
    "            new_words.append(word)\n",
    "    \n",
    "    # Ensure we don't delete all words\n",
    "    if not new_words:\n",
    "        return random.choice(words)\n",
    "        \n",
    "    return ' '.join(new_words)\n",
    "\n",
    "def word_swap(text, p=0.1):\n",
    "    \"\"\"Randomly swap adjacent words with probability p\"\"\"\n",
    "    words = text.split()\n",
    "    if len(words) <= 1:\n",
    "        return text\n",
    "        \n",
    "    for i in range(len(words) - 1):\n",
    "        if random.random() < p:\n",
    "            words[i], words[i+1] = words[i+1], words[i]\n",
    "    \n",
    "    return ' '.join(words)\n",
    "\n",
    "def add_punctuation(text):\n",
    "    \"\"\"Add or modify punctuation without changing meaning\"\"\"\n",
    "    # Add emphasis for positive/negative texts\n",
    "    if text[-1] not in '!?.':\n",
    "        if random.random() < 0.5:\n",
    "            text += '!'\n",
    "        else:\n",
    "            text += '.'\n",
    "    elif text[-1] == '.' and random.random() < 0.3:\n",
    "        text = text[:-1] + '!'\n",
    "    \n",
    "    return text\n",
    "\n",
    "def balance_and_augment_dataset(dataset, target_neutral_ratio=0.7, split='train'):\n",
    "    \"\"\"\n",
    "    Balance and augment dataset:\n",
    "    1. Undersample neutral class\n",
    "    2. Augment negative and positive classes using simple techniques\n",
    "    \"\"\"\n",
    "    # Count the occurrences of each label\n",
    "    label_counts = {}\n",
    "    for label in ['negative', 'neutral', 'positive']:\n",
    "        label_counts[label] = sum(1 for l in dataset[split]['label'] if l == label)\n",
    "    \n",
    "    # Separate data by label\n",
    "    data_by_label = {\n",
    "        'negative': [],\n",
    "        'neutral': [],\n",
    "        'positive': []\n",
    "    }\n",
    "    \n",
    "    for i in range(len(dataset[split])):\n",
    "        label = dataset[split][i]['label']\n",
    "        data_by_label[label].append({\n",
    "            'tweet': dataset[split][i]['tweet'],\n",
    "            'label': label\n",
    "        })\n",
    "    \n",
    "    # Undersample neutral class\n",
    "    neutral_target_size = int(label_counts['neutral'] * target_neutral_ratio)\n",
    "    sampled_neutral = random.sample(data_by_label['neutral'], neutral_target_size)\n",
    "    \n",
    "    # Prepare balanced data with originals\n",
    "    balanced_data = data_by_label['negative'] + sampled_neutral + data_by_label['positive']\n",
    "    \n",
    "    # Augment minority classes\n",
    "    augmented_data = balanced_data.copy()\n",
    "    \n",
    "    # Target count for each class after balancing\n",
    "    target_count = max(len(data_by_label['negative']), len(data_by_label['positive']), neutral_target_size)\n",
    "    \n",
    "    # Augment negative class\n",
    "    negative_to_add = target_count - len(data_by_label['negative'])\n",
    "    if negative_to_add > 0:\n",
    "        # Select samples to augment (can select the same sample multiple times)\n",
    "        for _ in range(negative_to_add):\n",
    "            sample = random.choice(data_by_label['negative'])\n",
    "            augmented_tweet = simple_augment_text(sample['tweet'])\n",
    "            augmented_data.append({'tweet': augmented_tweet, 'label': 'negative'})\n",
    "    \n",
    "    # Augment positive class\n",
    "    positive_to_add = target_count - len(data_by_label['positive'])\n",
    "    if positive_to_add > 0:\n",
    "        for _ in range(positive_to_add):\n",
    "            sample = random.choice(data_by_label['positive'])\n",
    "            augmented_tweet = simple_augment_text(sample['tweet'])\n",
    "            augmented_data.append({'tweet': augmented_tweet, 'label': 'positive'})\n",
    "    \n",
    "    # Shuffle the augmented data\n",
    "    random.shuffle(augmented_data)\n",
    "    \n",
    "    # Return the augmented data directly (not as a Dataset object)\n",
    "    return augmented_data\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    swa, por, sot = load_local_datasets()\n",
    "    \n",
    "    if por is not None:\n",
    "        print(\"Original Portuguese dataset:\")\n",
    "        analyze_sentiment_distribution(por)\n",
    "        \n",
    "        # Balance and augment dataset\n",
    "        augmented_data = balance_and_augment_dataset(por)\n",
    "        \n",
    "        # Create a new dataset from the augmented data\n",
    "        augmented_dataset = Dataset.from_dict({\n",
    "            'tweet': [item['tweet'] for item in augmented_data],\n",
    "            'label': [item['label'] for item in augmented_data]\n",
    "        })\n",
    "        \n",
    "        print(\"\\nAfter balancing and augmentation:\")\n",
    "        analyze_sentiment_distribution(augmented_dataset)  # Now analyzing the dataset directly\n",
    "        \n",
    "        # Create a dataset with splits for saving\n",
    "        full_dataset_with_splits = {\"train\": augmented_dataset}\n",
    "        full_dataset = Dataset.from_dict({\n",
    "            'train': augmented_dataset\n",
    "        })\n",
    "        \n",
    "        # Save the balanced and augmented dataset\n",
    "        full_dataset.save_to_disk(\"./datasets/afrisenti/por_balanced_augmented\")\n",
    "        print(\"\\nBalanced and augmented dataset saved!\")\n",
    "        "
   ],
   "id": "4fd9d0b1ea1f10cb"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "These text augmentation techniques (word deletion, word swap, punctuation changes) are more appropriate for Mozambican Portuguese since they don't rely on external translation models that might not understand the dialect. They preserve the unique characteristics of Mozambican Portuguese while still creating useful variations of the original text.",
   "id": "27987649eb06b624"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Now let's see if there is any improvement when training the best performing BERT model on the Mozambican Portuguese data which was Afro-XLMR:",
   "id": "57d3ff05acafc801"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# %pip install -U transformers datasets peft evaluate plotly sentencepiece --quiet\n",
    "\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from transformers import XLMRobertaForSequenceClassification, XLMRobertaTokenizer\n",
    "from datasets import load_dataset\n",
    "# import torch_optimizer as optim\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, confusion_matrix\n",
    "# import numpy as np\n",
    "import pandas as pd\n",
    "# import matplotlib.pyplot as plt\n",
    "# import seaborn as sns\n",
    "# from tqdm import tqdm\n",
    "import logging\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "class SentimentDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_length=128):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = str(self.texts[idx])\n",
    "        label = self.labels[idx]\n",
    "\n",
    "        encoding = self.tokenizer(\n",
    "            text,\n",
    "            truncation=True,\n",
    "            padding='max_length',\n",
    "            max_length=self.max_length,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'label': torch.tensor(label, dtype=torch.long)\n",
    "        }\n",
    "\n",
    "def load_data(language):\n",
    "    try:\n",
    "        aug_por_path = \"./datasets/afrisenti/por_balanced_augmented\"\n",
    "\n",
    "        if language=='por':\n",
    "            chosen_dataset=load_from_disk(aug_por_path)\n",
    "        else:\n",
    "            raise Exception\n",
    "\n",
    "        for ds in [chosen_dataset]: # Change to all three later\n",
    "            for lbl in [\"train\",\"validation\",\"test\"]:\n",
    "                if ds[lbl].column_names[0]== \"tweet\":\n",
    "                    ds[lbl] = ds[lbl].rename_column(\"tweet\",\"text\")\n",
    "                else:\n",
    "                    ds[lbl] = ds[lbl].rename_column(\"headline\",\"text\")\n",
    "\n",
    "        train_df  = chosen_dataset[\"train\"].to_pandas()\n",
    "        val_df  = chosen_dataset[\"validation\"].to_pandas()\n",
    "        test_df  = chosen_dataset[\"test\"].to_pandas()\n",
    "        logger.info(f\"Data loaded successfully: {len(train_df)} training, {len(val_df)} validation, {len(test_df)} test examples\")\n",
    "        return train_df, val_df, test_df\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error loading data: {str(e)}\")\n",
    "        raise e\n",
    "\n",
    "# Function to create DataLoaders\n",
    "def create_data_loaders(train_df, val_df, test_df, tokenizer, batch_size=16, text_column='text', label_column='label'):\n",
    "    train_dataset = SentimentDataset(\n",
    "        texts=train_df[text_column].tolist(),\n",
    "        labels=train_df[label_column].tolist(),\n",
    "        tokenizer=tokenizer\n",
    "    )\n",
    "\n",
    "    val_dataset = SentimentDataset(\n",
    "        texts=val_df[text_column].tolist(),\n",
    "        labels=val_df[label_column].tolist(),\n",
    "        tokenizer=tokenizer\n",
    "    )\n",
    "\n",
    "    test_dataset = SentimentDataset(\n",
    "        texts=test_df[text_column].tolist(),\n",
    "        labels=test_df[label_column].tolist(),\n",
    "        tokenizer=tokenizer\n",
    "    )\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size)\n",
    "\n",
    "    return train_loader, val_loader, test_loader\n",
    "\n",
    "def train_epoch(model, dataloader, optimizer, scheduler, device):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "\n",
    "    progress_bar = tqdm(dataloader, desc=\"Training\")\n",
    "    for batch in progress_bar:\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['label'].to(device)\n",
    "\n",
    "        outputs = model(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            labels=labels\n",
    "        )\n",
    "\n",
    "        loss = outputs.loss\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "        progress_bar.set_postfix({\"loss\": f\"{loss.item():.4f}\"})\n",
    "\n",
    "    return epoch_loss / len(dataloader)\n",
    "\n",
    "def evaluate(model, dataloader, device):\n",
    "    \"\"\"\n",
    "    Returns:\n",
    "        Tuple of (loss, accuracy, precision, recall, f1)\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(dataloader, desc=\"Evaluating\"):\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['label'].to(device)\n",
    "\n",
    "            outputs = model(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                labels=labels\n",
    "            )\n",
    "\n",
    "            loss = outputs.loss\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            logits = outputs.logits\n",
    "            preds = torch.argmax(logits, dim=1).cpu().numpy()\n",
    "\n",
    "            all_preds.extend(preds)\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    # Calculate metrics\n",
    "    accuracy = accuracy_score(all_labels, all_preds)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(\n",
    "        all_labels, all_preds, average='weighted'\n",
    "    )\n",
    "\n",
    "    return total_loss / len(dataloader), accuracy, precision, recall, f1, all_preds, all_labels\n",
    "\n",
    "def plot_confusion_matrix(true_labels, predictions, class_names):\n",
    "    cm = confusion_matrix(true_labels, predictions)\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=class_names, yticklabels=class_names)\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('True')\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('confusion_matrix.png')\n",
    "    plt.close()\n",
    "\n",
    "def evaluate_afro_xlmr_for_lang(language):\n",
    "    config = {\n",
    "        'model_name': 'Davlan/afro-xlmr-base',\n",
    "        'num_labels': 3,\n",
    "        'batch_size': 16,\n",
    "        'learning_rate': 2e-5,\n",
    "        'epochs': 3,\n",
    "        'warmup_steps': 0,\n",
    "        'max_grad_norm': 1.0,\n",
    "        'text_column': 'text',\n",
    "        'label_column': 'label',\n",
    "        'class_names': ['negative', 'neutral', 'positive']\n",
    "    }\n",
    "\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    logger.info(f\"Using device: {device}\")\n",
    "\n",
    "    logger.info(f\"Loading model: {config['model_name']}...\")\n",
    "    tokenizer = XLMRobertaTokenizer.from_pretrained(config['model_name'])\n",
    "    model = XLMRobertaForSequenceClassification.from_pretrained(\n",
    "        config['model_name'],\n",
    "        num_labels=config['num_labels']\n",
    "    )\n",
    "    model.to(device)\n",
    "\n",
    "    # Load data\n",
    "    logger.info(\"Loading data...\")\n",
    "    train_df, val_df, test_df = load_data(f'{language}')\n",
    "\n",
    "    # Create data loaders\n",
    "    logger.info(\"Creating data loaders...\")\n",
    "    train_loader, val_loader, test_loader = create_data_loaders(\n",
    "        train_df, val_df, test_df,\n",
    "        tokenizer,\n",
    "        batch_size=config['batch_size'],\n",
    "        text_column=config['text_column'],\n",
    "        label_column=config['label_column']\n",
    "    )\n",
    "\n",
    "\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=config['learning_rate'])\n",
    "    total_steps = len(train_loader) * config['epochs']\n",
    "    scheduler = get_linear_schedule_with_warmup(\n",
    "        optimizer,\n",
    "        num_warmup_steps=config['warmup_steps'],\n",
    "        num_training_steps=total_steps\n",
    "    )\n",
    "\n",
    "    logger.info(\"Starting training...\")\n",
    "    best_val_f1 = 0\n",
    "    best_model_state = None\n",
    "\n",
    "    for epoch in range(config['epochs']):\n",
    "        logger.info(f\"Epoch {epoch + 1}/{config['epochs']}\")\n",
    "\n",
    "        # Train\n",
    "        start_time = time.time()\n",
    "        train_loss = train_epoch(model, train_loader,optimizer,scheduler, device) # train_epoch(model, train_loader, scheduler, device)\n",
    "        train_time = time.time() - start_time\n",
    "\n",
    "        # Validate\n",
    "        val_loss, val_accuracy, val_precision, val_recall, val_f1, _, _ = evaluate(model, val_loader, device)\n",
    "\n",
    "        logger.info(f\"Epoch {epoch + 1} results:\")\n",
    "        logger.info(f\"Train Loss: {train_loss:.4f}, Time: {train_time:.2f}s\")\n",
    "        logger.info(f\"Val Loss: {val_loss:.4f}, Accuracy: {val_accuracy:.4f}, Precision: {val_precision:.4f}, Recall: {val_recall:.4f}, F1: {val_f1:.4f}\")\n",
    "\n",
    "        # Save best model\n",
    "        if val_f1 > best_val_f1:\n",
    "            best_val_f1 = val_f1\n",
    "            best_model_state = model.state_dict().copy()\n",
    "            logger.info(f\"New best model with F1: {best_val_f1:.4f}\")\n",
    "\n",
    "    # Load best model for testing\n",
    "    if best_model_state:\n",
    "        logger.info(\"Loading best model for testing...\")\n",
    "        model.load_state_dict(best_model_state)\n",
    "\n",
    "    # Test evaluation\n",
    "    logger.info(\"Evaluating on test set...\")\n",
    "    test_loss, test_accuracy, test_precision, test_recall, test_f1, test_preds, test_labels = evaluate(model, test_loader, device)\n",
    "\n",
    "    logger.info(f\"Test Results:\")\n",
    "    logger.info(f\"Loss: {test_loss:.4f}\")\n",
    "    logger.info(f\"Accuracy: {test_accuracy:.4f}\")\n",
    "    logger.info(f\"Precision: {test_precision:.4f}\")\n",
    "    logger.info(f\"Recall: {test_recall:.4f}\")\n",
    "    logger.info(f\"F1 Score: {test_f1:.4f}\")\n",
    "\n",
    "    results_df_afro = {\n",
    "    \"test_loss\": test_loss,\n",
    "    \"test_accuracy\": test_accuracy,\n",
    "    \"test_f1\": test_f1,\n",
    "    \"test_precision\": test_precision,\n",
    "    \"test_recall\": test_recall,\n",
    "    \"epochs\": config['epochs'],\n",
    "    \"learning_rate\": config['learning_rate'],\n",
    "    \"batch_size\": config['batch_size'],\n",
    "}\n",
    "\n",
    "    path = f\"afroxlmr_results_{language}_augmented.csv\"\n",
    "    pd.DataFrame([results_df_afro]).to_csv(path, index=False)\n",
    "    print(f\"Results saved to {path}\")\n",
    "\n",
    "    # Save model\n",
    "    logger.info(\"Saving model...\")\n",
    "    model_save_path = f'./xmlr_sentiment_model_{language}_augmented'\n",
    "    model.save_pretrained(model_save_path)\n",
    "    tokenizer.save_pretrained(model_save_path)\n",
    "    logger.info(f\"Model saved to {model_save_path}\")\n",
    "\n",
    "# evaluate_afro_xlmr()\n",
    "langs = ['por']\n",
    "\n",
    "for l in langs:\n",
    "    evaluate_afro_xlmr_for_lang(l)"
   ],
   "id": "b45334d4c6fe9165"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Next, we evaluate the use of Adapters to perform cross-lingual transfer\n",
    "This section will explore the effect of Adapters on model performance when doing cross-lingual transfer. This evaluation will use the Afroxlmr model, because it had the highest accuracy when analysing the Swahili data."
   ],
   "id": "295c94aea83fb67"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "We firstly evaluate the performance of the model trained on Swahili data, on the Sesotho dataset",
   "id": "374a589f216ed046"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-14T16:52:17.215444Z",
     "start_time": "2025-06-14T16:51:43.696697Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from transformers import XLMRobertaForSequenceClassification, XLMRobertaTokenizer\n",
    "from datasets import load_dataset\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, confusion_matrix\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import logging\n",
    "from adapters import AutoAdapterModel,AdapterConfig\n",
    "from torch.nn import CrossEntropyLoss\n",
    "\n",
    "\n",
    "# Temp\n",
    "class SentimentDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_length=128, lang=None):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        self.lang = lang\n",
    "        if self.lang=='sot':\n",
    "            self.label_mapping = {\"negative\": 0, \"neutral\": 1, \"positive\": 2}  # Label conversion\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = str(self.texts[idx])\n",
    "        if self.lang=='sot':\n",
    "            label = self.label_mapping[self.labels[idx]]\n",
    "        else:\n",
    "            label = self.labels[idx]\n",
    "\n",
    "        encoding = self.tokenizer(\n",
    "            text,\n",
    "            truncation=True,\n",
    "            padding='max_length',\n",
    "            max_length=self.max_length,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'labels': torch.tensor(label, dtype=torch.long)\n",
    "        }\n",
    "\n",
    "def load_data(language):\n",
    "    try:\n",
    "        if language=='por':\n",
    "            chosen_dataset=load_dataset(\"HausaNLP/AfriSenti-Twitter\", \"por\",trust_remote_code=True)\n",
    "        elif language=='swa':\n",
    "            chosen_dataset=load_dataset(\"HausaNLP/AfriSenti-Twitter\", \"swa\",trust_remote_code=True)\n",
    "        elif language=='sot':\n",
    "            chosen_dataset=load_dataset(\"hamza-student-123/nlp-assignment-news-data\",'sot')\n",
    "        else:\n",
    "            raise Exception\n",
    "\n",
    "        for ds in [chosen_dataset]: # Change to all three later\n",
    "            for lbl in [\"train\",\"validation\",\"test\"]:\n",
    "                if ds[lbl].column_names[0]== \"tweet\":\n",
    "                    ds[lbl] = ds[lbl].rename_column(\"tweet\",\"text\")\n",
    "                else:\n",
    "                    ds[lbl] = ds[lbl].rename_column(\"headline\",\"text\")\n",
    "\n",
    "        train_df  = chosen_dataset[\"train\"].to_pandas()\n",
    "        val_df  = chosen_dataset[\"validation\"].to_pandas()\n",
    "        test_df  = chosen_dataset[\"test\"].to_pandas()\n",
    "        logger.info(f\"Data loaded successfully: {len(train_df)} training, {len(val_df)} validation, {len(test_df)} test examples\")\n",
    "        return train_df, val_df, test_df\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error loading data: {str(e)}\")\n",
    "        raise e\n",
    "\n",
    "\n",
    "def create_data_loaders(train_df, val_df, test_df, tokenizer, batch_size=16, text_column='text', label_column='label', lang=None):\n",
    "    train_dataset = SentimentDataset(\n",
    "        texts=train_df[text_column].tolist(),\n",
    "        labels=train_df[label_column].tolist(),\n",
    "        tokenizer=tokenizer,\n",
    "        lang=lang\n",
    "    )\n",
    "\n",
    "    val_dataset = SentimentDataset(\n",
    "        texts=val_df[text_column].tolist(),\n",
    "        labels=val_df[label_column].tolist(),\n",
    "        tokenizer=tokenizer,\n",
    "        lang=lang\n",
    "    )\n",
    "\n",
    "    test_dataset = SentimentDataset(\n",
    "        texts=test_df[text_column].tolist(),\n",
    "        labels=test_df[label_column].tolist(),\n",
    "        tokenizer=tokenizer,\n",
    "        lang=lang\n",
    "    )\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size)\n",
    "\n",
    "    return train_loader, val_loader, test_loader\n",
    "\n",
    "def evaluate(model, dataloader, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(dataloader, desc=\"Evaluating\"):\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "\n",
    "            outputs = model(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                labels=labels\n",
    "            )\n",
    "\n",
    "            loss = outputs.loss\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            logits = outputs.logits\n",
    "            preds = torch.argmax(logits, dim=1).cpu().numpy()\n",
    "\n",
    "            all_preds.extend(preds)\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    # Calculate metrics\n",
    "    accuracy = accuracy_score(all_labels, all_preds)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(\n",
    "        all_labels, all_preds, average='weighted'\n",
    "    )\n",
    "\n",
    "    return total_loss / len(dataloader), accuracy, precision, recall, f1, all_preds, all_labels\n",
    "\n",
    "\n",
    "def evaluate_afro_xlmr_before_adapter(source_language,target_language):\n",
    "    config = {\n",
    "        'extern_model_name': f'Davlan/afro-xlmr-base',\n",
    "        'model_name': f'./xmlr_sentiment_model_{source_language}',\n",
    "        'adapter_model_name': f'xlmr_sentiment_model_swa',\n",
    "        'num_labels': 3,\n",
    "        'batch_size': 16,\n",
    "        'learning_rate': 2e-5,\n",
    "        'epochs': 3,\n",
    "        'warmup_steps': 0,\n",
    "        'max_grad_norm': 1.0,\n",
    "        'text_column': 'text',\n",
    "        'label_column': 'label',\n",
    "        'class_names': ['negative', 'neutral', 'positive']\n",
    "    }\n",
    "\n",
    "    tokenizer = XLMRobertaTokenizer.from_pretrained(config['extern_model_name'])\n",
    "\n",
    "    model = AutoAdapterModel.from_pretrained(config['model_name'], num_labels=config['num_labels'])\n",
    "    logger.info(\"Successfully loaded as AutoAdapterModel\")\n",
    "\n",
    "    logger.info(\"Loading target data...\")\n",
    "    train_df, val_df, test_df = load_data(f'{target_language}')\n",
    "\n",
    "    logger.info(\"Creating data loaders...\")\n",
    "    target_train_loader, val_loader, test_loader = create_data_loaders(\n",
    "        train_df, val_df, test_df,\n",
    "        tokenizer,\n",
    "        batch_size=config['batch_size'],\n",
    "        text_column=config['text_column'],\n",
    "        label_column=config['label_column'],\n",
    "        lang=target_language\n",
    "    )\n",
    "    # Test evaluation: Showing results after training and adapters\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    logger.info(\"Evaluating on test set...\")\n",
    "    test_loss, test_accuracy, test_precision, test_recall, test_f1, test_preds, test_labels = evaluate(model, test_loader, device)\n",
    "\n",
    "    logger.info(f\"Test Results:\")\n",
    "    logger.info(f\"Loss: {test_loss:.4f}\")\n",
    "    logger.info(f\"Accuracy: {test_accuracy:.4f}\")\n",
    "    logger.info(f\"Precision: {test_precision:.4f}\")\n",
    "    logger.info(f\"Recall: {test_recall:.4f}\")\n",
    "    logger.info(f\"F1 Score: {test_f1:.4f}\")\n",
    "\n",
    "\n",
    "evaluate_afro_xlmr_before_adapter('swa','sot')"
   ],
   "id": "69d3fff407d8d976",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-14 18:51:46,468 - INFO - Adding head 'default' with config {'head_type': 'classification', 'num_labels': 3, 'layers': 2, 'activation_function': 'tanh', 'label2id': {'LABEL_0': 0, 'LABEL_1': 1, 'LABEL_2': 2}, 'use_pooler': False, 'bias': True, 'dropout_prob': None}.\n",
      "Some weights of XLMRobertaAdapterModel were not initialized from the model checkpoint at ./xmlr_sentiment_model_swa and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "2025-06-14 18:51:46,535 - INFO - Successfully loaded as AutoAdapterModel\n",
      "2025-06-14 18:51:46,536 - INFO - Loading target data...\n",
      "2025-06-14 18:51:54,712 - INFO - Data loaded successfully: 1740 training, 349 validation, 349 test examples\n",
      "2025-06-14 18:51:54,713 - INFO - Creating data loaders...\n",
      "2025-06-14 18:51:54,714 - INFO - Evaluating on test set...\n",
      "Evaluating: 100%|| 22/22 [00:22<00:00,  1.02s/it]\n",
      "C:\\Users\\User\\Desktop\\UP\\COS 760 (Natural Language Processing)\\Research\\Environment\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "2025-06-14 18:52:17,173 - INFO - Test Results:\n",
      "2025-06-14 18:52:17,174 - INFO - Loss: 2.1723\n",
      "2025-06-14 18:52:17,175 - INFO - Accuracy: 0.0544\n",
      "2025-06-14 18:52:17,176 - INFO - Precision: 0.0030\n",
      "2025-06-14 18:52:17,176 - INFO - Recall: 0.0544\n",
      "2025-06-14 18:52:17,177 - INFO - F1 Score: 0.0056\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### We then Add Adapters to the model",
   "id": "66afdd4d5f4d3423"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# !pip install -U transformers datasets peft evaluate plotly sentencepiece adapters adapter-transformers --quiet\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from transformers import XLMRobertaForSequenceClassification, XLMRobertaTokenizer\n",
    "from datasets import load_dataset\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, confusion_matrix\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import logging\n",
    "from adapters import AutoAdapterModel,AdapterConfig\n",
    "from torch.nn import CrossEntropyLoss\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "class SentimentDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_length=128, lang=None):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        self.lang = lang\n",
    "        if self.lang=='sot':\n",
    "            self.label_mapping = {\"negative\": 0, \"neutral\": 1, \"positive\": 2}  # Label conversion\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = str(self.texts[idx])\n",
    "        if self.lang=='sot':\n",
    "            label = self.label_mapping[self.labels[idx]]\n",
    "        else:\n",
    "            label = self.labels[idx]\n",
    "\n",
    "        encoding = self.tokenizer(\n",
    "            text,\n",
    "            truncation=True,\n",
    "            padding='max_length',\n",
    "            max_length=self.max_length,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'labels': torch.tensor(label, dtype=torch.long)\n",
    "        }\n",
    "\n",
    "def load_data(language):\n",
    "    try:\n",
    "        if language=='por':\n",
    "            chosen_dataset=load_dataset(\"HausaNLP/AfriSenti-Twitter\", \"por\",trust_remote_code=True)\n",
    "        elif language=='swa':\n",
    "            chosen_dataset=load_dataset(\"HausaNLP/AfriSenti-Twitter\", \"swa\",trust_remote_code=True)\n",
    "        elif language=='sot':\n",
    "            chosen_dataset=load_dataset(\"hamza-student-123/nlp-assignment-news-data\",'sot')\n",
    "        else:\n",
    "            raise Exception\n",
    "\n",
    "        for ds in [chosen_dataset]: # Change to all three later\n",
    "            for lbl in [\"train\",\"validation\",\"test\"]:\n",
    "                if ds[lbl].column_names[0]== \"tweet\":\n",
    "                    ds[lbl] = ds[lbl].rename_column(\"tweet\",\"text\")\n",
    "                else:\n",
    "                    ds[lbl] = ds[lbl].rename_column(\"headline\",\"text\")\n",
    "\n",
    "        train_df  = chosen_dataset[\"train\"].to_pandas()\n",
    "        val_df  = chosen_dataset[\"validation\"].to_pandas()\n",
    "        test_df  = chosen_dataset[\"test\"].to_pandas()\n",
    "        logger.info(f\"Data loaded successfully: {len(train_df)} training, {len(val_df)} validation, {len(test_df)} test examples\")\n",
    "        return train_df, val_df, test_df\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error loading data: {str(e)}\")\n",
    "        raise e\n",
    "\n",
    "\n",
    "def create_data_loaders(train_df, val_df, test_df, tokenizer, batch_size=16, text_column='text', label_column='label', lang=None):\n",
    "    train_dataset = SentimentDataset(\n",
    "        texts=train_df[text_column].tolist(),\n",
    "        labels=train_df[label_column].tolist(),\n",
    "        tokenizer=tokenizer,\n",
    "        lang=lang\n",
    "    )\n",
    "\n",
    "    val_dataset = SentimentDataset(\n",
    "        texts=val_df[text_column].tolist(),\n",
    "        labels=val_df[label_column].tolist(),\n",
    "        tokenizer=tokenizer,\n",
    "        lang=lang\n",
    "    )\n",
    "\n",
    "    test_dataset = SentimentDataset(\n",
    "        texts=test_df[text_column].tolist(),\n",
    "        labels=test_df[label_column].tolist(),\n",
    "        tokenizer=tokenizer,\n",
    "        lang=lang\n",
    "    )\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size)\n",
    "\n",
    "    return train_loader, val_loader, test_loader\n",
    "\n",
    "def train_epoch(model, dataloader, optimizer, scheduler, device):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "\n",
    "    progress_bar = tqdm(dataloader, desc=\"Training\")\n",
    "    for batch in progress_bar:\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['label'].to(device)\n",
    "\n",
    "        outputs = model(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            labels=labels\n",
    "        )\n",
    "\n",
    "        loss = outputs.loss\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "        progress_bar.set_postfix({\"loss\": f\"{loss.item():.4f}\"})\n",
    "\n",
    "    return epoch_loss / len(dataloader)\n",
    "\n",
    "def evaluate(model, dataloader, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(dataloader, desc=\"Evaluating\"):\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "\n",
    "            outputs = model(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                labels=labels\n",
    "            )\n",
    "\n",
    "            loss = outputs.loss\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            logits = outputs.logits\n",
    "            preds = torch.argmax(logits, dim=1).cpu().numpy()\n",
    "\n",
    "            all_preds.extend(preds)\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    accuracy = accuracy_score(all_labels, all_preds)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(\n",
    "        all_labels, all_preds, average='weighted'\n",
    "    )\n",
    "\n",
    "    return total_loss / len(dataloader), accuracy, precision, recall, f1, all_preds, all_labels\n",
    "\n",
    "def setup_crosslingual_adapters(config, source_language, target_language):\n",
    "\n",
    "    logger.info(f\"Loading model: {config['model_name']}...\")\n",
    "    tokenizer = XLMRobertaTokenizer.from_pretrained(config['extern_model_name'])\n",
    "    model = AutoAdapterModel.from_pretrained(config['model_name'], num_labels=config['num_labels'])\n",
    "    logger.info(\"Successfully loaded as AutoAdapterModel\")\n",
    "\n",
    "    lang_adapter_config = AdapterConfig.load(\"pfeiffer\", reduction_factor=2)\n",
    "\n",
    "    source_adapter_name = f\"lang_{source_language}\"\n",
    "    model.add_adapter(source_adapter_name, config=lang_adapter_config)\n",
    "    logger.info(f\"Added source language adapter: {source_adapter_name}\")\n",
    "\n",
    "    target_adapter_name = f\"lang_{target_language}\"\n",
    "    model.add_adapter(target_adapter_name, config=lang_adapter_config)\n",
    "    logger.info(f\"Added target language adapter: {target_adapter_name}\")\n",
    "\n",
    "    task_adapter_name = \"sentiment_task\"\n",
    "    task_adapter_config = AdapterConfig.load(\"pfeiffer\", reduction_factor=16)\n",
    "    model.add_adapter(task_adapter_name, config=task_adapter_config)\n",
    "\n",
    "    model.add_classification_head(\n",
    "        task_adapter_name,\n",
    "        num_labels=config['num_labels'],\n",
    "        id2label={i: label for i, label in enumerate(config['class_names'])}\n",
    "    )\n",
    "\n",
    "    return model, tokenizer, source_adapter_name, target_adapter_name, task_adapter_name\n",
    "\n",
    "def setup_madx_adapter(config, source_language, target_language):\n",
    "    tokenizer = XLMRobertaTokenizer.from_pretrained(config['extern_model_name'])\n",
    "    model = AutoAdapterModel.from_pretrained(config['model_name'], num_labels=config['num_labels'])\n",
    "\n",
    "    # I chose to use a MAD-X style configuration (using standard AdapterConfig)\n",
    "    # reason being, it typically uses smaller reduction factors for language adapters\n",
    "    lang_adapter_config = AdapterConfig.load(\"pfeiffer\", reduction_factor=2)\n",
    "    task_adapter_config = AdapterConfig.load(\"pfeiffer\", reduction_factor=16)\n",
    "\n",
    "    model.add_adapter(f\"lang_{source_language}\", config=lang_adapter_config)\n",
    "    model.add_adapter(f\"lang_{target_language}\", config=lang_adapter_config)\n",
    "\n",
    "    model.add_adapter(\"sentiment\", config=task_adapter_config)\n",
    "    model.add_classification_head(\"sentiment\", num_labels=config['num_labels'])\n",
    "\n",
    "    return model, tokenizer\n",
    "\n",
    "def train_cross_lingual_transfer(model, source_train_loader, target_train_loader,\n",
    "                               val_loader, config, source_adapter, target_adapter, task_adapter):\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model.to(device)\n",
    "\n",
    "    criterion = CrossEntropyLoss()\n",
    "\n",
    "    logger.info(\"Phase 1: Training source language + task adapters...\")\n",
    "\n",
    "    model.set_active_adapters([source_adapter, task_adapter])\n",
    "    model.train_adapter([source_adapter, task_adapter])\n",
    "\n",
    "    trainable_params = []\n",
    "    for name, param in model.named_parameters():\n",
    "        if param.requires_grad:\n",
    "            trainable_params.append(param)\n",
    "\n",
    "    optimizer_source = torch.optim.AdamW(trainable_params, lr=config['learning_rate'])\n",
    "\n",
    "    for epoch in range(config['epochs']):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "\n",
    "        progress_bar = tqdm(source_train_loader, desc=f\"Source Epoch {epoch+1}/{config['epochs']}\")\n",
    "\n",
    "        for batch_idx, batch in enumerate(progress_bar):\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "\n",
    "            optimizer_source.zero_grad()\n",
    "\n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            loss = criterion(outputs.logits, labels)\n",
    "\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(trainable_params, config['max_grad_norm'])\n",
    "            optimizer_source.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            progress_bar.set_postfix({'loss': loss.item()})\n",
    "\n",
    "        avg_loss = total_loss / len(source_train_loader)\n",
    "        logger.info(f\"Source Epoch {epoch+1}: Average Loss = {avg_loss:.4f}\")\n",
    "\n",
    "        val_accuracy = validate_model(model, val_loader, device)\n",
    "        logger.info(f\"Source Epoch {epoch+1}: Validation Accuracy = {val_accuracy:.4f}\")\n",
    "\n",
    "\n",
    "    logger.info(\"Phase 2: Training target language adapter...\")\n",
    "\n",
    "\n",
    "    model.set_active_adapters([target_adapter, task_adapter])\n",
    "    model.train_adapter([target_adapter])\n",
    "\n",
    "    target_params = []\n",
    "    for name, param in model.named_parameters():\n",
    "        if param.requires_grad and target_adapter in name:\n",
    "            target_params.append(param)\n",
    "\n",
    "\n",
    "    optimizer_target = torch.optim.AdamW(target_params, lr=config['learning_rate'] * 0.1)\n",
    "\n",
    "\n",
    "    for epoch in range(config['epochs']):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "\n",
    "        progress_bar = tqdm(target_train_loader, desc=f\"Target Epoch {epoch+1}/{config['epochs']}\")\n",
    "\n",
    "        for batch_idx, batch in enumerate(progress_bar):\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "\n",
    "            optimizer_target.zero_grad()\n",
    "\n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            loss = criterion(outputs.logits, labels)\n",
    "\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(target_params, config['max_grad_norm'])\n",
    "            optimizer_target.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            progress_bar.set_postfix({'loss': loss.item()})\n",
    "\n",
    "        avg_loss = total_loss / len(target_train_loader)\n",
    "        logger.info(f\"Target Epoch {epoch+1}: Average Loss = {avg_loss:.4f}\")\n",
    "\n",
    "        # Validation on target language\n",
    "        val_accuracy = validate_model(model, val_loader, device)\n",
    "        logger.info(f\"Target Epoch {epoch+1}: Validation Accuracy = {val_accuracy:.4f}\")\n",
    "\n",
    "    return model\n",
    "\n",
    "def validate_model(model, val_loader, device):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "\n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            _, predicted = torch.max(outputs.logits.data, 1)\n",
    "\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    return correct / total\n",
    "\n",
    "def load_pretrained_lang_adapters(model, source_language, target_language):\n",
    "    try:\n",
    "        model.load_adapter(f\"lang/{source_language}\", source=\"hf\", load_as=f\"lang_{source_language}\")\n",
    "        logger.info(f\"Loaded pretrained {source_language} adapter\")\n",
    "    except:\n",
    "        logger.warning(f\"No pretrained adapter found for {source_language}, using random initialization\")\n",
    "\n",
    "    try:\n",
    "        model.load_adapter(f\"lang/{target_language}\", source=\"hf\", load_as=f\"lang_{target_language}\")\n",
    "        logger.info(f\"Loaded pretrained {target_language} adapter\")\n",
    "    except:\n",
    "        logger.warning(f\"No pretrained adapter found for {target_language}, using random initialization\")\n",
    "\n",
    "    return model\n",
    "\n",
    "def train_afro_xlmr_adapter(source_language, target_language='sot'):\n",
    "    config = {\n",
    "        'extern_model_name': f'Davlan/afro-xlmr-base',\n",
    "        'model_name': f'./xmlr_sentiment_model_{source_language}',\n",
    "        'num_labels': 3,\n",
    "        'batch_size': 16,\n",
    "        'learning_rate': 2e-5,\n",
    "        'epochs': 3,\n",
    "        'warmup_steps': 0,\n",
    "        'max_grad_norm': 1.0,\n",
    "        'text_column': 'text',\n",
    "        'label_column': 'label',\n",
    "        'class_names': ['negative', 'neutral', 'positive']\n",
    "    }\n",
    "\n",
    "    model, tokenizer, src_adapter, tgt_adapter, task_adapter = setup_crosslingual_adapters(\n",
    "    config, source_language, target_language)\n",
    "\n",
    "    logger.info(\"Loading source data...\")\n",
    "    train_df, val_df, test_df = load_data(f'{source_language}')\n",
    "\n",
    "    logger.info(\"Creating data loaders...\")\n",
    "    source_train_loader, source_val_loader, source_test_loader = create_data_loaders(\n",
    "        train_df, val_df, test_df,\n",
    "        tokenizer,\n",
    "        batch_size=config['batch_size'],\n",
    "        text_column=config['text_column'],\n",
    "        label_column=config['label_column'],\n",
    "        lang=source_language\n",
    "    )\n",
    "\n",
    "    logger.info(\"Loading target data...\")\n",
    "    train_df, val_df, test_df = load_data(f'{target_language}')\n",
    "\n",
    "    logger.info(\"Creating data loaders...\")\n",
    "    target_train_loader, val_loader, test_loader = create_data_loaders(\n",
    "        train_df, val_df, test_df,\n",
    "        tokenizer,\n",
    "        batch_size=config['batch_size'],\n",
    "        text_column=config['text_column'],\n",
    "        label_column=config['label_column'],\n",
    "        lang=target_language\n",
    "    )\n",
    "\n",
    "\n",
    "    print(\"Starting sequential cross-lingual training...\")\n",
    "    model = train_cross_lingual_transfer(\n",
    "        model, source_train_loader, target_train_loader, val_loader,\n",
    "        config, src_adapter, tgt_adapter, task_adapter\n",
    "    )\n",
    "\n",
    "    try:\n",
    "        logger.info(\"Attempting to save model and tokenizer\")\n",
    "        model.save_pretrained(f\"./cross_lingual_model_{source_language}_{target_language}\")\n",
    "        tokenizer.save_pretrained(f\"./cross_lingual_model_{source_language}_{target_language}\")\n",
    "    except:\n",
    "        logger.info(\"Failed to save model and tokenizer\")\n",
    "\n",
    "    # Test evaluation: Showing results after training and adapters\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    logger.info(\"Evaluating on test set...\")\n",
    "    test_loss, test_accuracy, test_precision, test_recall, test_f1, test_preds, test_labels = evaluate(model, test_loader, device)\n",
    "\n",
    "    logger.info(f\"Test Results:\")\n",
    "    logger.info(f\"Loss: {test_loss:.4f}\")\n",
    "    logger.info(f\"Accuracy: {test_accuracy:.4f}\")\n",
    "    logger.info(f\"Precision: {test_precision:.4f}\")\n",
    "    logger.info(f\"Recall: {test_recall:.4f}\")\n",
    "    logger.info(f\"F1 Score: {test_f1:.4f}\")\n",
    "\n",
    "    results_df_afro = {\n",
    "    \"test_loss\": test_loss,\n",
    "    \"test_accuracy\": test_accuracy,\n",
    "    \"test_f1\": test_f1,\n",
    "    \"test_precision\": test_precision,\n",
    "    \"test_recall\": test_recall,\n",
    "    \"epochs\": config['epochs'],\n",
    "    \"learning_rate\": config['learning_rate'],\n",
    "    \"batch_size\": config['batch_size'],\n",
    "}\n",
    "\n",
    "    path = f\"afroxlmr_results_{target_language}_adapters.csv\"\n",
    "    pd.DataFrame([results_df_afro]).to_csv(path, index=False)\n",
    "    print(f\"Results saved to {path}\")\n",
    "\n",
    "    # Saving the adapters\n",
    "    model.save_adapter(f\"./adapters/{src_adapter}\", src_adapter)\n",
    "    model.save_adapter(f\"./adapters/{tgt_adapter}\", tgt_adapter)\n",
    "    model.save_adapter(f\"./adapters/{task_adapter}\", task_adapter)\n",
    "\n",
    "\n",
    "train_afro_xlmr_adapter('swa','sot')"
   ],
   "id": "84c591d289b9f914"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def evaluate(model, dataloader, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(dataloader, desc=\"Evaluating\"):\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "\n",
    "            outputs = model(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                labels=labels\n",
    "            )\n",
    "\n",
    "            loss = outputs.loss\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            logits = outputs.logits\n",
    "            preds = torch.argmax(logits, dim=1).cpu().numpy()\n",
    "\n",
    "            all_preds.extend(preds)\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    # Calculate metrics\n",
    "    accuracy = accuracy_score(all_labels, all_preds)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(\n",
    "        all_labels, all_preds, average='weighted'\n",
    "    )\n",
    "\n",
    "    return total_loss / len(dataloader), accuracy, precision, recall, f1, all_preds, all_labels\n",
    "\n",
    "\n",
    "def evaluate_afro_xlmr_adapter(source_language,target_language):\n",
    "    config = {\n",
    "        'extern_model_name': f'Davlan/afro-xlmr-base',\n",
    "        'model_name': f'./xmlr_sentiment_model_{source_language}',\n",
    "        'adapter_model_name': f'./cross_lingual_model_{source_language}_{target_language}',\n",
    "        'num_labels': 3,\n",
    "        'batch_size': 16,\n",
    "        'learning_rate': 2e-5,\n",
    "        'epochs': 3,\n",
    "        'warmup_steps': 0,\n",
    "        'max_grad_norm': 1.0,\n",
    "        'text_column': 'text',\n",
    "        'label_column': 'label',\n",
    "        'class_names': ['negative', 'neutral', 'positive']\n",
    "    }\n",
    "\n",
    "    tokenizer = XLMRobertaTokenizer.from_pretrained(config['extern_model_name'])\n",
    "\n",
    "    try:\n",
    "        model = AutoAdapterModel.from_pretrained(config['adapter_model_name'], num_labels=config['num_labels'])\n",
    "        logger.info(\"Successfully loaded as AutoAdapterModel\")\n",
    "    except:\n",
    "        exit(9)\n",
    "\n",
    "    logger.info(\"Loading target data...\")\n",
    "    train_df, val_df, test_df = load_data(f'{target_language}')\n",
    "\n",
    "    logger.info(\"Creating data loaders...\")\n",
    "    target_train_loader, val_loader, test_loader = create_data_loaders(\n",
    "        train_df, val_df, test_df,\n",
    "        tokenizer,\n",
    "        batch_size=config['batch_size'],\n",
    "        text_column=config['text_column'],\n",
    "        label_column=config['label_column'],\n",
    "        lang=target_language\n",
    "    )\n",
    "    # Test evaluation: Showing results after training and adapters\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    logger.info(\"Evaluating on test set...\")\n",
    "    test_loss, test_accuracy, test_precision, test_recall, test_f1, test_preds, test_labels = evaluate(model, test_loader, device)\n",
    "\n",
    "    logger.info(f\"Test Results:\")\n",
    "    logger.info(f\"Loss: {test_loss:.4f}\")\n",
    "    logger.info(f\"Accuracy: {test_accuracy:.4f}\")\n",
    "    logger.info(f\"Precision: {test_precision:.4f}\")\n",
    "    logger.info(f\"Recall: {test_recall:.4f}\")\n",
    "    logger.info(f\"F1 Score: {test_f1:.4f}\")\n",
    "\n",
    "    results_df_afro = {\n",
    "    \"test_loss\": test_loss,\n",
    "    \"test_accuracy\": test_accuracy,\n",
    "    \"test_f1\": test_f1,\n",
    "    \"test_precision\": test_precision,\n",
    "    \"test_recall\": test_recall,\n",
    "    \"epochs\": config['epochs'],\n",
    "    \"learning_rate\": config['learning_rate'],\n",
    "    \"batch_size\": config['batch_size'],\n",
    "}\n",
    "\n",
    "    path = f\"afroxlmr_results_{target_language}_adapters.csv\"\n",
    "    pd.DataFrame([results_df_afro]).to_csv(path, index=False)\n",
    "    print(f\"Results saved to {path}\")\n",
    "\n",
    "    # Saving the individual adapters\n",
    "    # model.save_adapter(f\"./adapters/{src_adapter}\", src_adapter)\n",
    "    # model.save_adapter(f\"./adapters/{tgt_adapter}\", tgt_adapter)\n",
    "    # model.save_adapter(f\"./adapters/{task_adapter}\", task_adapter)\n",
    "\n",
    "evaluate_afro_xlmr_adapter('swa','sot')"
   ],
   "id": "52cda5243087bc85"
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
