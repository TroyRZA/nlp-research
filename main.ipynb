{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "122d19e5-25f7-4b2e-9d44-f706fe7ddc36",
   "metadata": {},
   "source": [
    "# COS 760 Research Project: Analysing Sentiments for Low-resource African Languages"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3a3c357-1a7b-4731-bfb1-c3db6af9d407",
   "metadata": {},
   "source": [
    "## Group Members: Mihir Arjun, Troy Clark, Hamza Mokiwa"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Establishing Baselines with Monolingual Long Short-Term Memory networks(LSTMs) and pre-trained Multilingual transformers",
   "id": "566879365751e0a2"
  },
  {
   "cell_type": "markdown",
   "id": "d058d144-ec71-42a2-ba89-92e7d9062a0d",
   "metadata": {},
   "source": "### First we need to install the datasets"
  },
  {
   "cell_type": "code",
   "id": "01cc4f81-df43-4e94-8dc6-c84fef76a32c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-30T08:48:24.488062Z",
     "start_time": "2025-04-30T08:48:21.121613Z"
    }
   },
   "source": [
    "%pip install datasets"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: datasets in c:\\users\\user\\desktop\\up\\cos 760 (natural language processing)\\research\\environment\\lib\\site-packages (3.5.1)\n",
      "Requirement already satisfied: filelock in c:\\users\\user\\desktop\\up\\cos 760 (natural language processing)\\research\\environment\\lib\\site-packages (from datasets) (3.18.0)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\user\\desktop\\up\\cos 760 (natural language processing)\\research\\environment\\lib\\site-packages (from datasets) (2.2.5)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in c:\\users\\user\\desktop\\up\\cos 760 (natural language processing)\\research\\environment\\lib\\site-packages (from datasets) (20.0.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in c:\\users\\user\\desktop\\up\\cos 760 (natural language processing)\\research\\environment\\lib\\site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in c:\\users\\user\\desktop\\up\\cos 760 (natural language processing)\\research\\environment\\lib\\site-packages (from datasets) (2.2.3)\n",
      "Requirement already satisfied: requests>=2.32.2 in c:\\users\\user\\desktop\\up\\cos 760 (natural language processing)\\research\\environment\\lib\\site-packages (from datasets) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in c:\\users\\user\\desktop\\up\\cos 760 (natural language processing)\\research\\environment\\lib\\site-packages (from datasets) (4.67.1)\n",
      "Requirement already satisfied: xxhash in c:\\users\\user\\desktop\\up\\cos 760 (natural language processing)\\research\\environment\\lib\\site-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in c:\\users\\user\\desktop\\up\\cos 760 (natural language processing)\\research\\environment\\lib\\site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: fsspec[http]<=2025.3.0,>=2023.1.0 in c:\\users\\user\\desktop\\up\\cos 760 (natural language processing)\\research\\environment\\lib\\site-packages (from datasets) (2025.3.0)\n",
      "Requirement already satisfied: aiohttp in c:\\users\\user\\desktop\\up\\cos 760 (natural language processing)\\research\\environment\\lib\\site-packages (from datasets) (3.11.18)\n",
      "Requirement already satisfied: huggingface-hub>=0.24.0 in c:\\users\\user\\desktop\\up\\cos 760 (natural language processing)\\research\\environment\\lib\\site-packages (from datasets) (0.30.2)\n",
      "Requirement already satisfied: packaging in c:\\users\\user\\desktop\\up\\cos 760 (natural language processing)\\research\\environment\\lib\\site-packages (from datasets) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\user\\desktop\\up\\cos 760 (natural language processing)\\research\\environment\\lib\\site-packages (from datasets) (6.0.2)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in c:\\users\\user\\desktop\\up\\cos 760 (natural language processing)\\research\\environment\\lib\\site-packages (from aiohttp->datasets) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\user\\desktop\\up\\cos 760 (natural language processing)\\research\\environment\\lib\\site-packages (from aiohttp->datasets) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\user\\desktop\\up\\cos 760 (natural language processing)\\research\\environment\\lib\\site-packages (from aiohttp->datasets) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\user\\desktop\\up\\cos 760 (natural language processing)\\research\\environment\\lib\\site-packages (from aiohttp->datasets) (1.6.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\user\\desktop\\up\\cos 760 (natural language processing)\\research\\environment\\lib\\site-packages (from aiohttp->datasets) (6.4.3)\n",
      "Requirement already satisfied: propcache>=0.2.0 in c:\\users\\user\\desktop\\up\\cos 760 (natural language processing)\\research\\environment\\lib\\site-packages (from aiohttp->datasets) (0.3.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in c:\\users\\user\\desktop\\up\\cos 760 (natural language processing)\\research\\environment\\lib\\site-packages (from aiohttp->datasets) (1.20.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\user\\desktop\\up\\cos 760 (natural language processing)\\research\\environment\\lib\\site-packages (from huggingface-hub>=0.24.0->datasets) (4.13.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\user\\desktop\\up\\cos 760 (natural language processing)\\research\\environment\\lib\\site-packages (from requests>=2.32.2->datasets) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\user\\desktop\\up\\cos 760 (natural language processing)\\research\\environment\\lib\\site-packages (from requests>=2.32.2->datasets) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\user\\desktop\\up\\cos 760 (natural language processing)\\research\\environment\\lib\\site-packages (from requests>=2.32.2->datasets) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\user\\desktop\\up\\cos 760 (natural language processing)\\research\\environment\\lib\\site-packages (from requests>=2.32.2->datasets) (2025.4.26)\n",
      "Requirement already satisfied: colorama in c:\\users\\user\\desktop\\up\\cos 760 (natural language processing)\\research\\environment\\lib\\site-packages (from tqdm>=4.66.3->datasets) (0.4.6)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\user\\desktop\\up\\cos 760 (natural language processing)\\research\\environment\\lib\\site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\user\\desktop\\up\\cos 760 (natural language processing)\\research\\environment\\lib\\site-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\user\\desktop\\up\\cos 760 (natural language processing)\\research\\environment\\lib\\site-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\user\\desktop\\up\\cos 760 (natural language processing)\\research\\environment\\lib\\site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.2.1 -> 25.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "id": "054dd69e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-30T14:09:51.019371Z",
     "start_time": "2025-04-30T14:09:49.433052Z"
    }
   },
   "source": [
    "import os\n",
    "from datasets import load_from_disk, load_dataset\n",
    "\n",
    "def load_local_datasets():\n",
    "    # Define the paths to your local datasets\n",
    "    swa_path = \"./datasets/afrisenti/swa\"\n",
    "    por_path = \"./datasets/afrisenti/por\"\n",
    "    sot_path = \"./datasets/news\"\n",
    "    \n",
    "    # Check if the directories exist\n",
    "    if not all(os.path.exists(path) for path in [swa_path, por_path,sot_path]):\n",
    "        print(\"One or more dataset directories not found. Please check the paths.\")\n",
    "        return None, None, None\n",
    "    \n",
    "    # Load datasets from disk\n",
    "    print(\"Loading Swahili (swa) dataset from disk...\")\n",
    "    swa_dataset = load_from_disk(swa_path)\n",
    "    print(\"Swahili dataset loaded!\")\n",
    "    \n",
    "    print(\"Loading Portuguese (por) dataset from disk...\")\n",
    "    por_dataset = load_from_disk(por_path)\n",
    "    print(\"Portuguese dataset loaded!\")\n",
    "    \n",
    "    print(\"Loading Sesotho (sot) dataset from disk...\")\n",
    "    sot_dataset = load_dataset(\"csv\",  data_files=\"datasets/sotho-news/sotho_news_dataset.csv\")\n",
    "    print(\"Sesotho dataset loaded!\")\n",
    "\n",
    "    return swa_dataset, por_dataset, sot_dataset\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    swa, por, sot = load_local_datasets() #add call for Sesotho dataset here\n",
    "    \n",
    "    # Now you can work with these datasets\n",
    "    if swa is not None:\n",
    "        print(f\"Swahili dataset size: {len(swa['train'])} examples\")\n",
    "    if por is not None:\n",
    "        print(f\"Portuguese dataset size: {len(por['train'])} examples\")\n",
    "    if sot is not None:\n",
    "        print(f\"Sesotho dataset size: {len(sot['train'])} examples\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Swahili (swa) dataset from disk...\n",
      "Swahili dataset loaded!\n",
      "Loading Portuguese (por) dataset from disk...\n",
      "Portuguese dataset loaded!\n",
      "Loading Sesotho (sot) dataset from disk...\n",
      "Sesotho dataset loaded!\n",
      "Swahili dataset size: 1810 examples\n",
      "Portuguese dataset size: 3063 examples\n",
      "Sesotho dataset size: 2177 examples\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "cell_type": "markdown",
   "id": "0bf9822d",
   "metadata": {},
   "source": "### Now that the datasets have been loaded, we can start creating our LSTM baseline models below:"
  },
  {
   "cell_type": "markdown",
   "id": "7cc2a9bc",
   "metadata": {},
   "source": "#### First we will build an LSTM model for Swahili"
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3d28f49b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "Swahili dataset loaded successfully!\n",
      "Available columns in train split: ['tweet', 'label']\n",
      "Dataset structure: DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['tweet', 'label'],\n",
      "        num_rows: 1810\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['tweet', 'label'],\n",
      "        num_rows: 453\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['tweet', 'label'],\n",
      "        num_rows: 748\n",
      "    })\n",
      "})\n",
      "Train set size: 1810\n",
      "Test set size: 748\n",
      "Validation set size: 453\n",
      "Label mapping: {'negative': 0, 'neutral': 1, 'positive': 2}\n",
      "Vocabulary size: 3055\n",
      "Model architecture:\n",
      "LSTMSentiment(\n",
      "  (embedding): Embedding(3055, 100, padding_idx=0)\n",
      "  (lstm): LSTM(100, 128, num_layers=2, batch_first=True, dropout=0.2, bidirectional=True)\n",
      "  (fc): Linear(in_features=256, out_features=3, bias=True)\n",
      "  (dropout): Dropout(p=0.2, inplace=False)\n",
      ")\n",
      "Starting training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10 - Training: 100%|██████████| 57/57 [00:07<00:00,  7.55it/s]\n",
      "Epoch 1/10 - Validation: 100%|██████████| 15/15 [00:00<00:00, 36.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10:\n",
      "  Train Loss: 0.9255\n",
      "  Val Loss: 0.8964, Val Accuracy: 0.5872, Val F1: 0.4377\n",
      "  Saved new best model!\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/10 - Training: 100%|██████████| 57/57 [00:07<00:00,  7.84it/s]\n",
      "Epoch 2/10 - Validation: 100%|██████████| 15/15 [00:00<00:00, 42.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/10:\n",
      "  Train Loss: 0.8834\n",
      "  Val Loss: 0.9085, Val Accuracy: 0.5872, Val F1: 0.4720\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/10 - Training: 100%|██████████| 57/57 [00:06<00:00,  8.59it/s]\n",
      "Epoch 3/10 - Validation: 100%|██████████| 15/15 [00:00<00:00, 32.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/10:\n",
      "  Train Loss: 0.8355\n",
      "  Val Loss: 0.9070, Val Accuracy: 0.5960, Val F1: 0.5168\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/10 - Training: 100%|██████████| 57/57 [00:06<00:00,  8.93it/s]\n",
      "Epoch 4/10 - Validation: 100%|██████████| 15/15 [00:00<00:00, 32.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/10:\n",
      "  Train Loss: 0.7643\n",
      "  Val Loss: 0.9697, Val Accuracy: 0.5762, Val F1: 0.4923\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/10 - Training: 100%|██████████| 57/57 [00:06<00:00,  8.20it/s]\n",
      "Epoch 5/10 - Validation: 100%|██████████| 15/15 [00:00<00:00, 27.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/10:\n",
      "  Train Loss: 0.6388\n",
      "  Val Loss: 1.0302, Val Accuracy: 0.5497, Val F1: 0.5318\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/10 - Training: 100%|██████████| 57/57 [00:06<00:00,  8.23it/s]\n",
      "Epoch 6/10 - Validation: 100%|██████████| 15/15 [00:00<00:00, 29.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/10:\n",
      "  Train Loss: 0.5488\n",
      "  Val Loss: 1.2454, Val Accuracy: 0.5055, Val F1: 0.5120\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/10 - Training: 100%|██████████| 57/57 [00:07<00:00,  7.92it/s]\n",
      "Epoch 7/10 - Validation: 100%|██████████| 15/15 [00:00<00:00, 28.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/10:\n",
      "  Train Loss: 0.4460\n",
      "  Val Loss: 1.2486, Val Accuracy: 0.5585, Val F1: 0.5315\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/10 - Training: 100%|██████████| 57/57 [00:07<00:00,  7.64it/s]\n",
      "Epoch 8/10 - Validation: 100%|██████████| 15/15 [00:00<00:00, 26.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/10:\n",
      "  Train Loss: 0.3874\n",
      "  Val Loss: 1.5211, Val Accuracy: 0.4857, Val F1: 0.4953\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/10 - Training: 100%|██████████| 57/57 [00:07<00:00,  7.96it/s]\n",
      "Epoch 9/10 - Validation: 100%|██████████| 15/15 [00:00<00:00, 32.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/10:\n",
      "  Train Loss: 0.2739\n",
      "  Val Loss: 1.5772, Val Accuracy: 0.5055, Val F1: 0.5088\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/10 - Training: 100%|██████████| 57/57 [00:06<00:00,  8.60it/s]\n",
      "Epoch 10/10 - Validation: 100%|██████████| 15/15 [00:00<00:00, 35.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/10:\n",
      "  Train Loss: 0.2220\n",
      "  Val Loss: 1.5910, Val Accuracy: 0.5541, Val F1: 0.5337\n",
      "------------------------------------------------------------\n",
      "\n",
      "Evaluating on test set...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|██████████| 24/24 [00:00<00:00, 27.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.8965\n",
      "Test Accuracy: 0.5936\n",
      "Test F1 Score: 0.4446\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.00      0.00      0.00        80\n",
      "     neutral       0.59      1.00      0.74       444\n",
      "    positive       0.50      0.00      0.01       224\n",
      "\n",
      "    accuracy                           0.59       748\n",
      "   macro avg       0.36      0.33      0.25       748\n",
      "weighted avg       0.50      0.59      0.44       748\n",
      "\n",
      "Results saved to swahili_lstm_results.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "c:\\Mihir - University\\2025\\COS 760\\Project\\nlp-research\\.venv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Mihir - University\\2025\\COS 760\\Project\\nlp-research\\.venv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Mihir - University\\2025\\COS 760\\Project\\nlp-research\\.venv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datasets import load_from_disk\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from sklearn.metrics import accuracy_score, f1_score, classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "from collections import Counter\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# LSTM Hyperparameters\n",
    "EMBEDDING_DIM = 100\n",
    "HIDDEN_DIM = 128\n",
    "NUM_LAYERS = 2\n",
    "DROPOUT = 0.2\n",
    "LEARNING_RATE = 0.001\n",
    "BATCH_SIZE = 32\n",
    "NUM_EPOCHS = 10\n",
    "\n",
    "class SwahiliSentimentDataset(Dataset):\n",
    "    def __init__(self, tweets, labels, vocab, label_map):\n",
    "        self.tweets = tweets\n",
    "        self.labels = labels\n",
    "        self.vocab = vocab\n",
    "        self.label_map = label_map\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.tweets)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        tweet = self.tweets[idx]\n",
    "        label = self.labels[idx]\n",
    "        \n",
    "        # Convert text to indices using the vocabulary\n",
    "        tokenized = [self.vocab.get(word, self.vocab['<UNK>']) for word in tweet.split()]\n",
    "        return torch.tensor(tokenized, dtype=torch.long), torch.tensor(self.label_map[label], dtype=torch.long)\n",
    "\n",
    "def collate_fn(batch):\n",
    "    tweets, labels = zip(*batch)\n",
    "    # Pad sequences to the length of the longest sequence in the batch\n",
    "    tweets_padded = pad_sequence(tweets, batch_first=True, padding_value=0)\n",
    "    return tweets_padded, torch.stack(labels)\n",
    "\n",
    "class LSTMSentiment(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim, n_layers, dropout, pad_idx):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=pad_idx)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, num_layers=n_layers, \n",
    "                           bidirectional=True, dropout=dropout, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim * 2, output_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, text):\n",
    "        # text shape: [batch size, seq length]\n",
    "        embedded = self.dropout(self.embedding(text))\n",
    "        # embedded shape: [batch size, seq length, embedding dim]\n",
    "        \n",
    "        output, (hidden, cell) = self.lstm(embedded)\n",
    "        # hidden shape: [2*num_layers, batch size, hidden dim]\n",
    "        \n",
    "        # Concatenate the final forward and backward hidden states\n",
    "        hidden = torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim=1)\n",
    "        # hidden shape: [batch size, hidden dim * 2]\n",
    "        \n",
    "        hidden = self.dropout(hidden)\n",
    "        return self.fc(hidden)\n",
    "\n",
    "def train_model(model, train_loader, val_loader, optimizer, criterion, device, epochs):\n",
    "    best_val_loss = float('inf')\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        # Training\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        \n",
    "        for tweets, labels in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{epochs} - Training\"):\n",
    "            tweets, labels = tweets.to(device), labels.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model(tweets)\n",
    "            \n",
    "            # Calculate loss\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            # Backward pass and optimize\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            running_loss += loss.item()\n",
    "        \n",
    "        train_loss = running_loss / len(train_loader)\n",
    "        \n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        all_preds = []\n",
    "        all_labels = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for tweets, labels in tqdm(val_loader, desc=f\"Epoch {epoch+1}/{epochs} - Validation\"):\n",
    "                tweets, labels = tweets.to(device), labels.to(device)\n",
    "                \n",
    "                outputs = model(tweets)\n",
    "                loss = criterion(outputs, labels)\n",
    "                \n",
    "                val_loss += loss.item()\n",
    "                \n",
    "                _, preds = torch.max(outputs, 1)\n",
    "                all_preds.extend(preds.cpu().numpy())\n",
    "                all_labels.extend(labels.cpu().numpy())\n",
    "        \n",
    "        val_loss /= len(val_loader)\n",
    "        val_accuracy = accuracy_score(all_labels, all_preds)\n",
    "        val_f1 = f1_score(all_labels, all_preds, average='weighted')\n",
    "        \n",
    "        print(f\"Epoch {epoch+1}/{epochs}:\")\n",
    "        print(f\"  Train Loss: {train_loss:.4f}\")\n",
    "        print(f\"  Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.4f}, Val F1: {val_f1:.4f}\")\n",
    "        \n",
    "        # Save the best model\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            torch.save(model.state_dict(), \"best_swahili_lstm_model.pt\")\n",
    "            print(\"  Saved new best model!\")\n",
    "        \n",
    "        print(\"-\" * 60)\n",
    "    \n",
    "    return model\n",
    "\n",
    "def evaluate_model(model, test_loader, criterion, device, label_list):\n",
    "    model.eval()\n",
    "    test_loss = 0.0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for tweets, labels in tqdm(test_loader, desc=\"Testing\"):\n",
    "            tweets, labels = tweets.to(device), labels.to(device)\n",
    "            \n",
    "            outputs = model(tweets)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            test_loss += loss.item()\n",
    "            \n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "    \n",
    "    test_loss /= len(test_loader)\n",
    "    test_accuracy = accuracy_score(all_labels, all_preds)\n",
    "    test_f1 = f1_score(all_labels, all_preds, average='weighted')\n",
    "    \n",
    "    print(f\"Test Loss: {test_loss:.4f}\")\n",
    "    print(f\"Test Accuracy: {test_accuracy:.4f}\")\n",
    "    print(f\"Test F1 Score: {test_f1:.4f}\")\n",
    "    \n",
    "    # Print detailed classification report\n",
    "    class_names = [label_list[i] for i in range(len(label_list))]\n",
    "    report = classification_report(all_labels, all_preds, target_names=class_names)\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(report)\n",
    "    \n",
    "    return test_loss, test_accuracy, test_f1\n",
    "\n",
    "def main():\n",
    "    # Check if GPU is available\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print(f\"Using device: {device}\")\n",
    "    \n",
    "    # Load the SwahiliSenti dataset from local storage\n",
    "    try:\n",
    "        swa_dataset = load_from_disk(\"./datasets/afrisenti/swa\")\n",
    "        print(\"Swahili dataset loaded successfully!\")\n",
    "        \n",
    "        # Print out available columns to debug\n",
    "        print(f\"Available columns in train split: {swa_dataset['train'].column_names}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading dataset: {e}\")\n",
    "        return\n",
    "    \n",
    "    # Explore dataset structure\n",
    "    print(f\"Dataset structure: {swa_dataset}\")\n",
    "    print(f\"Train set size: {len(swa_dataset['train'])}\")\n",
    "    print(f\"Test set size: {len(swa_dataset['test'])}\")\n",
    "    print(f\"Validation set size: {len(swa_dataset['validation'])}\")\n",
    "    \n",
    "    # Extract data - using 'tweet' column instead of 'text'\n",
    "    train_tweets = swa_dataset['train']['tweet']\n",
    "    train_labels = swa_dataset['train']['label']\n",
    "    val_tweets = swa_dataset['validation']['tweet']\n",
    "    val_labels = swa_dataset['validation']['label']\n",
    "    test_tweets = swa_dataset['test']['tweet']\n",
    "    test_labels = swa_dataset['test']['label']\n",
    "    \n",
    "    # Get unique labels and create label to index mapping\n",
    "    # In AfriSenti, labels are usually 'positive', 'negative', 'neutral'\n",
    "    unique_labels = set(train_labels + val_labels + test_labels)\n",
    "    label_to_idx = {label: idx for idx, label in enumerate(unique_labels)}\n",
    "    idx_to_label = {idx: label for label, idx in label_to_idx.items()}\n",
    "    print(f\"Label mapping: {label_to_idx}\")\n",
    "    \n",
    "    # Build vocabulary from training data\n",
    "    word_counts = Counter()\n",
    "    for tweet in train_tweets:\n",
    "        word_counts.update(tweet.split())\n",
    "    \n",
    "    # Keep only words that appear at least 2 times\n",
    "    min_freq = 2\n",
    "    vocabulary = {'<PAD>': 0, '<UNK>': 1}\n",
    "    vocab_idx = 2\n",
    "    \n",
    "    for word, count in word_counts.items():\n",
    "        if count >= min_freq:\n",
    "            vocabulary[word] = vocab_idx\n",
    "            vocab_idx += 1\n",
    "    \n",
    "    print(f\"Vocabulary size: {len(vocabulary)}\")\n",
    "    \n",
    "    # Create datasets\n",
    "    train_dataset = SwahiliSentimentDataset(train_tweets, train_labels, vocabulary, label_to_idx)\n",
    "    val_dataset = SwahiliSentimentDataset(val_tweets, val_labels, vocabulary, label_to_idx)\n",
    "    test_dataset = SwahiliSentimentDataset(test_tweets, test_labels, vocabulary, label_to_idx)\n",
    "    \n",
    "    # Create dataloaders\n",
    "    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_fn)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, collate_fn=collate_fn)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, collate_fn=collate_fn)\n",
    "    \n",
    "    # Initialize model\n",
    "    model = LSTMSentiment(\n",
    "        vocab_size=len(vocabulary),\n",
    "        embedding_dim=EMBEDDING_DIM,\n",
    "        hidden_dim=HIDDEN_DIM,\n",
    "        output_dim=len(unique_labels),\n",
    "        n_layers=NUM_LAYERS,\n",
    "        dropout=DROPOUT,\n",
    "        pad_idx=vocabulary['<PAD>']\n",
    "    ).to(device)\n",
    "    \n",
    "    # Print model architecture\n",
    "    print(f\"Model architecture:\\n{model}\")\n",
    "    \n",
    "    # Define optimizer and loss function\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    # Train the model\n",
    "    print(\"Starting training...\")\n",
    "    model = train_model(\n",
    "        model=model,\n",
    "        train_loader=train_loader,\n",
    "        val_loader=val_loader,\n",
    "        optimizer=optimizer,\n",
    "        criterion=criterion,\n",
    "        device=device,\n",
    "        epochs=NUM_EPOCHS\n",
    "    )\n",
    "    \n",
    "    # Load best model for evaluation\n",
    "    model.load_state_dict(torch.load(\"best_swahili_lstm_model.pt\"))\n",
    "    \n",
    "    # Evaluate on test set\n",
    "    print(\"\\nEvaluating on test set...\")\n",
    "    test_loss, test_accuracy, test_f1 = evaluate_model(\n",
    "        model=model,\n",
    "        test_loader=test_loader,\n",
    "        criterion=criterion,\n",
    "        device=device,\n",
    "        label_list=list(idx_to_label.values())\n",
    "    )\n",
    "    \n",
    "    # Save evaluation results\n",
    "    results = {\n",
    "        \"test_loss\": test_loss,\n",
    "        \"test_accuracy\": test_accuracy,\n",
    "        \"test_f1\": test_f1,\n",
    "        \"embedding_dim\": EMBEDDING_DIM,\n",
    "        \"hidden_dim\": HIDDEN_DIM,\n",
    "        \"num_layers\": NUM_LAYERS,\n",
    "        \"dropout\": DROPOUT,\n",
    "        \"learning_rate\": LEARNING_RATE,\n",
    "        \"batch_size\": BATCH_SIZE,\n",
    "        \"epochs\": NUM_EPOCHS,\n",
    "        \"vocab_size\": len(vocabulary)\n",
    "    }\n",
    "    \n",
    "    # Save results to CSV\n",
    "    pd.DataFrame([results]).to_csv(\"swahili_lstm_results.csv\", index=False)\n",
    "    print(f\"Results saved to swahili_lstm_results.csv\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e693c96",
   "metadata": {},
   "source": "#### Next, we build an LSTM model for Mozambican Portuguese:"
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d62e04f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "Mozambican Portuguese dataset loaded successfully!\n",
      "Available columns in train split: ['tweet', 'label']\n",
      "Dataset structure: DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['tweet', 'label'],\n",
      "        num_rows: 3063\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['tweet', 'label'],\n",
      "        num_rows: 767\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['tweet', 'label'],\n",
      "        num_rows: 3662\n",
      "    })\n",
      "})\n",
      "Train set size: 3063\n",
      "Test set size: 3662\n",
      "Validation set size: 767\n",
      "Label mapping: {'negative': 0, 'neutral': 1, 'positive': 2}\n",
      "Vocabulary size: 4075\n",
      "Model architecture:\n",
      "LSTMSentiment(\n",
      "  (embedding): Embedding(4075, 100, padding_idx=0)\n",
      "  (lstm): LSTM(100, 128, num_layers=2, batch_first=True, dropout=0.2, bidirectional=True)\n",
      "  (fc): Linear(in_features=256, out_features=3, bias=True)\n",
      "  (dropout): Dropout(p=0.2, inplace=False)\n",
      ")\n",
      "Starting training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10 - Training: 100%|██████████| 96/96 [00:12<00:00,  7.80it/s]\n",
      "Epoch 1/10 - Validation: 100%|██████████| 24/24 [00:01<00:00, 19.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10:\n",
      "  Train Loss: 1.0226\n",
      "  Val Loss: 0.9994, Val Accuracy: 0.5215, Val F1: 0.3575\n",
      "  Saved new best model!\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/10 - Training: 100%|██████████| 96/96 [00:15<00:00,  6.37it/s]\n",
      "Epoch 2/10 - Validation: 100%|██████████| 24/24 [00:00<00:00, 24.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/10:\n",
      "  Train Loss: 0.9703\n",
      "  Val Loss: 0.9651, Val Accuracy: 0.5450, Val F1: 0.4652\n",
      "  Saved new best model!\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/10 - Training: 100%|██████████| 96/96 [00:13<00:00,  6.98it/s]\n",
      "Epoch 3/10 - Validation: 100%|██████████| 24/24 [00:00<00:00, 24.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/10:\n",
      "  Train Loss: 0.8965\n",
      "  Val Loss: 0.9932, Val Accuracy: 0.5567, Val F1: 0.4923\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/10 - Training: 100%|██████████| 96/96 [00:13<00:00,  6.95it/s]\n",
      "Epoch 4/10 - Validation: 100%|██████████| 24/24 [00:01<00:00, 20.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/10:\n",
      "  Train Loss: 0.7983\n",
      "  Val Loss: 0.9902, Val Accuracy: 0.5606, Val F1: 0.5459\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/10 - Training: 100%|██████████| 96/96 [00:14<00:00,  6.67it/s]\n",
      "Epoch 5/10 - Validation: 100%|██████████| 24/24 [00:01<00:00, 23.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/10:\n",
      "  Train Loss: 0.6935\n",
      "  Val Loss: 1.0411, Val Accuracy: 0.5528, Val F1: 0.5396\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/10 - Training: 100%|██████████| 96/96 [00:13<00:00,  6.92it/s]\n",
      "Epoch 6/10 - Validation: 100%|██████████| 24/24 [00:00<00:00, 25.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/10:\n",
      "  Train Loss: 0.5816\n",
      "  Val Loss: 1.1400, Val Accuracy: 0.5424, Val F1: 0.5358\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/10 - Training: 100%|██████████| 96/96 [00:13<00:00,  7.09it/s]\n",
      "Epoch 7/10 - Validation: 100%|██████████| 24/24 [00:01<00:00, 21.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/10:\n",
      "  Train Loss: 0.4691\n",
      "  Val Loss: 1.2334, Val Accuracy: 0.5593, Val F1: 0.5388\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/10 - Training: 100%|██████████| 96/96 [00:14<00:00,  6.71it/s]\n",
      "Epoch 8/10 - Validation: 100%|██████████| 24/24 [00:01<00:00, 23.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/10:\n",
      "  Train Loss: 0.3591\n",
      "  Val Loss: 1.4979, Val Accuracy: 0.5332, Val F1: 0.5243\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/10 - Training: 100%|██████████| 96/96 [00:13<00:00,  7.02it/s]\n",
      "Epoch 9/10 - Validation: 100%|██████████| 24/24 [00:00<00:00, 28.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/10:\n",
      "  Train Loss: 0.3022\n",
      "  Val Loss: 1.4727, Val Accuracy: 0.5111, Val F1: 0.5080\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/10 - Training: 100%|██████████| 96/96 [00:13<00:00,  6.88it/s]\n",
      "Epoch 10/10 - Validation: 100%|██████████| 24/24 [00:01<00:00, 21.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/10:\n",
      "  Train Loss: 0.2282\n",
      "  Val Loss: 1.6321, Val Accuracy: 0.5332, Val F1: 0.5234\n",
      "------------------------------------------------------------\n",
      "\n",
      "Evaluating on test set...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|██████████| 115/115 [00:04<00:00, 25.81it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.8757\n",
      "Test Accuracy: 0.6393\n",
      "Test F1 Score: 0.5655\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.31      0.15      0.21       655\n",
      "     neutral       0.67      0.92      0.78      2379\n",
      "    positive       0.62      0.08      0.14       628\n",
      "\n",
      "    accuracy                           0.64      3662\n",
      "   macro avg       0.54      0.38      0.37      3662\n",
      "weighted avg       0.60      0.64      0.57      3662\n",
      "\n",
      "Results saved to portuguese_lstm_results.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datasets import load_from_disk\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from sklearn.metrics import accuracy_score, f1_score, classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "from collections import Counter\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# LSTM Hyperparameters\n",
    "EMBEDDING_DIM = 100\n",
    "HIDDEN_DIM = 128\n",
    "NUM_LAYERS = 2\n",
    "DROPOUT = 0.2\n",
    "LEARNING_RATE = 0.001\n",
    "BATCH_SIZE = 32\n",
    "NUM_EPOCHS = 10\n",
    "\n",
    "class PorSentimentDataset(Dataset):\n",
    "    def __init__(self, tweets, labels, vocab, label_map):\n",
    "        self.tweets = tweets\n",
    "        self.labels = labels\n",
    "        self.vocab = vocab\n",
    "        self.label_map = label_map\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.tweets)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        tweet = self.tweets[idx]\n",
    "        label = self.labels[idx]\n",
    "        \n",
    "        # Convert text to indices using the vocabulary\n",
    "        tokenized = [self.vocab.get(word, self.vocab['<UNK>']) for word in tweet.split()]\n",
    "        return torch.tensor(tokenized, dtype=torch.long), torch.tensor(self.label_map[label], dtype=torch.long)\n",
    "\n",
    "def collate_fn(batch):\n",
    "    tweets, labels = zip(*batch)\n",
    "    # Pad sequences to the length of the longest sequence in the batch\n",
    "    tweets_padded = pad_sequence(tweets, batch_first=True, padding_value=0)\n",
    "    return tweets_padded, torch.stack(labels)\n",
    "\n",
    "class LSTMSentiment(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim, n_layers, dropout, pad_idx):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=pad_idx)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, num_layers=n_layers, \n",
    "                           bidirectional=True, dropout=dropout, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim * 2, output_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, text):\n",
    "        # text shape: [batch size, seq length]\n",
    "        embedded = self.dropout(self.embedding(text))\n",
    "        # embedded shape: [batch size, seq length, embedding dim]\n",
    "        \n",
    "        output, (hidden, cell) = self.lstm(embedded)\n",
    "        # hidden shape: [2*num_layers, batch size, hidden dim]\n",
    "        \n",
    "        # Concatenate the final forward and backward hidden states\n",
    "        hidden = torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim=1)\n",
    "        # hidden shape: [batch size, hidden dim * 2]\n",
    "        \n",
    "        hidden = self.dropout(hidden)\n",
    "        return self.fc(hidden)\n",
    "\n",
    "def train_model(model, train_loader, val_loader, optimizer, criterion, device, epochs):\n",
    "    best_val_loss = float('inf')\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        # Training\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        \n",
    "        for tweets, labels in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{epochs} - Training\"):\n",
    "            tweets, labels = tweets.to(device), labels.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model(tweets)\n",
    "            \n",
    "            # Calculate loss\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            # Backward pass and optimize\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            running_loss += loss.item()\n",
    "        \n",
    "        train_loss = running_loss / len(train_loader)\n",
    "        \n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        all_preds = []\n",
    "        all_labels = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for tweets, labels in tqdm(val_loader, desc=f\"Epoch {epoch+1}/{epochs} - Validation\"):\n",
    "                tweets, labels = tweets.to(device), labels.to(device)\n",
    "                \n",
    "                outputs = model(tweets)\n",
    "                loss = criterion(outputs, labels)\n",
    "                \n",
    "                val_loss += loss.item()\n",
    "                \n",
    "                _, preds = torch.max(outputs, 1)\n",
    "                all_preds.extend(preds.cpu().numpy())\n",
    "                all_labels.extend(labels.cpu().numpy())\n",
    "        \n",
    "        val_loss /= len(val_loader)\n",
    "        val_accuracy = accuracy_score(all_labels, all_preds)\n",
    "        val_f1 = f1_score(all_labels, all_preds, average='weighted')\n",
    "        \n",
    "        print(f\"Epoch {epoch+1}/{epochs}:\")\n",
    "        print(f\"  Train Loss: {train_loss:.4f}\")\n",
    "        print(f\"  Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.4f}, Val F1: {val_f1:.4f}\")\n",
    "        \n",
    "        # Save the best model\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            torch.save(model.state_dict(), \"best_portuguese_lstm_model.pt\")\n",
    "            print(\"  Saved new best model!\")\n",
    "        \n",
    "        print(\"-\" * 60)\n",
    "    \n",
    "    return model\n",
    "\n",
    "def evaluate_model(model, test_loader, criterion, device, label_list):\n",
    "    model.eval()\n",
    "    test_loss = 0.0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for tweets, labels in tqdm(test_loader, desc=\"Testing\"):\n",
    "            tweets, labels = tweets.to(device), labels.to(device)\n",
    "            \n",
    "            outputs = model(tweets)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            test_loss += loss.item()\n",
    "            \n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "    \n",
    "    test_loss /= len(test_loader)\n",
    "    test_accuracy = accuracy_score(all_labels, all_preds)\n",
    "    test_f1 = f1_score(all_labels, all_preds, average='weighted')\n",
    "    \n",
    "    print(f\"Test Loss: {test_loss:.4f}\")\n",
    "    print(f\"Test Accuracy: {test_accuracy:.4f}\")\n",
    "    print(f\"Test F1 Score: {test_f1:.4f}\")\n",
    "    \n",
    "    # Print detailed classification report\n",
    "    class_names = [label_list[i] for i in range(len(label_list))]\n",
    "    report = classification_report(all_labels, all_preds, target_names=class_names)\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(report)\n",
    "    \n",
    "    return test_loss, test_accuracy, test_f1\n",
    "\n",
    "def main():\n",
    "    # Check if GPU is available\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print(f\"Using device: {device}\")\n",
    "    \n",
    "    # Load the Portuguese dataset from local storage\n",
    "    try:\n",
    "        por_dataset = load_from_disk(\"./datasets/afrisenti/por\")\n",
    "        print(\"Mozambican Portuguese dataset loaded successfully!\")\n",
    "        \n",
    "        # Print out available columns to debug\n",
    "        print(f\"Available columns in train split: {por_dataset['train'].column_names}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading dataset: {e}\")\n",
    "        return\n",
    "    \n",
    "    # Explore dataset structure\n",
    "    print(f\"Dataset structure: {por_dataset}\")\n",
    "    print(f\"Train set size: {len(por_dataset['train'])}\")\n",
    "    print(f\"Test set size: {len(por_dataset['test'])}\")\n",
    "    print(f\"Validation set size: {len(por_dataset['validation'])}\")\n",
    "    \n",
    "    # Extract data - using 'tweet' column instead of 'text'\n",
    "    train_tweets = por_dataset['train']['tweet']\n",
    "    train_labels = por_dataset['train']['label']\n",
    "    val_tweets = por_dataset['validation']['tweet']\n",
    "    val_labels = por_dataset['validation']['label']\n",
    "    test_tweets = por_dataset['test']['tweet']\n",
    "    test_labels = por_dataset['test']['label']\n",
    "    \n",
    "    # Get unique labels and create label to index mapping\n",
    "    # In AfriSenti, labels are usually 'positive', 'negative', 'neutral'\n",
    "    unique_labels = set(train_labels + val_labels + test_labels)\n",
    "    label_to_idx = {label: idx for idx, label in enumerate(unique_labels)}\n",
    "    idx_to_label = {idx: label for label, idx in label_to_idx.items()}\n",
    "    print(f\"Label mapping: {label_to_idx}\")\n",
    "    \n",
    "    # Build vocabulary from training data\n",
    "    word_counts = Counter()\n",
    "    for tweet in train_tweets:\n",
    "        word_counts.update(tweet.split())\n",
    "    \n",
    "    # Keep only words that appear at least 2 times\n",
    "    min_freq = 2\n",
    "    vocabulary = {'<PAD>': 0, '<UNK>': 1}\n",
    "    vocab_idx = 2\n",
    "    \n",
    "    for word, count in word_counts.items():\n",
    "        if count >= min_freq:\n",
    "            vocabulary[word] = vocab_idx\n",
    "            vocab_idx += 1\n",
    "    \n",
    "    print(f\"Vocabulary size: {len(vocabulary)}\")\n",
    "    \n",
    "    # Create datasets\n",
    "    train_dataset = PorSentimentDataset(train_tweets, train_labels, vocabulary, label_to_idx)\n",
    "    val_dataset = PorSentimentDataset(val_tweets, val_labels, vocabulary, label_to_idx)\n",
    "    test_dataset = PorSentimentDataset(test_tweets, test_labels, vocabulary, label_to_idx)\n",
    "    \n",
    "    # Create dataloaders\n",
    "    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_fn)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, collate_fn=collate_fn)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, collate_fn=collate_fn)\n",
    "    \n",
    "    # Initialize model\n",
    "    model = LSTMSentiment(\n",
    "        vocab_size=len(vocabulary),\n",
    "        embedding_dim=EMBEDDING_DIM,\n",
    "        hidden_dim=HIDDEN_DIM,\n",
    "        output_dim=len(unique_labels),\n",
    "        n_layers=NUM_LAYERS,\n",
    "        dropout=DROPOUT,\n",
    "        pad_idx=vocabulary['<PAD>']\n",
    "    ).to(device)\n",
    "    \n",
    "    # Print model architecture\n",
    "    print(f\"Model architecture:\\n{model}\")\n",
    "    \n",
    "    # Define optimizer and loss function\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    # Train the model\n",
    "    print(\"Starting training...\")\n",
    "    model = train_model(\n",
    "        model=model,\n",
    "        train_loader=train_loader,\n",
    "        val_loader=val_loader,\n",
    "        optimizer=optimizer,\n",
    "        criterion=criterion,\n",
    "        device=device,\n",
    "        epochs=NUM_EPOCHS\n",
    "    )\n",
    "    \n",
    "    # Load best model for evaluation\n",
    "    model.load_state_dict(torch.load(\"best_portuguese_lstm_model.pt\"))\n",
    "    \n",
    "    # Evaluate on test set\n",
    "    print(\"\\nEvaluating on test set...\")\n",
    "    test_loss, test_accuracy, test_f1 = evaluate_model(\n",
    "        model=model,\n",
    "        test_loader=test_loader,\n",
    "        criterion=criterion,\n",
    "        device=device,\n",
    "        label_list=list(idx_to_label.values())\n",
    "    )\n",
    "    \n",
    "    # Save evaluation results\n",
    "    results = {\n",
    "        \"test_loss\": test_loss,\n",
    "        \"test_accuracy\": test_accuracy,\n",
    "        \"test_f1\": test_f1,\n",
    "        \"embedding_dim\": EMBEDDING_DIM,\n",
    "        \"hidden_dim\": HIDDEN_DIM,\n",
    "        \"num_layers\": NUM_LAYERS,\n",
    "        \"dropout\": DROPOUT,\n",
    "        \"learning_rate\": LEARNING_RATE,\n",
    "        \"batch_size\": BATCH_SIZE,\n",
    "        \"epochs\": NUM_EPOCHS,\n",
    "        \"vocab_size\": len(vocabulary)\n",
    "    }\n",
    "    \n",
    "    # Save results to CSV\n",
    "    pd.DataFrame([results]).to_csv(\"portuguese_lstm_results.csv\", index=False)\n",
    "    print(f\"Results saved to portuguese_lstm_results.csv\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Lastly, we build an LSTM model for Sesotho",
   "id": "23b24b1c44eb59b2"
  },
  {
   "cell_type": "code",
   "id": "0ed05fc8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-03T07:44:06.391800Z",
     "start_time": "2025-05-03T07:43:58.107727Z"
    }
   },
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datasets import load_from_disk\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from sklearn.metrics import accuracy_score, f1_score, classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "from collections import Counter\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# LSTM Hyperparameters\n",
    "EMBEDDING_DIM = 100\n",
    "HIDDEN_DIM = 128\n",
    "NUM_LAYERS = 2\n",
    "DROPOUT = 0.2\n",
    "LEARNING_RATE = 0.001\n",
    "BATCH_SIZE = 32\n",
    "NUM_EPOCHS = 10\n",
    "\n",
    "class SotSentimentDataset(Dataset):\n",
    "    def __init__(self, headlines, labels, vocab, label_map):\n",
    "        self.headlines = headlines\n",
    "        self.labels = labels\n",
    "        self.vocab = vocab\n",
    "        self.label_map = label_map\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.headlines)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        tweet = self.headlines[idx]\n",
    "        label = self.labels[idx]\n",
    "\n",
    "        # Convert text to indices using the vocabulary\n",
    "        tokenized = [self.vocab.get(word, self.vocab['<UNK>']) for word in tweet.split()]\n",
    "        return torch.tensor(tokenized, dtype=torch.long), torch.tensor(self.label_map[label], dtype=torch.long)\n",
    "\n",
    "def collate_fn(batch):\n",
    "    headlines, labels = zip(*batch)\n",
    "    # Pad sequences to the length of the longest sequence in the batch\n",
    "    headlines_padded = pad_sequence(headlines, batch_first=True, padding_value=0)\n",
    "    return headlines_padded, torch.stack(labels)\n",
    "\n",
    "class LSTMSentiment(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim, n_layers, dropout, pad_idx):\n",
    "        super().__init__()\n",
    "\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=pad_idx)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, num_layers=n_layers,\n",
    "                           bidirectional=True, dropout=dropout, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim * 2, output_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, text):\n",
    "        # text shape: [batch size, seq length]\n",
    "        embedded = self.dropout(self.embedding(text))\n",
    "        # embedded shape: [batch size, seq length, embedding dim]\n",
    "\n",
    "        output, (hidden, cell) = self.lstm(embedded)\n",
    "        # hidden shape: [2*num_layers, batch size, hidden dim]\n",
    "\n",
    "        # Concatenate the final forward and backward hidden states\n",
    "        hidden = torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim=1)\n",
    "        # hidden shape: [batch size, hidden dim * 2]\n",
    "\n",
    "        hidden = self.dropout(hidden)\n",
    "        return self.fc(hidden)\n",
    "\n",
    "def train_model(model, train_loader, val_loader, optimizer, criterion, device, epochs):\n",
    "    best_val_loss = float('inf')\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        # Training\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "\n",
    "        for headlines, labels in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{epochs} - Training\"):\n",
    "            headlines, labels = headlines.to(device), labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(headlines)\n",
    "\n",
    "            # Calculate loss\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            # Backward pass and optimize\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "\n",
    "        train_loss = running_loss / len(train_loader)\n",
    "\n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        all_preds = []\n",
    "        all_labels = []\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for headlines, labels in tqdm(val_loader, desc=f\"Epoch {epoch+1}/{epochs} - Validation\"):\n",
    "                headlines, labels = headlines.to(device), labels.to(device)\n",
    "\n",
    "                outputs = model(headlines)\n",
    "                loss = criterion(outputs, labels)\n",
    "\n",
    "                val_loss += loss.item()\n",
    "\n",
    "                _, preds = torch.max(outputs, 1)\n",
    "                all_preds.extend(preds.cpu().numpy())\n",
    "                all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "        val_loss /= len(val_loader)\n",
    "        val_accuracy = accuracy_score(all_labels, all_preds)\n",
    "        val_f1 = f1_score(all_labels, all_preds, average='weighted')\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{epochs}:\")\n",
    "        print(f\"  Train Loss: {train_loss:.4f}\")\n",
    "        print(f\"  Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.4f}, Val F1: {val_f1:.4f}\")\n",
    "\n",
    "        # Save the best model\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            torch.save(model.state_dict(), \"best_portuguese_lstm_model.pt\")\n",
    "            print(\"  Saved new best model!\")\n",
    "\n",
    "        print(\"-\" * 60)\n",
    "\n",
    "    return model\n",
    "\n",
    "def evaluate_model(model, test_loader, criterion, device, label_list):\n",
    "    model.eval()\n",
    "    test_loss = 0.0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for headlines, labels in tqdm(test_loader, desc=\"Testing\"):\n",
    "            headlines, labels = headlines.to(device), labels.to(device)\n",
    "\n",
    "            outputs = model(headlines)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            test_loss += loss.item()\n",
    "\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    test_loss /= len(test_loader)\n",
    "    test_accuracy = accuracy_score(all_labels, all_preds)\n",
    "    test_f1 = f1_score(all_labels, all_preds, average='weighted')\n",
    "\n",
    "    print(f\"Test Loss: {test_loss:.4f}\")\n",
    "    print(f\"Test Accuracy: {test_accuracy:.4f}\")\n",
    "    print(f\"Test F1 Score: {test_f1:.4f}\")\n",
    "\n",
    "    # Print detailed classification report\n",
    "    class_names = [label_list[i] for i in range(len(label_list))]\n",
    "    report = classification_report(all_labels, all_preds, target_names=class_names)\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(report)\n",
    "\n",
    "    return test_loss, test_accuracy, test_f1\n",
    "\n",
    "def main():\n",
    "    # Check if GPU is available\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print(f\"Using device: {device}\")\n",
    "\n",
    "    # Load the Sesotho dataset from local storage\n",
    "    try:\n",
    "        sot_dataset = load_from_disk(\"./datasets/sesotho_news_dataset\")\n",
    "        print(\"Sesotho dataset loaded successfully!\")\n",
    "\n",
    "        # Print out available columns to debug\n",
    "        print(f\"Available columns in train split: {sot_dataset['train'].column_names}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading dataset: {e}\")\n",
    "        return\n",
    "\n",
    "    # Explore dataset structure\n",
    "    print(f\"Dataset structure: {sot_dataset}\")\n",
    "    print(f\"Train set size: {len(sot_dataset['train'])}\")\n",
    "    print(f\"Test set size: {len(sot_dataset['test'])}\")\n",
    "    print(f\"Validation set size: {len(sot_dataset['validation'])}\")\n",
    "\n",
    "    # Extract data - using 'headline' column instead of 'text'\n",
    "    train_headlines = sot_dataset['train']['headline']\n",
    "    train_labels = sot_dataset['train']['label']\n",
    "    val_headlines = sot_dataset['validation']['headline']\n",
    "    val_labels = sot_dataset['validation']['label']\n",
    "    test_headlines = sot_dataset['test']['headline']\n",
    "    test_labels = sot_dataset['test']['label']\n",
    "\n",
    "    # Get unique labels and create label to index mapping\n",
    "    # The labels in the sesotho news dataset were originally (-1,0,1) but were changed to 'positive', 'negative', 'neutral'\n",
    "    unique_labels = set(train_labels + val_labels + test_labels)\n",
    "    label_to_idx = {label: idx for idx, label in enumerate(unique_labels)}\n",
    "    idx_to_label = {idx: label for label, idx in label_to_idx.items()}\n",
    "    print(f\"Label mapping: {label_to_idx}\")\n",
    "\n",
    "    # Build vocabulary from training data\n",
    "    word_counts = Counter()\n",
    "    for headline in train_headlines:\n",
    "        word_counts.update(headline.split())\n",
    "\n",
    "    # Keep only words that appear at least 2 times\n",
    "    min_freq = 2\n",
    "    vocabulary = {'<PAD>': 0, '<UNK>': 1}\n",
    "    vocab_idx = 2\n",
    "\n",
    "    for word, count in word_counts.items():\n",
    "        if count >= min_freq:\n",
    "            vocabulary[word] = vocab_idx\n",
    "            vocab_idx += 1\n",
    "\n",
    "    print(f\"Vocabulary size: {len(vocabulary)}\")\n",
    "\n",
    "    # Create datasets\n",
    "    train_dataset = SotSentimentDataset(train_headlines, train_labels, vocabulary, label_to_idx)\n",
    "    val_dataset = SotSentimentDataset(val_headlines, val_labels, vocabulary, label_to_idx)\n",
    "    test_dataset = SotSentimentDataset(test_headlines, test_labels, vocabulary, label_to_idx)\n",
    "\n",
    "    # Create dataloaders\n",
    "    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_fn)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, collate_fn=collate_fn)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, collate_fn=collate_fn)\n",
    "\n",
    "    # Initialize model\n",
    "    model = LSTMSentiment(\n",
    "        vocab_size=len(vocabulary),\n",
    "        embedding_dim=EMBEDDING_DIM,\n",
    "        hidden_dim=HIDDEN_DIM,\n",
    "        output_dim=len(unique_labels),\n",
    "        n_layers=NUM_LAYERS,\n",
    "        dropout=DROPOUT,\n",
    "        pad_idx=vocabulary['<PAD>']\n",
    "    ).to(device)\n",
    "\n",
    "    # Print model architecture\n",
    "    print(f\"Model architecture:\\n{model}\")\n",
    "\n",
    "    # Define optimizer and loss function\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    # Train the model\n",
    "    print(\"Starting training...\")\n",
    "    model = train_model(\n",
    "        model=model,\n",
    "        train_loader=train_loader,\n",
    "        val_loader=val_loader,\n",
    "        optimizer=optimizer,\n",
    "        criterion=criterion,\n",
    "        device=device,\n",
    "        epochs=NUM_EPOCHS\n",
    "    )\n",
    "\n",
    "    # Save the model's state_dict\n",
    "    torch.save(model.state_dict(), \"best_sesotho_lstm_model.pt\");\n",
    "\n",
    "    # Load best model for evaluation\n",
    "    model.load_state_dict(torch.load(\"best_sesotho_lstm_model.pt\"))\n",
    "\n",
    "    # Evaluate on test set\n",
    "    print(\"\\nEvaluating on test set...\")\n",
    "    test_loss, test_accuracy, test_f1 = evaluate_model(\n",
    "        model=model,\n",
    "        test_loader=test_loader,\n",
    "        criterion=criterion,\n",
    "        device=device,\n",
    "        label_list=list(idx_to_label.values())\n",
    "    )\n",
    "\n",
    "    # Save evaluation results\n",
    "    results = {\n",
    "        \"test_loss\": test_loss,\n",
    "        \"test_accuracy\": test_accuracy,\n",
    "        \"test_f1\": test_f1,\n",
    "        \"embedding_dim\": EMBEDDING_DIM,\n",
    "        \"hidden_dim\": HIDDEN_DIM,\n",
    "        \"num_layers\": NUM_LAYERS,\n",
    "        \"dropout\": DROPOUT,\n",
    "        \"learning_rate\": LEARNING_RATE,\n",
    "        \"batch_size\": BATCH_SIZE,\n",
    "        \"epochs\": NUM_EPOCHS,\n",
    "        \"vocab_size\": len(vocabulary)\n",
    "    }\n",
    "\n",
    "    # Save results to CSV\n",
    "    pd.DataFrame([results]).to_csv(\"sesotho_lstm_results.csv\", index=False)\n",
    "    print(f\"Results saved to sesotho_lstm_results.csv\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "Sesotho dataset loaded successfully!\n",
      "Available columns in train split: ['headline', 'label', '__index_level_0__']\n",
      "Dataset structure: DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['headline', 'label', '__index_level_0__'],\n",
      "        num_rows: 1305\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['headline', 'label', '__index_level_0__'],\n",
      "        num_rows: 436\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['headline', 'label', '__index_level_0__'],\n",
      "        num_rows: 436\n",
      "    })\n",
      "})\n",
      "Train set size: 1305\n",
      "Test set size: 436\n",
      "Validation set size: 436\n",
      "Label mapping: {'neutral': 0, 'negative': 1, 'positive': 2}\n",
      "Vocabulary size: 876\n",
      "Model architecture:\n",
      "LSTMSentiment(\n",
      "  (embedding): Embedding(876, 100, padding_idx=0)\n",
      "  (lstm): LSTM(100, 128, num_layers=2, batch_first=True, dropout=0.2, bidirectional=True)\n",
      "  (fc): Linear(in_features=256, out_features=3, bias=True)\n",
      "  (dropout): Dropout(p=0.2, inplace=False)\n",
      ")\n",
      "Starting training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10 - Training: 100%|██████████| 41/41 [00:00<00:00, 50.00it/s]\n",
      "Epoch 1/10 - Validation: 100%|██████████| 14/14 [00:00<00:00, 212.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10:\n",
      "  Train Loss: 0.7751\n",
      "  Val Loss: 0.7289, Val Accuracy: 0.6950, Val F1: 0.5699\n",
      "  Saved new best model!\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/10 - Training: 100%|██████████| 41/41 [00:00<00:00, 51.51it/s]\n",
      "Epoch 2/10 - Validation: 100%|██████████| 14/14 [00:00<00:00, 202.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/10:\n",
      "  Train Loss: 0.6275\n",
      "  Val Loss: 0.7368, Val Accuracy: 0.7179, Val F1: 0.6419\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/10 - Training: 100%|██████████| 41/41 [00:00<00:00, 52.90it/s]\n",
      "Epoch 3/10 - Validation: 100%|██████████| 14/14 [00:00<00:00, 215.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/10:\n",
      "  Train Loss: 0.5516\n",
      "  Val Loss: 0.7012, Val Accuracy: 0.7317, Val F1: 0.6920\n",
      "  Saved new best model!\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/10 - Training: 100%|██████████| 41/41 [00:00<00:00, 56.79it/s]\n",
      "Epoch 4/10 - Validation: 100%|██████████| 14/14 [00:00<00:00, 200.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/10:\n",
      "  Train Loss: 0.5039\n",
      "  Val Loss: 0.7370, Val Accuracy: 0.7385, Val F1: 0.6711\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/10 - Training: 100%|██████████| 41/41 [00:00<00:00, 57.91it/s]\n",
      "Epoch 5/10 - Validation: 100%|██████████| 14/14 [00:00<00:00, 212.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/10:\n",
      "  Train Loss: 0.4261\n",
      "  Val Loss: 0.7007, Val Accuracy: 0.7271, Val F1: 0.7154\n",
      "  Saved new best model!\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/10 - Training: 100%|██████████| 41/41 [00:00<00:00, 55.86it/s]\n",
      "Epoch 6/10 - Validation: 100%|██████████| 14/14 [00:00<00:00, 215.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/10:\n",
      "  Train Loss: 0.3852\n",
      "  Val Loss: 0.7299, Val Accuracy: 0.7638, Val F1: 0.7379\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/10 - Training: 100%|██████████| 41/41 [00:00<00:00, 59.33it/s]\n",
      "Epoch 7/10 - Validation: 100%|██████████| 14/14 [00:00<00:00, 218.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/10:\n",
      "  Train Loss: 0.2882\n",
      "  Val Loss: 0.7523, Val Accuracy: 0.7454, Val F1: 0.7287\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/10 - Training: 100%|██████████| 41/41 [00:00<00:00, 58.32it/s]\n",
      "Epoch 8/10 - Validation: 100%|██████████| 14/14 [00:00<00:00, 218.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/10:\n",
      "  Train Loss: 0.2573\n",
      "  Val Loss: 0.7572, Val Accuracy: 0.7156, Val F1: 0.7123\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/10 - Training: 100%|██████████| 41/41 [00:00<00:00, 57.66it/s]\n",
      "Epoch 9/10 - Validation: 100%|██████████| 14/14 [00:00<00:00, 199.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/10:\n",
      "  Train Loss: 0.1974\n",
      "  Val Loss: 0.8882, Val Accuracy: 0.7271, Val F1: 0.7143\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/10 - Training: 100%|██████████| 41/41 [00:00<00:00, 53.81it/s]\n",
      "Epoch 10/10 - Validation: 100%|██████████| 14/14 [00:00<00:00, 222.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/10:\n",
      "  Train Loss: 0.1676\n",
      "  Val Loss: 0.9858, Val Accuracy: 0.7385, Val F1: 0.7134\n",
      "------------------------------------------------------------\n",
      "\n",
      "Evaluating on test set...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|██████████| 14/14 [00:00<00:00, 237.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.9303\n",
      "Test Accuracy: 0.7202\n",
      "Test F1 Score: 0.7024\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     neutral       0.83      0.25      0.38        20\n",
      "    negative       0.77      0.87      0.82       308\n",
      "    positive       0.49      0.38      0.43       108\n",
      "\n",
      "    accuracy                           0.72       436\n",
      "   macro avg       0.70      0.50      0.54       436\n",
      "weighted avg       0.71      0.72      0.70       436\n",
      "\n",
      "Results saved to sesotho_lstm_results.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-03T07:43:38.538623Z",
     "start_time": "2025-05-03T07:43:38.497686Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "55f5c5f7b5502b53",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving the dataset (1/1 shards): 100%|██████████| 1305/1305 [00:00<00:00, 326273.65 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 436/436 [00:00<00:00, 109014.40 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 436/436 [00:00<00:00, 145297.68 examples/s]\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### We now create Baselines with pre-trained Multilingual transformers",
   "id": "5bbb7afbfe25ad22"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Baseline with [mBERT](https://huggingface.co/google-bert/bert-base-multilingual-cased)",
   "id": "928f426b19b379b1"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Baseline with [XLM-RoBERTa](https://huggingface.co/docs/transformers/en/model_doc/xlm-roberta)",
   "id": "1a1c31f71943bd02"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Baseline with [AfroXLMR](https://huggingface.co/Davlan/afro-xlmr-large)\n",
   "id": "5938ba763e7fb284"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### We now perform fine-tuning on the Multilingual transformers\n",
   "id": "2bbaf73c7a762eab"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Fine-tuning [mBERT](https://huggingface.co/google-bert/bert-base-multilingual-cased)\n",
   "id": "e96f40a394aebaa6"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Fine-tuning [XLM-RoBERTa](https://huggingface.co/docs/transformers/en/model_doc/xlm-roberta)",
   "id": "88a4400037a4682"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Fine-tuning [XLM-RoBERTa](https://huggingface.co/docs/transformers/en/model_doc/xlm-roberta)",
   "id": "ac4ce50a2e9546bd"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## We now implement and evaluate the effect of Transfer learning on Transformer performance",
   "id": "d0cf85b26ad1bb5f"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## We now implement and evaluate the effects of several Augmentation Strategies",
   "id": "61953463fc0ec2b9"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Firstly, we explore the effects of Back-Translation on model performance when analysing the Mozambican Portuguese data",
   "id": "54c1d7e0be0ea4ed"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "87d56a54d7b786fe"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Next, we explore the effects of Contextual Substitution on model performance when analysing the Mozambican Portuguese data\n",
   "id": "ab953850b10d116a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "9aff377d11fb5368"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Next, we explore the effects of adapters on model performance\n",
    "This will be through using Cross-Lingual transfer learning (CLTL) with Swahili data with the aim of improving model performance on Sesotho.\n",
    "This decision was made because...(sesotho has less resources for example)\n"
   ],
   "id": "5e765e9d0d36f08a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "48cb657ff158095"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Lastly, we use LLM-based synthetic data generation with the aim of addressing class imbalance\n",
   "id": "78def7c99cf8e077"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "e0fd40509cfc1fb0"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## This section seeks to use attention visualise and feature attribution to find language-specific sentiment patterns\n",
    "Furthermore, we explore how linguistic nuances within our affect classification across languages. There is a particular focus on Sesotho models because...."
   ],
   "id": "73aad88c7d909859"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "fa610ae72db2bddc"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### SHAP\n",
    "Link: [SHAP docs](https://shap.readthedocs.io/en/latest/)"
   ],
   "id": "6f0b76bd6b6d033a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "e69b897654a94b25"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### LIME\n",
    "Link: [LIME docs](https://uc-r.github.io/lime)"
   ],
   "id": "5e04cbcf2f4883c1"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "9a6175a1aa9caa92"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
