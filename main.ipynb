{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "122d19e5-25f7-4b2e-9d44-f706fe7ddc36",
   "metadata": {},
   "source": [
    "# COS 760 Research Project: Analysing Sentiments for Low-resource African Languages"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3a3c357-1a7b-4731-bfb1-c3db6af9d407",
   "metadata": {},
   "source": [
    "## Group Members: Mihir Arjun, Troy Clark, Hamza Mokiwa"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d058d144-ec71-42a2-ba89-92e7d9062a0d",
   "metadata": {},
   "source": [
    "First we need to install the datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01cc4f81-df43-4e94-8dc6-c84fef76a32c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "054dd69e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Swahili (swa) dataset from disk...\n",
      "Swahili dataset loaded!\n",
      "Loading Portuguese (por) dataset from disk...\n",
      "Portuguese dataset loaded!\n",
      "Swahili dataset size: 1810 examples\n",
      "Portuguese dataset size: 3063 examples\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from datasets import load_from_disk\n",
    "\n",
    "def load_local_datasets():\n",
    "    # Define the paths to your local datasets\n",
    "    swa_path = \"./datasets/afrisenti/swa\"\n",
    "    por_path = \"./datasets/afrisenti/por\"\n",
    "    # Need to add in code for Sesotho dataset\n",
    "    #[CODE GOES HERE]\n",
    "    \n",
    "    # Check if the directories exist\n",
    "    if not all(os.path.exists(path) for path in [swa_path, por_path]):\n",
    "        print(\"One or more dataset directories not found. Please check the paths.\")\n",
    "        return None, None, None\n",
    "    \n",
    "    # Load datasets from disk\n",
    "    print(\"Loading Swahili (swa) dataset from disk...\")\n",
    "    swa_dataset = load_from_disk(swa_path)\n",
    "    print(\"Swahili dataset loaded!\")\n",
    "    \n",
    "    print(\"Loading Portuguese (por) dataset from disk...\")\n",
    "    por_dataset = load_from_disk(por_path)\n",
    "    print(\"Portuguese dataset loaded!\")\n",
    "    \n",
    "    #Add Code for Sesotho Dataset here\n",
    "    #[CODE GOES HERE]\n",
    "    \n",
    "    return swa_dataset, por_dataset\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    swa, por = load_local_datasets() #add call for Sesotho dataset here\n",
    "    \n",
    "    # Now you can work with these datasets\n",
    "    if swa is not None:\n",
    "        print(f\"Swahili dataset size: {len(swa['train'])} examples\")\n",
    "    if por is not None:\n",
    "        print(f\"Portuguese dataset size: {len(por['train'])} examples\")\n",
    "    # if ses is not None:\n",
    "    #     print(f\"Ses dataset size: {len(ses['train'])} examples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bf9822d",
   "metadata": {},
   "source": [
    "Now that the datasets have been loaded, we can start creating our LSTM baseline models below:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cc2a9bc",
   "metadata": {},
   "source": [
    "First we will build an LSTM model for Swahili"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3d28f49b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "Swahili dataset loaded successfully!\n",
      "Available columns in train split: ['tweet', 'label']\n",
      "Dataset structure: DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['tweet', 'label'],\n",
      "        num_rows: 1810\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['tweet', 'label'],\n",
      "        num_rows: 453\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['tweet', 'label'],\n",
      "        num_rows: 748\n",
      "    })\n",
      "})\n",
      "Train set size: 1810\n",
      "Test set size: 748\n",
      "Validation set size: 453\n",
      "Label mapping: {'negative': 0, 'neutral': 1, 'positive': 2}\n",
      "Vocabulary size: 3055\n",
      "Model architecture:\n",
      "LSTMSentiment(\n",
      "  (embedding): Embedding(3055, 100, padding_idx=0)\n",
      "  (lstm): LSTM(100, 128, num_layers=2, batch_first=True, dropout=0.2, bidirectional=True)\n",
      "  (fc): Linear(in_features=256, out_features=3, bias=True)\n",
      "  (dropout): Dropout(p=0.2, inplace=False)\n",
      ")\n",
      "Starting training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10 - Training: 100%|██████████| 57/57 [00:07<00:00,  7.55it/s]\n",
      "Epoch 1/10 - Validation: 100%|██████████| 15/15 [00:00<00:00, 36.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10:\n",
      "  Train Loss: 0.9255\n",
      "  Val Loss: 0.8964, Val Accuracy: 0.5872, Val F1: 0.4377\n",
      "  Saved new best model!\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/10 - Training: 100%|██████████| 57/57 [00:07<00:00,  7.84it/s]\n",
      "Epoch 2/10 - Validation: 100%|██████████| 15/15 [00:00<00:00, 42.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/10:\n",
      "  Train Loss: 0.8834\n",
      "  Val Loss: 0.9085, Val Accuracy: 0.5872, Val F1: 0.4720\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/10 - Training: 100%|██████████| 57/57 [00:06<00:00,  8.59it/s]\n",
      "Epoch 3/10 - Validation: 100%|██████████| 15/15 [00:00<00:00, 32.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/10:\n",
      "  Train Loss: 0.8355\n",
      "  Val Loss: 0.9070, Val Accuracy: 0.5960, Val F1: 0.5168\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/10 - Training: 100%|██████████| 57/57 [00:06<00:00,  8.93it/s]\n",
      "Epoch 4/10 - Validation: 100%|██████████| 15/15 [00:00<00:00, 32.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/10:\n",
      "  Train Loss: 0.7643\n",
      "  Val Loss: 0.9697, Val Accuracy: 0.5762, Val F1: 0.4923\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/10 - Training: 100%|██████████| 57/57 [00:06<00:00,  8.20it/s]\n",
      "Epoch 5/10 - Validation: 100%|██████████| 15/15 [00:00<00:00, 27.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/10:\n",
      "  Train Loss: 0.6388\n",
      "  Val Loss: 1.0302, Val Accuracy: 0.5497, Val F1: 0.5318\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/10 - Training: 100%|██████████| 57/57 [00:06<00:00,  8.23it/s]\n",
      "Epoch 6/10 - Validation: 100%|██████████| 15/15 [00:00<00:00, 29.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/10:\n",
      "  Train Loss: 0.5488\n",
      "  Val Loss: 1.2454, Val Accuracy: 0.5055, Val F1: 0.5120\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/10 - Training: 100%|██████████| 57/57 [00:07<00:00,  7.92it/s]\n",
      "Epoch 7/10 - Validation: 100%|██████████| 15/15 [00:00<00:00, 28.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/10:\n",
      "  Train Loss: 0.4460\n",
      "  Val Loss: 1.2486, Val Accuracy: 0.5585, Val F1: 0.5315\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/10 - Training: 100%|██████████| 57/57 [00:07<00:00,  7.64it/s]\n",
      "Epoch 8/10 - Validation: 100%|██████████| 15/15 [00:00<00:00, 26.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/10:\n",
      "  Train Loss: 0.3874\n",
      "  Val Loss: 1.5211, Val Accuracy: 0.4857, Val F1: 0.4953\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/10 - Training: 100%|██████████| 57/57 [00:07<00:00,  7.96it/s]\n",
      "Epoch 9/10 - Validation: 100%|██████████| 15/15 [00:00<00:00, 32.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/10:\n",
      "  Train Loss: 0.2739\n",
      "  Val Loss: 1.5772, Val Accuracy: 0.5055, Val F1: 0.5088\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/10 - Training: 100%|██████████| 57/57 [00:06<00:00,  8.60it/s]\n",
      "Epoch 10/10 - Validation: 100%|██████████| 15/15 [00:00<00:00, 35.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/10:\n",
      "  Train Loss: 0.2220\n",
      "  Val Loss: 1.5910, Val Accuracy: 0.5541, Val F1: 0.5337\n",
      "------------------------------------------------------------\n",
      "\n",
      "Evaluating on test set...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|██████████| 24/24 [00:00<00:00, 27.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.8965\n",
      "Test Accuracy: 0.5936\n",
      "Test F1 Score: 0.4446\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.00      0.00      0.00        80\n",
      "     neutral       0.59      1.00      0.74       444\n",
      "    positive       0.50      0.00      0.01       224\n",
      "\n",
      "    accuracy                           0.59       748\n",
      "   macro avg       0.36      0.33      0.25       748\n",
      "weighted avg       0.50      0.59      0.44       748\n",
      "\n",
      "Results saved to swahili_lstm_results.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "c:\\Mihir - University\\2025\\COS 760\\Project\\nlp-research\\.venv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Mihir - University\\2025\\COS 760\\Project\\nlp-research\\.venv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Mihir - University\\2025\\COS 760\\Project\\nlp-research\\.venv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datasets import load_from_disk\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from sklearn.metrics import accuracy_score, f1_score, classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "from collections import Counter\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# LSTM Hyperparameters\n",
    "EMBEDDING_DIM = 100\n",
    "HIDDEN_DIM = 128\n",
    "NUM_LAYERS = 2\n",
    "DROPOUT = 0.2\n",
    "LEARNING_RATE = 0.001\n",
    "BATCH_SIZE = 32\n",
    "NUM_EPOCHS = 10\n",
    "\n",
    "class SwahiliSentimentDataset(Dataset):\n",
    "    def __init__(self, tweets, labels, vocab, label_map):\n",
    "        self.tweets = tweets\n",
    "        self.labels = labels\n",
    "        self.vocab = vocab\n",
    "        self.label_map = label_map\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.tweets)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        tweet = self.tweets[idx]\n",
    "        label = self.labels[idx]\n",
    "        \n",
    "        # Convert text to indices using the vocabulary\n",
    "        tokenized = [self.vocab.get(word, self.vocab['<UNK>']) for word in tweet.split()]\n",
    "        return torch.tensor(tokenized, dtype=torch.long), torch.tensor(self.label_map[label], dtype=torch.long)\n",
    "\n",
    "def collate_fn(batch):\n",
    "    tweets, labels = zip(*batch)\n",
    "    # Pad sequences to the length of the longest sequence in the batch\n",
    "    tweets_padded = pad_sequence(tweets, batch_first=True, padding_value=0)\n",
    "    return tweets_padded, torch.stack(labels)\n",
    "\n",
    "class LSTMSentiment(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim, n_layers, dropout, pad_idx):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=pad_idx)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, num_layers=n_layers, \n",
    "                           bidirectional=True, dropout=dropout, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim * 2, output_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, text):\n",
    "        # text shape: [batch size, seq length]\n",
    "        embedded = self.dropout(self.embedding(text))\n",
    "        # embedded shape: [batch size, seq length, embedding dim]\n",
    "        \n",
    "        output, (hidden, cell) = self.lstm(embedded)\n",
    "        # hidden shape: [2*num_layers, batch size, hidden dim]\n",
    "        \n",
    "        # Concatenate the final forward and backward hidden states\n",
    "        hidden = torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim=1)\n",
    "        # hidden shape: [batch size, hidden dim * 2]\n",
    "        \n",
    "        hidden = self.dropout(hidden)\n",
    "        return self.fc(hidden)\n",
    "\n",
    "def train_model(model, train_loader, val_loader, optimizer, criterion, device, epochs):\n",
    "    best_val_loss = float('inf')\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        # Training\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        \n",
    "        for tweets, labels in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{epochs} - Training\"):\n",
    "            tweets, labels = tweets.to(device), labels.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model(tweets)\n",
    "            \n",
    "            # Calculate loss\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            # Backward pass and optimize\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            running_loss += loss.item()\n",
    "        \n",
    "        train_loss = running_loss / len(train_loader)\n",
    "        \n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        all_preds = []\n",
    "        all_labels = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for tweets, labels in tqdm(val_loader, desc=f\"Epoch {epoch+1}/{epochs} - Validation\"):\n",
    "                tweets, labels = tweets.to(device), labels.to(device)\n",
    "                \n",
    "                outputs = model(tweets)\n",
    "                loss = criterion(outputs, labels)\n",
    "                \n",
    "                val_loss += loss.item()\n",
    "                \n",
    "                _, preds = torch.max(outputs, 1)\n",
    "                all_preds.extend(preds.cpu().numpy())\n",
    "                all_labels.extend(labels.cpu().numpy())\n",
    "        \n",
    "        val_loss /= len(val_loader)\n",
    "        val_accuracy = accuracy_score(all_labels, all_preds)\n",
    "        val_f1 = f1_score(all_labels, all_preds, average='weighted')\n",
    "        \n",
    "        print(f\"Epoch {epoch+1}/{epochs}:\")\n",
    "        print(f\"  Train Loss: {train_loss:.4f}\")\n",
    "        print(f\"  Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.4f}, Val F1: {val_f1:.4f}\")\n",
    "        \n",
    "        # Save the best model\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            torch.save(model.state_dict(), \"best_swahili_lstm_model.pt\")\n",
    "            print(\"  Saved new best model!\")\n",
    "        \n",
    "        print(\"-\" * 60)\n",
    "    \n",
    "    return model\n",
    "\n",
    "def evaluate_model(model, test_loader, criterion, device, label_list):\n",
    "    model.eval()\n",
    "    test_loss = 0.0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for tweets, labels in tqdm(test_loader, desc=\"Testing\"):\n",
    "            tweets, labels = tweets.to(device), labels.to(device)\n",
    "            \n",
    "            outputs = model(tweets)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            test_loss += loss.item()\n",
    "            \n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "    \n",
    "    test_loss /= len(test_loader)\n",
    "    test_accuracy = accuracy_score(all_labels, all_preds)\n",
    "    test_f1 = f1_score(all_labels, all_preds, average='weighted')\n",
    "    \n",
    "    print(f\"Test Loss: {test_loss:.4f}\")\n",
    "    print(f\"Test Accuracy: {test_accuracy:.4f}\")\n",
    "    print(f\"Test F1 Score: {test_f1:.4f}\")\n",
    "    \n",
    "    # Print detailed classification report\n",
    "    class_names = [label_list[i] for i in range(len(label_list))]\n",
    "    report = classification_report(all_labels, all_preds, target_names=class_names)\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(report)\n",
    "    \n",
    "    return test_loss, test_accuracy, test_f1\n",
    "\n",
    "def main():\n",
    "    # Check if GPU is available\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print(f\"Using device: {device}\")\n",
    "    \n",
    "    # Load the SwahiliSenti dataset from local storage\n",
    "    try:\n",
    "        swa_dataset = load_from_disk(\"./datasets/afrisenti/swa\")\n",
    "        print(\"Swahili dataset loaded successfully!\")\n",
    "        \n",
    "        # Print out available columns to debug\n",
    "        print(f\"Available columns in train split: {swa_dataset['train'].column_names}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading dataset: {e}\")\n",
    "        return\n",
    "    \n",
    "    # Explore dataset structure\n",
    "    print(f\"Dataset structure: {swa_dataset}\")\n",
    "    print(f\"Train set size: {len(swa_dataset['train'])}\")\n",
    "    print(f\"Test set size: {len(swa_dataset['test'])}\")\n",
    "    print(f\"Validation set size: {len(swa_dataset['validation'])}\")\n",
    "    \n",
    "    # Extract data - using 'tweet' column instead of 'text'\n",
    "    train_tweets = swa_dataset['train']['tweet']\n",
    "    train_labels = swa_dataset['train']['label']\n",
    "    val_tweets = swa_dataset['validation']['tweet']\n",
    "    val_labels = swa_dataset['validation']['label']\n",
    "    test_tweets = swa_dataset['test']['tweet']\n",
    "    test_labels = swa_dataset['test']['label']\n",
    "    \n",
    "    # Get unique labels and create label to index mapping\n",
    "    # In AfriSenti, labels are usually 'positive', 'negative', 'neutral'\n",
    "    unique_labels = set(train_labels + val_labels + test_labels)\n",
    "    label_to_idx = {label: idx for idx, label in enumerate(unique_labels)}\n",
    "    idx_to_label = {idx: label for label, idx in label_to_idx.items()}\n",
    "    print(f\"Label mapping: {label_to_idx}\")\n",
    "    \n",
    "    # Build vocabulary from training data\n",
    "    word_counts = Counter()\n",
    "    for tweet in train_tweets:\n",
    "        word_counts.update(tweet.split())\n",
    "    \n",
    "    # Keep only words that appear at least 2 times\n",
    "    min_freq = 2\n",
    "    vocabulary = {'<PAD>': 0, '<UNK>': 1}\n",
    "    vocab_idx = 2\n",
    "    \n",
    "    for word, count in word_counts.items():\n",
    "        if count >= min_freq:\n",
    "            vocabulary[word] = vocab_idx\n",
    "            vocab_idx += 1\n",
    "    \n",
    "    print(f\"Vocabulary size: {len(vocabulary)}\")\n",
    "    \n",
    "    # Create datasets\n",
    "    train_dataset = SwahiliSentimentDataset(train_tweets, train_labels, vocabulary, label_to_idx)\n",
    "    val_dataset = SwahiliSentimentDataset(val_tweets, val_labels, vocabulary, label_to_idx)\n",
    "    test_dataset = SwahiliSentimentDataset(test_tweets, test_labels, vocabulary, label_to_idx)\n",
    "    \n",
    "    # Create dataloaders\n",
    "    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_fn)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, collate_fn=collate_fn)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, collate_fn=collate_fn)\n",
    "    \n",
    "    # Initialize model\n",
    "    model = LSTMSentiment(\n",
    "        vocab_size=len(vocabulary),\n",
    "        embedding_dim=EMBEDDING_DIM,\n",
    "        hidden_dim=HIDDEN_DIM,\n",
    "        output_dim=len(unique_labels),\n",
    "        n_layers=NUM_LAYERS,\n",
    "        dropout=DROPOUT,\n",
    "        pad_idx=vocabulary['<PAD>']\n",
    "    ).to(device)\n",
    "    \n",
    "    # Print model architecture\n",
    "    print(f\"Model architecture:\\n{model}\")\n",
    "    \n",
    "    # Define optimizer and loss function\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    # Train the model\n",
    "    print(\"Starting training...\")\n",
    "    model = train_model(\n",
    "        model=model,\n",
    "        train_loader=train_loader,\n",
    "        val_loader=val_loader,\n",
    "        optimizer=optimizer,\n",
    "        criterion=criterion,\n",
    "        device=device,\n",
    "        epochs=NUM_EPOCHS\n",
    "    )\n",
    "    \n",
    "    # Load best model for evaluation\n",
    "    model.load_state_dict(torch.load(\"best_swahili_lstm_model.pt\"))\n",
    "    \n",
    "    # Evaluate on test set\n",
    "    print(\"\\nEvaluating on test set...\")\n",
    "    test_loss, test_accuracy, test_f1 = evaluate_model(\n",
    "        model=model,\n",
    "        test_loader=test_loader,\n",
    "        criterion=criterion,\n",
    "        device=device,\n",
    "        label_list=list(idx_to_label.values())\n",
    "    )\n",
    "    \n",
    "    # Save evaluation results\n",
    "    results = {\n",
    "        \"test_loss\": test_loss,\n",
    "        \"test_accuracy\": test_accuracy,\n",
    "        \"test_f1\": test_f1,\n",
    "        \"embedding_dim\": EMBEDDING_DIM,\n",
    "        \"hidden_dim\": HIDDEN_DIM,\n",
    "        \"num_layers\": NUM_LAYERS,\n",
    "        \"dropout\": DROPOUT,\n",
    "        \"learning_rate\": LEARNING_RATE,\n",
    "        \"batch_size\": BATCH_SIZE,\n",
    "        \"epochs\": NUM_EPOCHS,\n",
    "        \"vocab_size\": len(vocabulary)\n",
    "    }\n",
    "    \n",
    "    # Save results to CSV\n",
    "    pd.DataFrame([results]).to_csv(\"swahili_lstm_results.csv\", index=False)\n",
    "    print(f\"Results saved to swahili_lstm_results.csv\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e693c96",
   "metadata": {},
   "source": [
    "Next we build an LSTM model for Mozambican Portuguese:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d62e04f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "Mozambican Portuguese dataset loaded successfully!\n",
      "Available columns in train split: ['tweet', 'label']\n",
      "Dataset structure: DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['tweet', 'label'],\n",
      "        num_rows: 3063\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['tweet', 'label'],\n",
      "        num_rows: 767\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['tweet', 'label'],\n",
      "        num_rows: 3662\n",
      "    })\n",
      "})\n",
      "Train set size: 3063\n",
      "Test set size: 3662\n",
      "Validation set size: 767\n",
      "Label mapping: {'negative': 0, 'neutral': 1, 'positive': 2}\n",
      "Vocabulary size: 4075\n",
      "Model architecture:\n",
      "LSTMSentiment(\n",
      "  (embedding): Embedding(4075, 100, padding_idx=0)\n",
      "  (lstm): LSTM(100, 128, num_layers=2, batch_first=True, dropout=0.2, bidirectional=True)\n",
      "  (fc): Linear(in_features=256, out_features=3, bias=True)\n",
      "  (dropout): Dropout(p=0.2, inplace=False)\n",
      ")\n",
      "Starting training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10 - Training: 100%|██████████| 96/96 [00:12<00:00,  7.80it/s]\n",
      "Epoch 1/10 - Validation: 100%|██████████| 24/24 [00:01<00:00, 19.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10:\n",
      "  Train Loss: 1.0226\n",
      "  Val Loss: 0.9994, Val Accuracy: 0.5215, Val F1: 0.3575\n",
      "  Saved new best model!\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/10 - Training: 100%|██████████| 96/96 [00:15<00:00,  6.37it/s]\n",
      "Epoch 2/10 - Validation: 100%|██████████| 24/24 [00:00<00:00, 24.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/10:\n",
      "  Train Loss: 0.9703\n",
      "  Val Loss: 0.9651, Val Accuracy: 0.5450, Val F1: 0.4652\n",
      "  Saved new best model!\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/10 - Training: 100%|██████████| 96/96 [00:13<00:00,  6.98it/s]\n",
      "Epoch 3/10 - Validation: 100%|██████████| 24/24 [00:00<00:00, 24.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/10:\n",
      "  Train Loss: 0.8965\n",
      "  Val Loss: 0.9932, Val Accuracy: 0.5567, Val F1: 0.4923\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/10 - Training: 100%|██████████| 96/96 [00:13<00:00,  6.95it/s]\n",
      "Epoch 4/10 - Validation: 100%|██████████| 24/24 [00:01<00:00, 20.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/10:\n",
      "  Train Loss: 0.7983\n",
      "  Val Loss: 0.9902, Val Accuracy: 0.5606, Val F1: 0.5459\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/10 - Training: 100%|██████████| 96/96 [00:14<00:00,  6.67it/s]\n",
      "Epoch 5/10 - Validation: 100%|██████████| 24/24 [00:01<00:00, 23.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/10:\n",
      "  Train Loss: 0.6935\n",
      "  Val Loss: 1.0411, Val Accuracy: 0.5528, Val F1: 0.5396\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/10 - Training: 100%|██████████| 96/96 [00:13<00:00,  6.92it/s]\n",
      "Epoch 6/10 - Validation: 100%|██████████| 24/24 [00:00<00:00, 25.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/10:\n",
      "  Train Loss: 0.5816\n",
      "  Val Loss: 1.1400, Val Accuracy: 0.5424, Val F1: 0.5358\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/10 - Training: 100%|██████████| 96/96 [00:13<00:00,  7.09it/s]\n",
      "Epoch 7/10 - Validation: 100%|██████████| 24/24 [00:01<00:00, 21.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/10:\n",
      "  Train Loss: 0.4691\n",
      "  Val Loss: 1.2334, Val Accuracy: 0.5593, Val F1: 0.5388\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/10 - Training: 100%|██████████| 96/96 [00:14<00:00,  6.71it/s]\n",
      "Epoch 8/10 - Validation: 100%|██████████| 24/24 [00:01<00:00, 23.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/10:\n",
      "  Train Loss: 0.3591\n",
      "  Val Loss: 1.4979, Val Accuracy: 0.5332, Val F1: 0.5243\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/10 - Training: 100%|██████████| 96/96 [00:13<00:00,  7.02it/s]\n",
      "Epoch 9/10 - Validation: 100%|██████████| 24/24 [00:00<00:00, 28.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/10:\n",
      "  Train Loss: 0.3022\n",
      "  Val Loss: 1.4727, Val Accuracy: 0.5111, Val F1: 0.5080\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/10 - Training: 100%|██████████| 96/96 [00:13<00:00,  6.88it/s]\n",
      "Epoch 10/10 - Validation: 100%|██████████| 24/24 [00:01<00:00, 21.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/10:\n",
      "  Train Loss: 0.2282\n",
      "  Val Loss: 1.6321, Val Accuracy: 0.5332, Val F1: 0.5234\n",
      "------------------------------------------------------------\n",
      "\n",
      "Evaluating on test set...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|██████████| 115/115 [00:04<00:00, 25.81it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.8757\n",
      "Test Accuracy: 0.6393\n",
      "Test F1 Score: 0.5655\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.31      0.15      0.21       655\n",
      "     neutral       0.67      0.92      0.78      2379\n",
      "    positive       0.62      0.08      0.14       628\n",
      "\n",
      "    accuracy                           0.64      3662\n",
      "   macro avg       0.54      0.38      0.37      3662\n",
      "weighted avg       0.60      0.64      0.57      3662\n",
      "\n",
      "Results saved to portuguese_lstm_results.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datasets import load_from_disk\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from sklearn.metrics import accuracy_score, f1_score, classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "from collections import Counter\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# LSTM Hyperparameters\n",
    "EMBEDDING_DIM = 100\n",
    "HIDDEN_DIM = 128\n",
    "NUM_LAYERS = 2\n",
    "DROPOUT = 0.2\n",
    "LEARNING_RATE = 0.001\n",
    "BATCH_SIZE = 32\n",
    "NUM_EPOCHS = 10\n",
    "\n",
    "class PorSentimentDataset(Dataset):\n",
    "    def __init__(self, tweets, labels, vocab, label_map):\n",
    "        self.tweets = tweets\n",
    "        self.labels = labels\n",
    "        self.vocab = vocab\n",
    "        self.label_map = label_map\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.tweets)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        tweet = self.tweets[idx]\n",
    "        label = self.labels[idx]\n",
    "        \n",
    "        # Convert text to indices using the vocabulary\n",
    "        tokenized = [self.vocab.get(word, self.vocab['<UNK>']) for word in tweet.split()]\n",
    "        return torch.tensor(tokenized, dtype=torch.long), torch.tensor(self.label_map[label], dtype=torch.long)\n",
    "\n",
    "def collate_fn(batch):\n",
    "    tweets, labels = zip(*batch)\n",
    "    # Pad sequences to the length of the longest sequence in the batch\n",
    "    tweets_padded = pad_sequence(tweets, batch_first=True, padding_value=0)\n",
    "    return tweets_padded, torch.stack(labels)\n",
    "\n",
    "class LSTMSentiment(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim, n_layers, dropout, pad_idx):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=pad_idx)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, num_layers=n_layers, \n",
    "                           bidirectional=True, dropout=dropout, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim * 2, output_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, text):\n",
    "        # text shape: [batch size, seq length]\n",
    "        embedded = self.dropout(self.embedding(text))\n",
    "        # embedded shape: [batch size, seq length, embedding dim]\n",
    "        \n",
    "        output, (hidden, cell) = self.lstm(embedded)\n",
    "        # hidden shape: [2*num_layers, batch size, hidden dim]\n",
    "        \n",
    "        # Concatenate the final forward and backward hidden states\n",
    "        hidden = torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim=1)\n",
    "        # hidden shape: [batch size, hidden dim * 2]\n",
    "        \n",
    "        hidden = self.dropout(hidden)\n",
    "        return self.fc(hidden)\n",
    "\n",
    "def train_model(model, train_loader, val_loader, optimizer, criterion, device, epochs):\n",
    "    best_val_loss = float('inf')\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        # Training\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        \n",
    "        for tweets, labels in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{epochs} - Training\"):\n",
    "            tweets, labels = tweets.to(device), labels.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model(tweets)\n",
    "            \n",
    "            # Calculate loss\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            # Backward pass and optimize\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            running_loss += loss.item()\n",
    "        \n",
    "        train_loss = running_loss / len(train_loader)\n",
    "        \n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        all_preds = []\n",
    "        all_labels = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for tweets, labels in tqdm(val_loader, desc=f\"Epoch {epoch+1}/{epochs} - Validation\"):\n",
    "                tweets, labels = tweets.to(device), labels.to(device)\n",
    "                \n",
    "                outputs = model(tweets)\n",
    "                loss = criterion(outputs, labels)\n",
    "                \n",
    "                val_loss += loss.item()\n",
    "                \n",
    "                _, preds = torch.max(outputs, 1)\n",
    "                all_preds.extend(preds.cpu().numpy())\n",
    "                all_labels.extend(labels.cpu().numpy())\n",
    "        \n",
    "        val_loss /= len(val_loader)\n",
    "        val_accuracy = accuracy_score(all_labels, all_preds)\n",
    "        val_f1 = f1_score(all_labels, all_preds, average='weighted')\n",
    "        \n",
    "        print(f\"Epoch {epoch+1}/{epochs}:\")\n",
    "        print(f\"  Train Loss: {train_loss:.4f}\")\n",
    "        print(f\"  Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.4f}, Val F1: {val_f1:.4f}\")\n",
    "        \n",
    "        # Save the best model\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            torch.save(model.state_dict(), \"best_portuguese_lstm_model.pt\")\n",
    "            print(\"  Saved new best model!\")\n",
    "        \n",
    "        print(\"-\" * 60)\n",
    "    \n",
    "    return model\n",
    "\n",
    "def evaluate_model(model, test_loader, criterion, device, label_list):\n",
    "    model.eval()\n",
    "    test_loss = 0.0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for tweets, labels in tqdm(test_loader, desc=\"Testing\"):\n",
    "            tweets, labels = tweets.to(device), labels.to(device)\n",
    "            \n",
    "            outputs = model(tweets)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            test_loss += loss.item()\n",
    "            \n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "    \n",
    "    test_loss /= len(test_loader)\n",
    "    test_accuracy = accuracy_score(all_labels, all_preds)\n",
    "    test_f1 = f1_score(all_labels, all_preds, average='weighted')\n",
    "    \n",
    "    print(f\"Test Loss: {test_loss:.4f}\")\n",
    "    print(f\"Test Accuracy: {test_accuracy:.4f}\")\n",
    "    print(f\"Test F1 Score: {test_f1:.4f}\")\n",
    "    \n",
    "    # Print detailed classification report\n",
    "    class_names = [label_list[i] for i in range(len(label_list))]\n",
    "    report = classification_report(all_labels, all_preds, target_names=class_names)\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(report)\n",
    "    \n",
    "    return test_loss, test_accuracy, test_f1\n",
    "\n",
    "def main():\n",
    "    # Check if GPU is available\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print(f\"Using device: {device}\")\n",
    "    \n",
    "    # Load the Portuguese dataset from local storage\n",
    "    try:\n",
    "        por_dataset = load_from_disk(\"./datasets/afrisenti/por\")\n",
    "        print(\"Mozambican Portuguese dataset loaded successfully!\")\n",
    "        \n",
    "        # Print out available columns to debug\n",
    "        print(f\"Available columns in train split: {por_dataset['train'].column_names}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading dataset: {e}\")\n",
    "        return\n",
    "    \n",
    "    # Explore dataset structure\n",
    "    print(f\"Dataset structure: {por_dataset}\")\n",
    "    print(f\"Train set size: {len(por_dataset['train'])}\")\n",
    "    print(f\"Test set size: {len(por_dataset['test'])}\")\n",
    "    print(f\"Validation set size: {len(por_dataset['validation'])}\")\n",
    "    \n",
    "    # Extract data - using 'tweet' column instead of 'text'\n",
    "    train_tweets = por_dataset['train']['tweet']\n",
    "    train_labels = por_dataset['train']['label']\n",
    "    val_tweets = por_dataset['validation']['tweet']\n",
    "    val_labels = por_dataset['validation']['label']\n",
    "    test_tweets = por_dataset['test']['tweet']\n",
    "    test_labels = por_dataset['test']['label']\n",
    "    \n",
    "    # Get unique labels and create label to index mapping\n",
    "    # In AfriSenti, labels are usually 'positive', 'negative', 'neutral'\n",
    "    unique_labels = set(train_labels + val_labels + test_labels)\n",
    "    label_to_idx = {label: idx for idx, label in enumerate(unique_labels)}\n",
    "    idx_to_label = {idx: label for label, idx in label_to_idx.items()}\n",
    "    print(f\"Label mapping: {label_to_idx}\")\n",
    "    \n",
    "    # Build vocabulary from training data\n",
    "    word_counts = Counter()\n",
    "    for tweet in train_tweets:\n",
    "        word_counts.update(tweet.split())\n",
    "    \n",
    "    # Keep only words that appear at least 2 times\n",
    "    min_freq = 2\n",
    "    vocabulary = {'<PAD>': 0, '<UNK>': 1}\n",
    "    vocab_idx = 2\n",
    "    \n",
    "    for word, count in word_counts.items():\n",
    "        if count >= min_freq:\n",
    "            vocabulary[word] = vocab_idx\n",
    "            vocab_idx += 1\n",
    "    \n",
    "    print(f\"Vocabulary size: {len(vocabulary)}\")\n",
    "    \n",
    "    # Create datasets\n",
    "    train_dataset = PorSentimentDataset(train_tweets, train_labels, vocabulary, label_to_idx)\n",
    "    val_dataset = PorSentimentDataset(val_tweets, val_labels, vocabulary, label_to_idx)\n",
    "    test_dataset = PorSentimentDataset(test_tweets, test_labels, vocabulary, label_to_idx)\n",
    "    \n",
    "    # Create dataloaders\n",
    "    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_fn)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, collate_fn=collate_fn)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, collate_fn=collate_fn)\n",
    "    \n",
    "    # Initialize model\n",
    "    model = LSTMSentiment(\n",
    "        vocab_size=len(vocabulary),\n",
    "        embedding_dim=EMBEDDING_DIM,\n",
    "        hidden_dim=HIDDEN_DIM,\n",
    "        output_dim=len(unique_labels),\n",
    "        n_layers=NUM_LAYERS,\n",
    "        dropout=DROPOUT,\n",
    "        pad_idx=vocabulary['<PAD>']\n",
    "    ).to(device)\n",
    "    \n",
    "    # Print model architecture\n",
    "    print(f\"Model architecture:\\n{model}\")\n",
    "    \n",
    "    # Define optimizer and loss function\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    # Train the model\n",
    "    print(\"Starting training...\")\n",
    "    model = train_model(\n",
    "        model=model,\n",
    "        train_loader=train_loader,\n",
    "        val_loader=val_loader,\n",
    "        optimizer=optimizer,\n",
    "        criterion=criterion,\n",
    "        device=device,\n",
    "        epochs=NUM_EPOCHS\n",
    "    )\n",
    "    \n",
    "    # Load best model for evaluation\n",
    "    model.load_state_dict(torch.load(\"best_portuguese_lstm_model.pt\"))\n",
    "    \n",
    "    # Evaluate on test set\n",
    "    print(\"\\nEvaluating on test set...\")\n",
    "    test_loss, test_accuracy, test_f1 = evaluate_model(\n",
    "        model=model,\n",
    "        test_loader=test_loader,\n",
    "        criterion=criterion,\n",
    "        device=device,\n",
    "        label_list=list(idx_to_label.values())\n",
    "    )\n",
    "    \n",
    "    # Save evaluation results\n",
    "    results = {\n",
    "        \"test_loss\": test_loss,\n",
    "        \"test_accuracy\": test_accuracy,\n",
    "        \"test_f1\": test_f1,\n",
    "        \"embedding_dim\": EMBEDDING_DIM,\n",
    "        \"hidden_dim\": HIDDEN_DIM,\n",
    "        \"num_layers\": NUM_LAYERS,\n",
    "        \"dropout\": DROPOUT,\n",
    "        \"learning_rate\": LEARNING_RATE,\n",
    "        \"batch_size\": BATCH_SIZE,\n",
    "        \"epochs\": NUM_EPOCHS,\n",
    "        \"vocab_size\": len(vocabulary)\n",
    "    }\n",
    "    \n",
    "    # Save results to CSV\n",
    "    pd.DataFrame([results]).to_csv(\"portuguese_lstm_results.csv\", index=False)\n",
    "    print(f\"Results saved to portuguese_lstm_results.csv\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ed05fc8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
