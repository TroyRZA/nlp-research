swa lstm:
test_loss: 1.4440673540035884

moz lstm:
test_loss: 1.4682833329490994

sot lstm:
test_loss: 0.8850015785012927

mbert losses:
Dataset Model Loss Accuracy F1  
Swahili mBERT 1.037037 0.346257 0.271630
Portuguese mBERT 1.078578 0.176952 0.079557  
Sesotho mBERT 1.198339 0.178899 0.093481

XLMR losses
Dataset | Val Loss | Val Acc | Val F1 | Test Loss | Test Acc | Test F1 | Path

---

Portuguese | 0.8233 | 0.6284 | 0.5986 | 0.8125 | 0.6379 | 0.6315 | ./models/portuguese  
Swahili | 0.9323 | 0.5916 | 0.4398 | 0.9309 | 0.5936 | 0.4422 | ./models/swahili  
Sotho | 1.1280 | 0.2000 | 0.0667 | 1.0593 | 0.4000 | 0.2286 | ./models/sotho

===== DISTILBERT FINE-TUNING RESULTS SUMMARY =====
Dataset | Val Loss | Val Acc | Val F1 | Test Loss | Test Acc | Test F1 | Path

---

Portuguese | 0.8424 | 0.6128 | 0.6092 | 0.8650 | 0.5795 | 0.5915 | ./models/portuguese  
Swahili | 0.9542 | 0.5916 | 0.4398 | 0.9515 | 0.5936 | 0.4422 | ./models/swahili  
Sotho | 1.0749 | 0.5000 | 0.4000 | 1.1201 | 0.1000 | 0.1250 | ./models/sotho

Add a reflections section, what we did well and didnt do well + limitations

Read up on LIME

Read up on LoRa

What mihir was saying about google trans api, what most people do in use english as a common language and then compare the models.
